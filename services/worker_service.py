import asyncio
import logging
import json
import random
import math
from datetime import datetime, timedelta
from core.pipeline import MessageContext
from utils.processing.forward_queue import FloodWaitException
import structlog
from core.exceptions import TransientError, PermanentError, BusinessLogicError
from core.config import settings

from utils.core.logger_utils import get_logger, short_id
from utils.processing.forward_queue import get_messages_queued, send_file_queued
from filters.delay_filter import RescheduleTaskException

logger = get_logger(__name__)

class WorkerService:
    def __init__(self, client, task_repo, pipeline, downloader=None):
        self.client = client
        self.repo = task_repo
        self.pipeline = pipeline
        self.downloader = downloader
        self.running = False
        # åŠ¨æ€ä¼‘çœ ç­–ç•¥é…ç½®
        self.min_sleep = 0.1  # æœ€å°ä¼‘çœ æ—¶é—´ (ç§’)
        self.max_sleep = 2.0   # æœ€å¤§ä¼‘çœ æ—¶é—´ (ç§’)
        self.current_sleep = self.min_sleep
        self.sleep_increment = 0.1  # æ¯æ¬¡å¢åŠ çš„ä¼‘çœ æ—¶é—´

    async def start(self):
        self.running = True
        logger.info("worker_started")
        while self.running:
            task = None
            try:
                task = await self.repo.fetch_next()
                if not task:
                    # æ²¡ä»»åŠ¡æ—¶ï¼Œé€æ¸å¢åŠ ä¼‘çœ æ—¶é—´ (0.1s -> 2s)
                    # é¿å…æ­»å¾ªç¯æŸ¥è¯¢æ•°æ®åº“ï¼Œé™ä½ CPU å’Œ DB è´Ÿè½½
                    await self._adaptive_sleep() 
                    continue
                
                self._reset_sleep() # æœ‰ä»»åŠ¡ï¼Œé‡ç½®ä¼‘çœ 
                
                # ç¡®ä¿è¿æ¥æ­£å¸¸ï¼Œé˜²æ­¢ Telethon æ–­è¿å¯¼è‡´å¤„ç†å¤±è´¥
                await self._ensure_connected()
                
                # [å…³é”®] ç»‘å®šä¸Šä¸‹æ–‡ï¼šæ­¤åè¯¥å¾ªç¯å†…çš„æ‰€æœ‰æ—¥å¿—éƒ½ä¼šè‡ªåŠ¨å¸¦ä¸Š task_id
                log = logger.bind(task_id=task.id, task_type=task.task_type)
                log.info("task_processing_start")
                
                # === ä¸Šä¸‹æ–‡è¿˜åŸ (Hydration) ===
                try:
                    payload = json.loads(task.task_data)
                    chat_id = payload.get('chat_id')
                    msg_id = payload.get('message_id')
                    
                    # [ä¼˜åŒ–] è·å–èŠå¤©æ˜¾ç¤ºåç§°
                    from utils.helpers.id_utils import get_display_name_async
                    chat_display = await get_display_name_async(chat_id)
                    
                    log.info(f"ğŸ”„ [Worker] å¼€å§‹å¤„ç†ä»»åŠ¡ {short_id(task.id)}: æ¥æº={chat_display}({chat_id}), æ¶ˆæ¯ID={msg_id}")
                    grouped_id = payload.get('grouped_id') # è·å– grouped_id
                    
                    if not chat_id or not msg_id:
                        log.error("task_invalid_payload", task_data=task.task_data)
                        await self.repo.fail(task.id, "Invalid Payload")
                        continue

                    # === åª’ä½“ç»„èšåˆé€»è¾‘ ===
                    group_tasks = []
                    if grouped_id:
                        # å°è¯•è·å–åŒç»„çš„å…¶ä»–ä»»åŠ¡
                        group_tasks = await self.repo.fetch_group_tasks(grouped_id, task.id)
                        if group_tasks:
                            log.info(f"aggregated_group_tasks", count=len(group_tasks), grouped_id=grouped_id)
                    
                    # æ”¶é›†æ‰€æœ‰ç›¸å…³ä»»åŠ¡ï¼ˆå½“å‰ä»»åŠ¡ + åŒç»„ä»»åŠ¡ï¼‰
                    all_related_tasks = [task] + group_tasks
                    all_message_ids = [msg_id]
                    
                    # è§£æåŒç»„ä»»åŠ¡çš„ message_id
                    if group_tasks:
                        for t in group_tasks:
                            try:
                                p = json.loads(t.task_data)
                                if p.get('message_id'):
                                    all_message_ids.append(p.get('message_id'))
                            except Exception as ex:
                                logger.warning(f"Failed to parse group task data: {ex}")
                                pass
                    
                    # å…³é”®ç‚¹ï¼šä» Telethon è·å–çœŸå®æ¶ˆæ¯å¯¹è±¡ (æ‰¹é‡è·å–)
                    # å¦‚æœæ¶ˆæ¯å·²è¿‡æœŸæˆ–è¢«åˆ ï¼Œè¿™é‡Œä¼šè¿”å› None
                    messages = await get_messages_queued(self.client, chat_id, ids=all_message_ids)
                    
                    # è¿‡æ»¤æ‰ None (æœ‰äº›æ¶ˆæ¯å¯èƒ½å·²è¢«åˆ )
                    valid_messages = []
                    if isinstance(messages, list):
                        valid_messages = [m for m in messages if m]
                    elif messages:
                         valid_messages = [messages]

                    if not valid_messages:
                        log.debug("task_source_message_not_found", chat_id=chat_id, message_ids=all_message_ids)
                        # æ¶ˆæ¯ä¸å­˜åœ¨ï¼Œæ ‡è®°ä¸ºå¤±è´¥
                        await self.repo.fail(task.id, "Source message not found")
                        for t in group_tasks:
                            await self.repo.fail(t.id, "Source message not found (Group)")
                        continue
                    
                    primary_message = valid_messages[0]
                    log.info(f"ğŸ“¥ [Worker] æˆåŠŸè·å–æ¶ˆæ¯å¯¹è±¡: ID={primary_message.id}, å†…å®¹é¢„è§ˆ={primary_message.text[:20] if primary_message.text else 'No Text'}")
                    
                    # === è¿›å…¥å¤„ç†ç®¡é“ ===
                    if task.task_type == "process_message":
                        # èµ°å®Œæ•´ç®¡é“
                        ctx = MessageContext(
                            client=self.client,
                            task_id=task.id,
                            chat_id=chat_id,
                            message_id=msg_id,
                            message_obj=primary_message,
                            # æ³¨å…¥åª’ä½“ç»„ä¿¡æ¯
                            is_group=bool(grouped_id),
                            group_messages=valid_messages if grouped_id else [],
                            related_tasks=group_tasks
                        )
                        # æ‰§è¡Œç®¡é“ (Middleware Chain)
                        try:
                            await self.pipeline.execute(ctx)
                        except FloodWaitException as e:
                            # æ•è·FloodWaitExceptionï¼Œå°†å…¶è½¬åŒ–ä¸ºæˆ‘ä»¬å®šä¹‰çš„ TransientError
                            await self._retry_group(all_related_tasks, e, log)
                            continue
                        except TransientError as e:
                            # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                            await self._retry_group(all_related_tasks, e, log)
                            continue
                        except PermanentError as e:
                            # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                            log.error("task_permanent_error", error=str(e), error_type="Permanent")
                            await self.repo.fail(task.id, str(e))
                            for t in group_tasks:
                                await self.repo.fail(t.id, str(e))
                            continue
                    
                    elif task.task_type == "download_file":
                        # ç›´æ¥è°ƒç”¨ä¸‹è½½æœåŠ¡ï¼Œç»•è¿‡ RuleLoader å’Œ Filter
                        # è¿™æ˜¯ä¸€ä¸ª"ç‰¹æƒ"ä»»åŠ¡
                        if not self.downloader:
                            log.error("downloader_not_initialized")
                            await self.repo.fail(task.id, "Downloader not initialized")
                            continue
                        
                        sub_folder = str(chat_id)
                        try:
                            await self.downloader.push_to_queue(primary_message, sub_folder)
                        except FloodWaitException as e:
                            # æ•è·FloodWaitExceptionï¼Œå°†å…¶è½¬åŒ–ä¸ºæˆ‘ä»¬å®šä¹‰çš„ TransientError
                            await self._retry_task(task, e, log)
                            continue
                        except TransientError as e:
                            # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                            await self._retry_task(task, e, log)
                            continue
                        except PermanentError as e:
                            # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                            log.error("task_permanent_error", error=str(e), error_type="Permanent")
                            await self.repo.fail(task.id, str(e))
                            continue
                    
                    elif task.task_type == "manual_download":
                        # å¤„ç†æ‰‹åŠ¨ä¸‹è½½ä»»åŠ¡ï¼Œç›´æ¥è°ƒç”¨DownloadService
                        # å¯ä»¥æŒ‡å®šä¸€ä¸ªç‰¹æ®Šçš„ä¸‹è½½ç›®å½•ï¼Œå¦‚ "./downloads/manual"
                        if not self.downloader:
                            log.error("downloader_not_initialized")
                            await self.repo.fail(task.id, "Downloader not initialized")
                            continue
                        
                        # ä½¿ç”¨"manual"ä½œä¸ºå­æ–‡ä»¶å¤¹ï¼ŒåŒºåˆ†æ‰‹åŠ¨ä¸‹è½½å’Œè‡ªåŠ¨ä¸‹è½½
                        try:
                            path = await self.downloader.push_to_queue(
                                primary_message, 
                                sub_folder="manual"
                            )
                            log.info("manual_download_completed", path=path)
                            
                            # [Scheme 7 Feature] å¦‚æœæœ‰ç›®æ ‡IDï¼Œåˆ™æ‰§è¡Œè½¬å‘
                            target_id = payload.get('target_chat_id')
                            if target_id:
                                try:
                                    await send_file_queued(
                                        self.client,
                                        target_id,
                                        path,
                                        caption=primary_message.text or ""
                                    )
                                    log.info(f"manual_forward_completed", target_id=target_id)
                                except Exception as e:
                                    log.error(f"manual_forward_failed", target_id=target_id, error=str(e))
                                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªè®°å½•é”™è¯¯ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºä¸‹è½½å·²ç»æˆåŠŸäº†
                        except FloodWaitException as e:
                            # æ•è·FloodWaitExceptionï¼Œä½¿ç”¨ç»Ÿä¸€çš„é‡è¯•é€»è¾‘
                            await self._retry_task(task, e, log)
                            continue
                        except TransientError as e:
                            # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                            await self._retry_task(task, e, log)
                            continue
                        except PermanentError as e:
                            # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                            log.error("task_permanent_error", error=str(e), error_type="Permanent")
                            await self.repo.fail(task.id, str(e))
                            continue
                    
                    # === ä»»åŠ¡æˆåŠŸ ===
                    await self.repo.complete(task.id)
                    log.info("task_completed")

                except Exception as e:
                    if isinstance(e, RescheduleTaskException):
                         # [éé˜»å¡å»¶è¿Ÿå¤„ç†]
                         # æ•è· RescheduleTaskExceptionï¼Œå°†ä»»åŠ¡ä»¥æŒ‡å®šå»¶è¿Ÿé‡æ–°æ”¾å…¥é˜Ÿåˆ—
                         log.info("task_delay_requested", delay_seconds=e.delay_seconds)
                         
                         next_run = datetime.utcnow() + timedelta(seconds=e.delay_seconds)
                         await self.repo.reschedule(task.id, next_run)
                         
                         # å¦‚æœæœ‰åŒç»„ä»»åŠ¡ï¼Œä¹Ÿä¸€èµ·å»¶è¿Ÿ
                         if group_tasks:
                             for t in group_tasks:
                                 await self.repo.reschedule(t.id, next_run)
                         continue
                         
                    if isinstance(e, (FloodWaitException, TransientError)):
                        # æ•è·FloodWaitExceptionæˆ–TransientErrorï¼Œä½¿ç”¨ç»Ÿä¸€çš„é‡è¯•é€»è¾‘
                        log.warning(f"ä»»åŠ¡é‡åˆ°ç¬æ€é”™è¯¯ï¼Œå°†é‡è¯•: ç±»å‹={type(e).__name__}, é”™è¯¯={str(e)}")
                        await self._retry_task(task, e, log)
                    elif isinstance(e, PermanentError):
                        # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                        log.error(f"ä»»åŠ¡æ°¸ä¹…å¤±è´¥: é”™è¯¯={str(e)}, ç±»å‹=Permanent, è§„åˆ™ID={task.rule_id if hasattr(task, 'rule_id') else 'N/A'}", exc_info=True)
                        await self.repo.fail(task.id, str(e))
                    else:
                        from utils.helpers.id_utils import get_display_name_async
                        chat_display = await get_display_name_async(chat_id)
                        log.exception(f"ä»»åŠ¡æœªå¤„ç†é”™è¯¯: é”™è¯¯={str(e)}, ä»»åŠ¡ID={short_id(task.id)}, ä»»åŠ¡ç±»å‹={task.task_type}, æ¥æº={chat_display}({chat_id}), æ¶ˆæ¯ID={msg_id}")
                        # è®°å½•å…·ä½“çš„é”™è¯¯ä¿¡æ¯åˆ°æ•°æ®åº“
                        await self.repo.fail(task.id, f"Unhandled: {str(e)}")

            except Exception as e:
                # å¤–å±‚å¾ªç¯ä¿æŠ¤ï¼Œé˜²æ­¢ Worker å› ä¸º DB è¿æ¥ç­‰ä¸¥é‡é”™è¯¯å´©æºƒé€€å‡º
                task_id = task.id if task else None
                task_type = task.task_type if task else 'æœªçŸ¥'
                chat_id = payload.get('chat_id') if task and payload else None
                msg_id = payload.get('message_id') if task and payload else None
                log_exception = logger.bind(task_id=task_id, task_type=task_type)
                from utils.helpers.id_utils import get_display_name_async
                chat_display = await get_display_name_async(chat_id)
                log_exception.exception(f"Worker å…³é”®é”™è¯¯: é”™è¯¯={str(e)}, æ¥æº={chat_display}({chat_id}), æ¶ˆæ¯ID={msg_id}")
                await asyncio.sleep(1) # å‡ºé”™åç¨ä½œæš‚åœ

    async def stop(self):
        """ä¼˜é›…åœæ­¢ Worker"""
        logger.info("worker_stopping")
        self.running = False

    async def _adaptive_sleep(self):
        """è‡ªé€‚åº”ä¼‘çœ ï¼šå¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œé€æ­¥å¢åŠ ä¼‘çœ æ—¶é—´ï¼Œå‡å°‘èµ„æºæ¶ˆè€—"""
        await asyncio.sleep(self.current_sleep)
        if self.current_sleep < self.max_sleep:
            self.current_sleep = min(self.current_sleep + self.sleep_increment, self.max_sleep)

    def _reset_sleep(self):
        """é‡ç½®ä¼‘çœ æ—¶é—´"""
        self.current_sleep = self.min_sleep

    async def _ensure_connected(self):
        """ç¡®ä¿ Telethon å®¢æˆ·ç«¯å·²è¿æ¥"""
        if not self.client.is_connected():
            logger.warning("Client disconnected. Attempting to reconnect...")
            try:
                await self.client.connect()
            except Exception as e:
                logger.error(f"Reconnection failed: {e}")
                # ç­‰å¾…ä¸€ä¼šå„¿å†é‡è¯•ï¼Œé¿å…æ­»å¾ªç¯å†²å‡»
                await asyncio.sleep(5)
    
    def _calculate_backoff(self, retry_count: int) -> float:
        """
        è®¡ç®—æŒ‡æ•°é€€é¿æ—¶é—´
        å…¬å¼: min(base * (factor ^ retries), max) + jitter
        """
        # é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸
        safe_retries = min(retry_count, 10)
        
        delay = settings.RETRY_BASE_DELAY * (settings.RETRY_BACKOFF_FACTOR ** safe_retries)
        
        # æˆªæ–­åˆ°æœ€å¤§å»¶è¿Ÿ
        delay = min(delay, settings.RETRY_MAX_DELAY)
        
        # æ·»åŠ  0-10% çš„éšæœºæŠ–åŠ¨ï¼Œé˜²æ­¢æƒŠç¾¤æ•ˆåº” (Thundering Herd)
        jitter = delay * random.uniform(0, 0.1)
        
        return delay + jitter
    
    async def _retry_task(self, task, error, log):
        """
        å¤„ç†ä»»åŠ¡é‡è¯•ï¼Œæ ¹æ®é”™è¯¯ç±»å‹å’Œé‡è¯•æ¬¡æ•°å†³å®šåç»­æ“ä½œ
        """
        current_retries = task.retry_count + 1
        
        # å¦‚æœè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå‡çº§ä¸ºæ°¸ä¹…å¤±è´¥
        if current_retries > settings.MAX_RETRIES:
            log.error("task_max_retries_exceeded", retry_count=current_retries, max_retries=settings.MAX_RETRIES, error=str(error))
            await self.repo.fail(task.id, f"Max retries exceeded: {str(error)}")
            return

        # è®¡ç®—ç­‰å¾…æ—¶é—´
        if isinstance(error, FloodWaitException):
            wait_seconds = error.seconds + 1 # é¢å¤–å¤šç­‰1ç§’ä¿é™©
        else:
            wait_seconds = self._calculate_backoff(current_retries)
            
        next_run = datetime.utcnow() + timedelta(seconds=wait_seconds)
        
        log.warning(
            "task_rescheduled", 
            retry_count=current_retries,
            max_retries=settings.MAX_RETRIES,
            wait_seconds=wait_seconds,
            next_run=next_run.isoformat(),
            error_type=type(error).__name__,
            error=str(error)
        )
        
        # è°ƒç”¨rescheduleæ–¹æ³•ï¼Œæ›´æ–°task.next_retry_atå­—æ®µ
        await self.repo.reschedule(
            task.id, 
            next_run
        )
        
    async def _retry_group(self, tasks, error, log):
        """
        æ‰¹é‡å¤„ç†ä»»åŠ¡é‡è¯•
        """
        for task in tasks:
            await self._retry_task(task, error, log)