import asyncio
import json
import random
import time
import psutil
from datetime import datetime, timedelta
from core.pipeline import MessageContext
from services.queue_service import FloodWaitException
from core.exceptions import TransientError, PermanentError
from core.config import settings

from core.logging import get_logger, short_id
from services.queue_service import get_messages_queued, send_file_queued
from filters.delay_filter import RescheduleTaskException

logger = get_logger(__name__)

class WorkerService:
    def __init__(self, client, task_repo, pipeline, downloader=None):
        self.client = client
        self.repo = task_repo
        self.pipeline = pipeline
        self.downloader = downloader
        self.running = False
        # åŠ¨æ€ä¼‘çœ ç­–ç•¥é…ç½®
        self.min_sleep = 0.5  # æœ€å°ä¼‘çœ æ—¶é—´ (ç§’)
        self.max_sleep = 30.0  # æœ€å¤§ä¼‘çœ æ—¶é—´ (ç§’)
        self.current_sleep = self.min_sleep
        self.sleep_increment = 1.0  
        
        # [NEW] ä¸­å¤®åˆ†å‘èµ„æº
        self.task_queue = asyncio.Queue(maxsize=settings.WORKER_QUEUE_SIZE)
        self.dispatcher = None # åœ¨ start() ä¸­åˆå§‹åŒ–
        
        # [NEW] èµ„æºé˜ˆå€¼
        self.mem_warning = settings.MEMORY_WARNING_THRESHOLD_MB
        self.mem_critical = settings.MEMORY_CRITICAL_THRESHOLD_MB
        self.last_gc_time = 0

    async def start(self):
        """å¯åŠ¨ Worker æœåŠ¡ (åŠ¨æ€å¹¶å‘æ± )"""
        self.running = True
        logger.info(f"WorkerService å¯åŠ¨ (Min: {settings.WORKER_MIN_CONCURRENCY}, Max: {settings.WORKER_MAX_CONCURRENCY})")
        
        self.workers = {} # task -> worker_id
        
        # [Fix] å¯åŠ¨å‰æ¸…ç†åƒµå°¸ä»»åŠ¡ï¼Œé˜²æ­¢å†å²é—ç•™çš„ running ä»»åŠ¡å ç”¨çŠ¶æ€
        try:
            count = await self.repo.rescue_stuck_tasks(timeout_minutes=15)
            if count > 0:
                logger.info(f"â™»ï¸ ç³»ç»Ÿå¯åŠ¨æ•‘æ´: å·²é‡ç½® {count} ä¸ªåƒµå°¸ä»»åŠ¡ä¸º PENDING çŠ¶æ€")
        except Exception as e:
            logger.error(f"Failed to rescue tasks during startup: {e}")

        # [Phase 14] å¯åŠ¨ä¸­å¤®åˆ†å‘å™¨ (Dispatcher)
        from services.task_dispatcher import TaskDispatcher
        self.dispatcher = TaskDispatcher(self.repo, self.task_queue)
        await self.dispatcher.start()

        # å¯åŠ¨åˆå§‹ Workers
        for i in range(settings.WORKER_MIN_CONCURRENCY):
            self._spawn_worker()

        # å¯åŠ¨å¼¹æ€§ä¼¸ç¼©ç›‘æ§
        self._monitor_task = asyncio.create_task(self._monitor_scaling(), name="worker_scaling_monitor")
        
        # [NEW] å¯åŠ¨ Loop Lag ç›‘æ§
        self._lag_monitor_task = asyncio.create_task(self._monitor_loop_lag(), name="loop_lag_monitor")
        
        # ä¿æŒä¸»ä»»åŠ¡è¿è¡Œï¼ˆç”¨äºæ¥æ”¶åœæ­¢ä¿¡å·ï¼‰
        while self.running:
            await asyncio.sleep(1)

    def _spawn_worker(self):
        """Spawn a new worker"""
        if len(self.workers) >= settings.WORKER_MAX_CONCURRENCY:
            return

        worker_id = f"worker-{short_id(None, 4)}"
        task = asyncio.create_task(self._worker_loop(worker_id), name=worker_id)
        self.workers[task] = worker_id
        logger.debug(f"Spawned worker {worker_id} (Total: {len(self.workers)})")
        task.add_done_callback(lambda t: self.workers.pop(t, None))

    async def _kill_worker(self):
        """Kill an idle worker (approximate)"""
        if len(self.workers) <= settings.WORKER_MIN_CONCURRENCY:
            return

        # Simple kill: Cancel the last added task
        # Improvement: Cancel idle workers?
        # For now, just pop one randomly or last
        task = list(self.workers.keys())[-1]
        worker_id = self.workers[task]
        task.cancel()
        logger.debug(f"Scaling down: Cancelled worker {worker_id}")

    async def _monitor_scaling(self):
        """
        æ™ºèƒ½åŒ–åŠ¨æ€ä¼¸ç¼©ç›‘æ§ (Resource-Aware & Load-Adaptive)
        ç­–ç•¥ï¼š
        1. [åˆ†çº§æ‰©å®¹] æ ¹æ®ç§¯å‹é‡çº§å†³å®šæ‰©å®¹é€Ÿåº¦
        2. [ç³»ç»Ÿè´Ÿè½½ä¿æŠ¤] ç›‘æ§ CPU/å†…å­˜ï¼Œé«˜è´Ÿè½½æ—¶æŠ‘åˆ¶æ‰©å®¹
        3. [å¹³æ»‘ç¼©å®¹] é‡‡ç”¨å»¶è¿Ÿç¼©å®¹ï¼Œé˜²æ­¢éœ‡è¡
        """
        logger.info(f"ğŸš€ [WorkerService] æ™ºèƒ½åŒ–åŠ¨æ€ä¼¸ç¼©ç›‘æ§å·²å¯åŠ¨ (Max: {settings.WORKER_MAX_CONCURRENCY})")
        
        idle_cycles = 0 # è¿ç»­ç©ºé—²å‘¨æœŸè®¡æ•°
        
        while self.running:
            try:
                await asyncio.sleep(10) # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
                # è·å–é˜Ÿåˆ—çŠ¶æ€
                queue_status = await self.repo.get_queue_status()
                # ä¿®å¤ P0: ä½¿ç”¨æ­£ç¡®é”®å active_queues
                pending_count = queue_status.get('active_queues', 0)
                current_workers = len(self.workers)
                
                # --- ç¬¬ä¸€æ­¥ï¼šèµ„æºå®ˆå« (Resource Guard) ---
                try:
                    # ä½¿ç”¨ psutil è·å–æ›´å‡†ç¡®çš„ CPU ä½¿ç”¨ç‡ (æŒ‡å®š interval)
                    # ä½†æ˜¯åœ¨ async å¾ªç¯ä¸­ä¸èƒ½é•¿æ—¶é—´é˜»å¡ï¼Œæ”¹ç”¨éé˜»å¡è·å–
                    # interval=None ä¼šè¿”å›ä¸Šä¸€ç§’åˆ°ç°åœ¨çš„æ—¶é—´å·®
                    cpu_usage = psutil.cpu_percent(interval=None)
                    process = psutil.Process()
                    memory_info = process.memory_info()
                    memory_mb = memory_info.rss / 1024 / 1024
                    
                    # è·å–ç³»ç»Ÿæ€»ä½“è´Ÿè½½ (Load Average)
                    try:
                        load_1, load_5, load_15 = psutil.getloadavg()
                        cpu_count = psutil.cpu_count()
                        load_ratio = load_1 / cpu_count if cpu_count else 0
                    except (AttributeError, Exception):
                        load_ratio = 0
                except Exception:
                    cpu_usage = 0
                    memory_mb = 0
                    load_ratio = 0

                # --- ç¬¬äºŒæ­¥ï¼šè®¡ç®—ç›®æ ‡ Worker æ•° (Target Logic) ---
                # ç­–ç•¥ï¼šæ›´ç¨³å¥çš„é˜¶æ¢¯å¼æ‰©å®¹
                if pending_count == 0:
                    target_count = settings.WORKER_MIN_CONCURRENCY
                else:
                    # æŒ‰ç…§ç§¯å‹é‡çº§åˆ†æ¡£
                    if pending_count > 10000:
                        # æç«¯ç§¯å‹ï¼šå…è®¸è¾¾åˆ°æœ€å¤§å€¼çš„ä¸€åŠæˆ– 15 (å–è¾ƒå¤§å€¼)ï¼Œè€Œä¸æ˜¯ç›´æ¥æ‹‰æ»¡
                        limit = max(settings.WORKER_MAX_CONCURRENCY // 2, 15)
                        target_count = min(settings.WORKER_MAX_CONCURRENCY, limit)
                    elif pending_count > 1000:
                        # ä¸­ç­‰ç§¯å‹ï¼šæ¯ 500 ä¸ªä»»åŠ¡åŠ  1 ä¸ª worker
                        target_count = settings.WORKER_MIN_CONCURRENCY + (pending_count // 500)
                    else:
                        # è½»å¾®ç§¯å‹ï¼šæ¯ 100 ä¸ªä»»åŠ¡åŠ  1 ä¸ª worker
                        target_count = settings.WORKER_MIN_CONCURRENCY + (pending_count // 100)

                # çº¦æŸç›®æ ‡å€¼
                target_count = max(settings.WORKER_MIN_CONCURRENCY, min(settings.WORKER_MAX_CONCURRENCY, target_count))

                # --- ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œè°ƒæ•´ (Execution & Guard) ---
                log_diagnostic = False
                if idle_cycles % 6 == 0: # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡è¯Šæ–­
                     log_diagnostic = True
                     # [Fix] æ¯åˆ†é’Ÿå°è¯•æ•‘æ´ä¸€æ¬¡è¶…æ—¶ä¸¥é‡çš„ä»»åŠ¡
                     await self.repo.rescue_stuck_tasks(timeout_minutes=20)
                
                if log_diagnostic:
                    logger.info(f"ğŸ“Š [WorkerService] ç³»ç»Ÿè´Ÿè½½: CPU={cpu_usage}%, Load={load_ratio:.2f}, RAM={memory_mb:.1f}MB | ç§¯å‹={pending_count}, Workers={current_workers}/{target_count}")

                if current_workers < target_count:
                    # æ‰©å®¹å®ˆå«ï¼šå¦‚æœç³»ç»Ÿå·²ç»é«˜è´Ÿè½½ï¼Œä¸¥ç¦è¿›ä¸€æ­¥æ‰©å®¹
                    # CPU > 80% æˆ– LoadRatio > 1.2 (è¯´æ˜ 1.2 å€ CPU æ ¸å¿ƒæ­£åœ¨æ’é˜Ÿ)
                    if cpu_usage > 80 or load_ratio > 1.2:
                        if log_diagnostic:
                            logger.warning(f"âš ï¸ [WorkerService] ç³»ç»Ÿé«˜è´Ÿè½½ï¼Œå·²æš‚åœæ‰©å®¹è®¡åˆ’ (CPU={cpu_usage}%, Load={load_ratio:.2f})")
                    if memory_mb > self.mem_critical:
                        logger.error(f"ğŸš¨ [ResourceGuard] å†…å­˜å±æœºä¼šè¯ (RSS={memory_mb:.1f}MB > {self.mem_critical}MB)ï¼Œæ‰§è¡ŒåŒæ­¥ç†”æ–­ï¼šæš‚åœåˆ†å‘å¹¶å¼ºåˆ¶å…¨é‡ GC")
                        if self.dispatcher: await self.dispatcher.stop()
                        import gc
                        gc.collect()
                        await asyncio.sleep(5) 
                        if self.running and self.dispatcher: await self.dispatcher.start()
                    elif memory_mb > self.mem_warning: 
                        if log_diagnostic:
                            logger.warning(f"âš ï¸ [ResourceGuard] å†…å­˜å ç”¨è¾ƒé«˜ ({memory_mb:.1f}MB > {self.mem_warning}MB)ï¼Œæš‚åœæ‰©å®¹å¹¶è§¦å‘è½»é‡çº§ GC")
                        import gc
                        gc.collect(1)
                    else:
                        diff = target_count - current_workers
                        # æ‰©å®¹æ­¥é•¿æ›´ä¿å®ˆï¼šä¸€æ¬¡æœ€å¤šå¢åŠ  3 ä¸ª (åŸæ¥æ˜¯ 5)
                        step = min(diff, 3) 
                        logger.info(f"ğŸ“ˆ [WorkerService] æ‰©å®¹ä¸­: +{step} workers (è´Ÿè½½æ­£å¸¸)")
                        for _ in range(step):
                            self._spawn_worker()
                    idle_cycles = 0 
                
                elif current_workers > target_count:
                    # ç¼©å®¹ç­–ç•¥ï¼šæ›´æ¿€è¿›åœ°é‡Šæ”¾èµ„æº
                    idle_cycles += 1
                    # å¦‚æœ CPU éå¸¸é«˜ï¼Œç«‹å³ç¼©å®¹ï¼Œä¸éœ€è¦ç­‰å¾… 3 ä¸ªå‘¨æœŸ
                    if cpu_usage > 95 or load_ratio > 2.0:
                         logger.warning(f"ğŸš¨ [WorkerService] æç«¯é«˜è´Ÿè½½ï¼Œç´§æ€¥é‡Šæ”¾èµ„æº: -2 workers")
                         for _ in range(min(current_workers - settings.WORKER_MIN_CONCURRENCY, 2)):
                             await self._kill_worker()
                         idle_cycles = 0
                    elif idle_cycles >= 3:
                        diff = max(1, (current_workers - target_count) // 2) # åˆ†æ‰¹æ¬¡ç¼©å®¹
                        logger.info(f"ğŸ“‰ [WorkerService] ç¼©å®¹ä¸­: -{diff} workers")
                        for _ in range(diff):
                            await self._kill_worker()
                        idle_cycles = 0
                else:
                    idle_cycles = 0

            except Exception as e:
                logger.error(f"Scaling monitor error: {e}", exc_info=True)
                await asyncio.sleep(5)

    async def _monitor_loop_lag(self):
        """[Resource Guard] ç›‘æ§äº‹ä»¶å¾ªç¯å»¶è¿Ÿ (Loop Lag)"""
        threshold = settings.LOOP_LAG_THRESHOLD_MS / 1000.0
        while self.running:
            try:
                start = asyncio.get_event_loop().time()
                await asyncio.sleep(1.0)
                lag = asyncio.get_event_loop().time() - start - 1.0
                
                if lag > threshold:
                    logger.warning(f"âš ï¸ [LoopLag] æ£€æµ‹åˆ°å¼‚æ­¥å»¶è¿Ÿ: {lag:.3f}s (é˜ˆå€¼: {threshold}s)ï¼Œç³»ç»Ÿè´Ÿè½½å¤„äºé«˜ä½")
                    # å¦‚æœå»¶è¿Ÿè¿‡é«˜ï¼Œç”±è°ƒåº¦å™¨å†³å®šæ˜¯å¦é™é€Ÿ (æœªæ¥å¯å¢åŠ è”åŠ¨)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Loop lag monitor error: {e}")
                await asyncio.sleep(10)

    async def _worker_loop(self, worker_id: str):
        """å•ä¸ª Worker çš„å·¥ä½œå¾ªç¯ (æ”¯æŒæ‰¹é‡ä»»åŠ¡å¤„ç†)"""
        logger.debug(f"[{worker_id}] Loop Started")
        
        while self.running:
            try:
                try:
                      # [Optimization] æ”¹ä¸ºä»ä¸­å¤®é˜Ÿåˆ—è·å–ä»»åŠ¡æ‰¹æ¬¡ï¼Œå½»åº•æ¶ˆé™¤ DB é”ç«äº‰
                      tasks = await self.task_queue.get()
                      # å¦‚æœ Dispatcher æ”¾å…¥çš„æ˜¯å•ä¸ªä»»åŠ¡ï¼ŒåŒ…è£…ä¸ºåˆ—è¡¨ï¼›å¦‚æœæ˜¯åˆ—è¡¨ï¼ˆåª’ä½“ç»„ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                      if not isinstance(tasks, list):
                          tasks = [tasks]
                except asyncio.CancelledError:
                     logger.debug(f"[{worker_id}] Cancelled during queue.get")
                     raise

                # [Fix] è¿™é‡Œçš„ tasks æ°¸è¿œä¸ä¸ºç©ºï¼Œå› ä¸º Dispatcher åªä¼šåœ¨æœ‰ä»»åŠ¡æ—¶æ‰æ”¾å…¥é˜Ÿåˆ—
                # queue.get() åœ¨æ²¡æœ‰ä»»åŠ¡æ—¶ä¼šå¤„äºé˜»å¡çŠ¶æ€ï¼Œä¸æ¶ˆè€— CPU
                
                # æŒ‰ç…§ grouped_id å¯¹æ‹‰å–åˆ°çš„ä»»åŠ¡è¿›è¡Œåˆ†ç»„ (åª’ä½“ç»„èšåˆ)
                # å¦‚æœæ²¡æœ‰ grouped_idï¼Œåˆ™è§†ä¸ºç‹¬ç«‹ä»»åŠ¡
                task_groups = {}
                for t in tasks:
                    gid = t.grouped_id or f"single-{t.id}"
                    if gid not in task_groups:
                        task_groups[gid] = []
                    task_groups[gid].append(t)

                # ä¾æ¬¡å¤„ç†æ¯ä¸ªä»»åŠ¡ç»„
                for gid, group in task_groups.items():
                    # ç¡®ä¿è¿æ¥æ­£å¸¸
                    await self._ensure_connected()
                    
                    main_task = group[0]
                    sub_tasks = group[1:] if len(group) > 1 else []
                    
                    # [å…³é”®] ç»‘å®šä¸Šä¸‹æ–‡
                    log = logger.bind(worker_id=worker_id, task_id=main_task.id, task_type=main_task.task_type)
                    
                    try:
                        await self._process_task_safely(main_task, log, group_tasks=sub_tasks)
                    except Exception as e:
                        log.error(f"group_processing_failed", error=str(e), gid=gid)

            except asyncio.CancelledError:
                logger.debug(f"[{worker_id}] Cancelled")
                break
            except Exception as e:
                logger.error(f"[{worker_id}] Loop Error: {e}")
                await asyncio.sleep(1)

    async def _process_task_safely(self, task, log, group_tasks: list = None):
        """å¤„ç†åŸºç¡€ä»»åŠ¡çš„å®‰å…¨å°è£…ï¼Œæ”¯æŒä¼ å…¥é¢„å…ˆé”å®šçš„åª’ä½“ç»„ä»»åŠ¡"""
        try:
            payload = json.loads(task.task_data)
            
            # [Optimization] å¤„ç†ä¸éœ€è¦é¢„å–æ¶ˆæ¯çš„ä»»åŠ¡ç±»å‹
            if task.task_type == "message_delete":
                chat_id = payload.get('chat_id')
                message_ids = payload.get('message_ids', [])
                if not chat_id or not message_ids:
                    log.error("delete_task_invalid_payload", payload=payload)
                    await self.repo.fail(task.id, "Invalid Delete Payload")
                    return
                
                try:
                    log.info(f"ğŸ—‘ï¸ [Worker] æ‰§è¡Œåˆ é™¤æ¶ˆæ¯ä»»åŠ¡: Chat={chat_id}, IDs={message_ids}")
                    await self.client.delete_messages(chat_id, message_ids)
                    await self.repo.complete(task.id)
                    return
                except Exception as e:
                    log.error(f"delete_messages_failed", error=str(e))
                    await self._retry_task(task, e, log)
                    return

            if task.task_type == "custom_task":
                log.info(f"âš™ï¸ [Worker] å¤„ç†è‡ªå®šä¹‰ä»»åŠ¡: {payload.get('action')}")
                # TODO: ä»¥åå¯æ‰©å±•åŸºäº action çš„è·¯ç”±
                await self.repo.complete(task.id)
                return

            # --- ä»¥ä¸‹æ˜¯éœ€è¦è·å–åŸå§‹æ¶ˆæ¯çš„ä»»åŠ¡ç±»å‹ (process_message, download_file, manual_download) ---
            chat_id = payload.get('chat_id')
            msg_id = payload.get('message_id')
            
            # [ä¼˜åŒ–] è·å–èŠå¤©æ˜¾ç¤ºåç§°
            from core.helpers.id_utils import get_display_name_async
            chat_display = await get_display_name_async(chat_id)
            
            log.info(f"ğŸ”„ [Worker] å¼€å§‹å¤„ç†ä»»åŠ¡ {short_id(task.id)}: æ¥æº={chat_display}({chat_id}), æ¶ˆæ¯ID={msg_id}")
            grouped_id = payload.get('grouped_id') # è·å– grouped_id
            
            if not chat_id or not msg_id:
                log.error("task_invalid_payload", task_data=task.task_data)
                await self.repo.fail(task.id, "Invalid Payload")
                return

            if group_tasks:
                log.info(f"aggregated_group_tasks", count=len(group_tasks), grouped_id=grouped_id)
            else:
                group_tasks = []
            
            # æ”¶é›†æ‰€æœ‰ç›¸å…³ä»»åŠ¡ï¼ˆå½“å‰ä»»åŠ¡ + åŒç»„ä»»åŠ¡ï¼‰
            all_related_tasks = [task] + group_tasks
            all_message_ids = [msg_id]
            
            # è§£æåŒç»„ä»»åŠ¡çš„ message_id
            if group_tasks:
                for t in group_tasks:
                    try:
                        p = json.loads(t.task_data)
                        if p.get('message_id'):
                            all_message_ids.append(p.get('message_id'))
                    except Exception as ex:
                        logger.warning(f"Failed to parse group task data: {ex}")
            
            # å…³é”®ç‚¹ï¼šä» Telethon è·å–çœŸå®æ¶ˆæ¯å¯¹è±¡ (æ‰¹é‡è·å–)
            # å¦‚æœæ¶ˆæ¯å·²è¿‡æœŸæˆ–è¢«åˆ ï¼Œè¿™é‡Œä¼šè¿”å› None
            messages = await get_messages_queued(self.client, chat_id, ids=all_message_ids)
            
            # è¿‡æ»¤æ‰ None (æœ‰äº›æ¶ˆæ¯å¯èƒ½å·²è¢«åˆ )
            valid_messages = []
            if isinstance(messages, list):
                valid_messages = [m for m in messages if m]
            elif messages:
                    valid_messages = [messages]

            if not valid_messages:
                log.debug("task_source_message_not_found", chat_id=chat_id, message_ids=all_message_ids)
                # æ¶ˆæ¯ä¸å­˜åœ¨ï¼Œæ ‡è®°ä¸ºå¤±è´¥
                await self.repo.fail(task.id, "Source message not found")
                for t in group_tasks:
                    await self.repo.fail(t.id, "Source message not found (Group)")
                return
            
            primary_message = valid_messages[0]
            logger.debug(f"ğŸ“¥ [Worker] æˆåŠŸè·å–æ¶ˆæ¯å¯¹è±¡: ID={primary_message.id}, å†…å®¹é¢„è§ˆ={primary_message.text[:20] if primary_message.text else 'No Text'}")
            
            # === è¿›å…¥å¤„ç†ç®¡é“ ===
            if task.task_type == "process_message":
                # èµ°å®Œæ•´ç®¡é“
                ctx = MessageContext(
                    client=self.client,
                    task_id=task.id,
                    chat_id=chat_id,
                    message_id=msg_id,
                    message_obj=primary_message,
                    # æ³¨å…¥åª’ä½“ç»„ä¿¡æ¯
                    is_group=bool(grouped_id),
                    group_messages=valid_messages if grouped_id else [],
                    related_tasks=group_tasks
                )
                # [å…³é”®] æ³¨å…¥ç›®æ ‡è§„åˆ™ ID (ç”¨äºå†å²ä»»åŠ¡æˆ–è½¬å‘å†å²)
                if payload.get('rule_id'):
                    ctx.metadata['target_rule_id'] = int(payload['rule_id'])
                
                # æ³¨å…¥å†å²ä»»åŠ¡æ ‡è®°
                if payload.get('is_history'):
                    ctx.metadata['is_history'] = True
                # æ‰§è¡Œç®¡é“ (Middleware Chain)
                try:
                    await self.pipeline.execute(ctx)
                except FloodWaitException as e:
                    # æ•è·FloodWaitExceptionï¼Œå°†å…¶è½¬åŒ–ä¸ºæˆ‘ä»¬å®šä¹‰çš„ TransientError
                    await self._retry_group(all_related_tasks, e, log)
                    return
                except TransientError as e:
                    # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                    await self._retry_group(all_related_tasks, e, log)
                    return
                except PermanentError as e:
                    # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                    log.error("task_permanent_error", error=str(e), error_type="Permanent")
                    await self.repo.fail(task.id, str(e))
                    for t in group_tasks:
                        await self.repo.fail(t.id, str(e))
                    return
            
            elif task.task_type == "download_file":
                # ç›´æ¥è°ƒç”¨ä¸‹è½½æœåŠ¡ï¼Œç»•è¿‡ RuleLoader å’Œ Filter
                # è¿™æ˜¯ä¸€ä¸ª"ç‰¹æƒ"ä»»åŠ¡
                if not self.downloader:
                    log.error("downloader_not_initialized")
                    await self.repo.fail(task.id, "Downloader not initialized")
                    return
                
                sub_folder = str(chat_id)
                try:
                    await self.downloader.push_to_queue(primary_message, sub_folder)
                except FloodWaitException as e:
                    # æ•è·FloodWaitExceptionï¼Œå°†å…¶è½¬åŒ–ä¸ºæˆ‘ä»¬å®šä¹‰çš„ TransientError
                    await self._retry_task(task, e, log)
                    return
                except TransientError as e:
                    # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                    await self._retry_task(task, e, log)
                    return
                except PermanentError as e:
                    # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                    log.error("task_permanent_error", error=str(e), error_type="Permanent")
                    await self.repo.fail(task.id, str(e))
                    return
            
            elif task.task_type == "manual_download":
                # å¤„ç†æ‰‹åŠ¨ä¸‹è½½ä»»åŠ¡ï¼Œç›´æ¥è°ƒç”¨DownloadService
                # å¯ä»¥æŒ‡å®šä¸€ä¸ªç‰¹æ®Šçš„ä¸‹è½½ç›®å½•ï¼Œå¦‚ "./downloads/manual"
                if not self.downloader:
                    log.error("downloader_not_initialized")
                    await self.repo.fail(task.id, "Downloader not initialized")
                    return
                
                # ä½¿ç”¨"manual"ä½œä¸ºå­æ–‡ä»¶å¤¹ï¼ŒåŒºåˆ†æ‰‹åŠ¨ä¸‹è½½å’Œè‡ªåŠ¨ä¸‹è½½
                try:
                    path = await self.downloader.push_to_queue(
                        primary_message, 
                        sub_folder="manual"
                    )
                    log.info("manual_download_completed", path=path)
                    
                    # [Scheme 7 Feature] å¦‚æœæœ‰ç›®æ ‡IDï¼Œåˆ™æ‰§è¡Œè½¬å‘
                    target_id = payload.get('target_chat_id')
                    if target_id:
                        try:
                            await send_file_queued(
                                self.client,
                                target_id,
                                path,
                                caption=primary_message.text or ""
                            )
                            log.info(f"manual_forward_completed", target_id=target_id)
                        except Exception as e:
                            log.error(f"manual_forward_failed", target_id=target_id, error=str(e))
                            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªè®°å½•é”™è¯¯ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºä¸‹è½½å·²ç»æˆåŠŸäº†
                except FloodWaitException as e:
                    # æ•è·FloodWaitExceptionï¼Œä½¿ç”¨ç»Ÿä¸€çš„é‡è¯•é€»è¾‘
                    await self._retry_task(task, e, log)
                    return
                except TransientError as e:
                    # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                    await self._retry_task(task, e, log)
                    return
                except PermanentError as e:
                    # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                    log.error("task_permanent_error", error=str(e), error_type="Permanent")
                    await self.repo.fail(task.id, str(e))
                    return
            
            # === ä»»åŠ¡æˆåŠŸ ===
            # [Fix] å¿…é¡»å®Œæˆæ‰€æœ‰ç›¸å…³çš„åª’ä½“ç»„ä»»åŠ¡ï¼Œå¦åˆ™å®ƒä»¬ä¼šè¢«å…¶ä»– Worker é‡å¤è·å–
            await self.repo.complete(task.id)
            if group_tasks:
                for t in group_tasks:
                    await self.repo.complete(t.id)
                logger.debug(f"task_completed_with_group: count={len(group_tasks)}")
            else:
                logger.debug("task_completed")

        except Exception as e:
            if isinstance(e, RescheduleTaskException):
                    # [éé˜»å¡å»¶è¿Ÿå¤„ç†]
                    # æ•è· RescheduleTaskExceptionï¼Œå°†ä»»åŠ¡ä»¥æŒ‡å®šå»¶è¿Ÿé‡æ–°æ”¾å…¥é˜Ÿåˆ—
                    log.info("task_delay_requested", delay_seconds=e.delay_seconds)
                    
                    next_run = datetime.utcnow() + timedelta(seconds=e.delay_seconds)
                    await self.repo.reschedule(task.id, next_run)
                    
                    # å¦‚æœæœ‰åŒç»„ä»»åŠ¡ï¼Œä¹Ÿä¸€èµ·å»¶è¿Ÿ
                    if group_tasks and 'group_tasks' in locals():
                        for t in group_tasks:
                            await self.repo.reschedule(t.id, next_run)
                    return
                    
            if isinstance(e, (FloodWaitException, TransientError)):
                # æ•è·FloodWaitExceptionæˆ–TransientErrorï¼Œä½¿ç”¨ç»Ÿä¸€çš„é‡è¯•é€»è¾‘
                log.warning(f"ä»»åŠ¡é‡åˆ°ç¬æ€é”™è¯¯ï¼Œå°†é‡è¯•: ç±»å‹={type(e).__name__}, é”™è¯¯={str(e)}")
                await self._retry_task(task, e, log)
            elif isinstance(e, PermanentError):
                # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                log.error(f"ä»»åŠ¡æ°¸ä¹…å¤±è´¥: é”™è¯¯={str(e)}, ç±»å‹=Permanent, è§„åˆ™ID={task.rule_id if hasattr(task, 'rule_id') else 'N/A'}", exc_info=True)
                await self.repo.fail(task.id, str(e))
            else:
                from core.helpers.id_utils import get_display_name_async
                chat_display = await get_display_name_async(chat_id)
                log.exception(f"ä»»åŠ¡æœªå¤„ç†é”™è¯¯: é”™è¯¯={str(e)}, ä»»åŠ¡ID={short_id(task.id)}, ä»»åŠ¡ç±»å‹={task.task_type}, æ¥æº={chat_display}({chat_id}), æ¶ˆæ¯ID={msg_id}")
                # è®°å½•å…·ä½“çš„é”™è¯¯ä¿¡æ¯åˆ°æ•°æ®åº“
                await self.repo.fail(task.id, f"Unhandled: {str(e)}")

    # ... Helper methods stay same ...

    def get_performance_stats(self):
        """[Observability] è·å– Worker æ€§èƒ½ä¸è°ƒåº¦ç»Ÿè®¡"""
        # [NEW] ç»Ÿè®¡ä¿¡æ¯æ±‡æ€»
        stats = {
            "current_workers": len(self.workers),
            "queue_depth": self.task_queue.qsize() if self.task_queue else 0,
            "max_concurrency": settings.WORKER_MAX_CONCURRENCY,
        }
        
        # è°ƒåº¦å™¨ç»Ÿè®¡
        if getattr(self, 'dispatcher', None):
            stats["dispatcher"] = self.dispatcher.get_stats()
        
        # ç²¾ç¡®å†…å­˜ç»Ÿè®¡ (psutil)
        try:
            import psutil
            process = psutil.Process()
            mem = psutil.virtual_memory()
            stats["memory"] = {
                "process_rss_mb": round(process.memory_info().rss / (1024*1024), 1),
                "sys_available_mb": round(mem.available / (1024*1024), 1),
                "sys_usage_percent": mem.percent
            }
            if hasattr(mem, 'swap_used'):
                stats["memory"]["swap_used_mb"] = round(mem.swap_used / (1024*1024), 1)
        except Exception:
            pass
            
        return stats

    async def stop(self):
        """ä¼˜é›…åœæ­¢ Worker"""
        logger.info("worker_stopping")
        self.running = False
        if getattr(self, '_monitor_task', None):
            self._monitor_task.cancel()
        if getattr(self, '_lag_monitor_task', None):
            self._lag_monitor_task.cancel()
        
        # Stop dispatcher
        if self.dispatcher:
            await self.dispatcher.stop()
        
        # Cancel all workers
        for task in list(self.workers.keys()):
            task.cancel()
        
        if self.workers:
            await asyncio.gather(*self.workers.keys(), return_exceptions=True)
            
        logger.info("worker_stopped_completely")


    async def _adaptive_sleep(self):
        """è‡ªé€‚åº”ä¼‘çœ ï¼šå¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œé€æ­¥å¢åŠ ä¼‘çœ æ—¶é—´ï¼Œå‡å°‘èµ„æºæ¶ˆè€—"""
        # [Phase 13 Optimization] å¦‚æœè¿›å…¥æ·±åº¦ä¼‘çœ  (current_sleep å·²ç»è¾¾åˆ°è¾ƒå¤§å€¼)ï¼Œè§¦å‘ GC
        if self.current_sleep >= self.max_sleep:
             import gc
             collected = gc.collect()
             if collected > 0:
                 logger.debug(f"[GC] Idle cleanup collected {collected} objects")
                 
        await asyncio.sleep(self.current_sleep)
        if self.current_sleep < self.max_sleep:
            self.current_sleep = min(self.current_sleep + self.sleep_increment, self.max_sleep)

    def _reset_sleep(self):
        """é‡ç½®ä¼‘çœ æ—¶é—´"""
        self.current_sleep = self.min_sleep

    async def _ensure_connected(self):
        """ç¡®ä¿ Telethon å®¢æˆ·ç«¯å·²è¿æ¥"""
        if not self.client.is_connected():
            logger.warning("Client disconnected. Attempting to reconnect...")
            try:
                await self.client.connect()
            except Exception as e:
                logger.error(f"Reconnection failed: {e}")
                # ç­‰å¾…ä¸€ä¼šå„¿å†é‡è¯•ï¼Œé¿å…æ­»å¾ªç¯å†²å‡»
                await asyncio.sleep(5)
    
    def _calculate_backoff(self, retry_count: int) -> float:
        """
        è®¡ç®—æŒ‡æ•°é€€é¿æ—¶é—´
        å…¬å¼: min(base * (factor ^ retries), max) + jitter
        """
        # é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸
        safe_retries = min(retry_count, 10)
        
        delay = settings.RETRY_BASE_DELAY * (settings.RETRY_BACKOFF_FACTOR ** safe_retries)
        
        # æˆªæ–­åˆ°æœ€å¤§å»¶è¿Ÿ
        delay = min(delay, settings.RETRY_MAX_DELAY)
        
        # æ·»åŠ  0-10% çš„éšæœºæŠ–åŠ¨ï¼Œé˜²æ­¢æƒŠç¾¤æ•ˆåº” (Thundering Herd)
        jitter = delay * random.uniform(0, 0.1)
        
        return delay + jitter
    
    async def _retry_task(self, task, error, log):
        """
        å¤„ç†ä»»åŠ¡é‡è¯•ï¼Œæ ¹æ®é”™è¯¯ç±»å‹å’Œé‡è¯•æ¬¡æ•°å†³å®šåç»­æ“ä½œ
        """
        current_retries = task.attempts + 1
        
        # å¦‚æœè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå‡çº§ä¸ºæ°¸ä¹…å¤±è´¥
        if current_retries > settings.MAX_RETRIES:
            log.error("task_max_retries_exceeded", retry_count=current_retries, max_retries=settings.MAX_RETRIES, error=str(error))
            await self.repo.fail(task.id, f"Max retries exceeded: {str(error)}")
            return

        # è®¡ç®—ç­‰å¾…æ—¶é—´
        if isinstance(error, FloodWaitException):
            wait_seconds = error.seconds + 1 # é¢å¤–å¤šç­‰1ç§’ä¿é™©
        else:
            wait_seconds = self._calculate_backoff(current_retries)
            
        next_run = datetime.utcnow() + timedelta(seconds=wait_seconds)
        
        log.warning(
            "task_rescheduled", 
            retry_count=current_retries,
            max_retries=settings.MAX_RETRIES,
            wait_seconds=wait_seconds,
            next_run=next_run.isoformat(),
            error_type=type(error).__name__,
            error=str(error)
        )
        
        # è°ƒç”¨rescheduleæ–¹æ³•ï¼Œæ›´æ–°task.next_retry_atå­—æ®µ
        await self.repo.reschedule(
            task.id, 
            next_run
        )
        
    async def _retry_group(self, tasks, error, log):
        """
        æ‰¹é‡å¤„ç†ä»»åŠ¡é‡è¯•
        """
        for task in tasks:
            await self._retry_task(task, error, log)