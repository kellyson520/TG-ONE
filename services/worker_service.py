import asyncio
import json
import random
import math
from datetime import datetime, timedelta
from core.pipeline import MessageContext
from services.queue_service import FloodWaitException
from core.exceptions import TransientError, PermanentError
from core.config import settings

from core.logging import get_logger, short_id
from services.queue_service import get_messages_queued, send_file_queued
from filters.delay_filter import RescheduleTaskException

logger = get_logger(__name__)

class WorkerService:
    def __init__(self, client, task_repo, pipeline, downloader=None):
        self.client = client
        self.repo = task_repo
        self.pipeline = pipeline
        self.downloader = downloader
        self.running = False
        # åŠ¨æ€ä¼‘çœ ç­–ç•¥é…ç½®
        self.min_sleep = 0.5  # æœ€å°ä¼‘çœ æ—¶é—´ (ç§’)
        self.max_sleep = 30.0  # æœ€å¤§ä¼‘çœ æ—¶é—´ (ç§’)
        self.current_sleep = self.min_sleep
        self.sleep_increment = 1.0  # æ¯æ¬¡å¢åŠ çš„ä¼‘çœ æ—¶é—´

    async def start(self):
        """å¯åŠ¨ Worker æœåŠ¡ (åŠ¨æ€å¹¶å‘æ± )"""
        self.running = True
        logger.info(f"WorkerService å¯åŠ¨ (Min: {settings.WORKER_MIN_CONCURRENCY}, Max: {settings.WORKER_MAX_CONCURRENCY})")
        
        self.workers = {} # task -> worker_id
        
        # å¯åŠ¨åˆå§‹ Workers
        for i in range(settings.WORKER_MIN_CONCURRENCY):
            self._spawn_worker()
            
        # å¯åŠ¨å¼¹æ€§ä¼¸ç¼©ç›‘æ§
        self._monitor_task = asyncio.create_task(self._monitor_scaling(), name="worker_scaling_monitor")
        
        # ä¿æŒä¸»ä»»åŠ¡è¿è¡Œï¼ˆç”¨äºæ¥æ”¶åœæ­¢ä¿¡å·ï¼‰
        while self.running:
            await asyncio.sleep(1)

    def _spawn_worker(self):
        """Spawn a new worker"""
        if len(self.workers) >= settings.WORKER_MAX_CONCURRENCY:
            return

        worker_id = f"worker-{short_id(None, 4)}"
        task = asyncio.create_task(self._worker_loop(worker_id), name=worker_id)
        self.workers[task] = worker_id
        logger.debug(f"Spawned worker {worker_id} (Total: {len(self.workers)})")
        task.add_done_callback(lambda t: self.workers.pop(t, None))

    async def _kill_worker(self):
        """Kill an idle worker (approximate)"""
        if len(self.workers) <= settings.WORKER_MIN_CONCURRENCY:
            return

        # Simple kill: Cancel the last added task
        # Improvement: Cancel idle workers?
        # For now, just pop one randomly or last
        task = list(self.workers.keys())[-1]
        worker_id = self.workers[task]
        task.cancel()
        logger.debug(f"Scaling down: Cancelled worker {worker_id}")

    async def _monitor_scaling(self):
        """Monitor queue depth and scale workers"""
        while self.running:
            try:
                await asyncio.sleep(10) # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
                status = await self.repo.get_queue_status()
                pending = status.get('active_queues', 0)
                current_workers = len(self.workers)
                
                # Scaling Logic
                # å¦‚æœ pending > current_workers * 2ï¼Œæ‰©å®¹
                # å¦‚æœ pending == 0ï¼Œç¼©å®¹
                
                if pending > current_workers * 2 and current_workers < settings.WORKER_MAX_CONCURRENCY:
                    scale_up = min(settings.WORKER_MAX_CONCURRENCY - current_workers, math.ceil(pending / 2))
                    logger.info(f"Scaling UP: Pending={pending}, Workers={current_workers} -> +{scale_up}")
                    for _ in range(scale_up):
                        self._spawn_worker()
                        
                elif pending == 0 and current_workers > settings.WORKER_MIN_CONCURRENCY:
                    logger.info(f"Scaling DOWN: Pending=0, Workers={current_workers} -> -1")
                    await self._kill_worker()
                    
            except Exception as e:
                logger.error(f"Scaling monitor error: {e}")
                await asyncio.sleep(5)

    async def _worker_loop(self, worker_id: str):
        """å•ä¸ª Worker çš„å·¥ä½œå¾ªç¯"""
        logger.debug(f"[{worker_id}] Loop Started")
        
        while self.running:
            task = None
            try:
                # è·å–ä»»åŠ¡
                # æ³¨æ„ï¼šWorker cancel æ—¶ï¼Œè¿™é‡Œå¯èƒ½ä¼šæŠ›å‡º CancelledError
                try:
                      tasks = await self.repo.fetch_next()
                except asyncio.CancelledError:
                     logger.debug(f"[{worker_id}] Cancelled during fetch")
                     raise

                if not tasks:
                    # æ²¡ä»»åŠ¡æ—¶ï¼Œå¢åŠ ä¼‘çœ 
                    await self._adaptive_sleep() 
                    continue
                
                # ç¬¬ä¸€ä¸ªæ˜¯ä¸»ä»»åŠ¡
                task = tasks[0]
                group_tasks = tasks[1:] if len(tasks) > 1 else []
                
                self._reset_sleep() # æœ‰ä»»åŠ¡ï¼Œé‡ç½®ä¼‘çœ 
                
                # ----------------- Worker Logic Copied & Adapted -----------------
                # ç¡®ä¿è¿æ¥æ­£å¸¸ï¼Œé˜²æ­¢ Telethon æ–­è¿å¯¼è‡´å¤„ç†å¤±è´¥
                await self._ensure_connected()
                
                # [å…³é”®] ç»‘å®šä¸Šä¸‹æ–‡ï¼šæ­¤åè¯¥å¾ªç¯å†…çš„æ‰€æœ‰æ—¥å¿—éƒ½ä¼šè‡ªåŠ¨å¸¦ä¸Š task_id
                log = logger.bind(worker_id=worker_id, task_id=task.id, task_type=task.task_type)
                
                # Worker Logic (Simplified for integration)
                await self._process_task_safely(task, log, group_tasks=group_tasks)
                # -----------------------------------------------------------------

            except asyncio.CancelledError:
                logger.debug(f"[{worker_id}] Cancelled")
                break
            except Exception as e:
                logger.error(f"[{worker_id}] Loop Error: {e}")
                await asyncio.sleep(1)

    async def _process_task_safely(self, task, log, group_tasks: list = None):
        """å¤„ç†åŸºç¡€ä»»åŠ¡çš„å®‰å…¨å°è£…ï¼Œæ”¯æŒä¼ å…¥é¢„å…ˆé”å®šçš„åª’ä½“ç»„ä»»åŠ¡"""
        try:
            payload = json.loads(task.task_data)
            
            # [Optimization] å¤„ç†ä¸éœ€è¦é¢„å–æ¶ˆæ¯çš„ä»»åŠ¡ç±»å‹
            if task.task_type == "message_delete":
                chat_id = payload.get('chat_id')
                message_ids = payload.get('message_ids', [])
                if not chat_id or not message_ids:
                    log.error("delete_task_invalid_payload", payload=payload)
                    await self.repo.fail(task.id, "Invalid Delete Payload")
                    return
                
                try:
                    log.info(f"ğŸ—‘ï¸ [Worker] æ‰§è¡Œåˆ é™¤æ¶ˆæ¯ä»»åŠ¡: Chat={chat_id}, IDs={message_ids}")
                    await self.client.delete_messages(chat_id, message_ids)
                    await self.repo.complete(task.id)
                    return
                except Exception as e:
                    log.error(f"delete_messages_failed", error=str(e))
                    await self._retry_task(task, e, log)
                    return

            if task.task_type == "custom_task":
                log.info(f"âš™ï¸ [Worker] å¤„ç†è‡ªå®šä¹‰ä»»åŠ¡: {payload.get('action')}")
                # TODO: ä»¥åå¯æ‰©å±•åŸºäº action çš„è·¯ç”±
                await self.repo.complete(task.id)
                return

            # --- ä»¥ä¸‹æ˜¯éœ€è¦è·å–åŸå§‹æ¶ˆæ¯çš„ä»»åŠ¡ç±»å‹ (process_message, download_file, manual_download) ---
            chat_id = payload.get('chat_id')
            msg_id = payload.get('message_id')
            
            # [ä¼˜åŒ–] è·å–èŠå¤©æ˜¾ç¤ºåç§°
            from core.helpers.id_utils import get_display_name_async
            chat_display = await get_display_name_async(chat_id)
            
            log.info(f"ğŸ”„ [Worker] å¼€å§‹å¤„ç†ä»»åŠ¡ {short_id(task.id)}: æ¥æº={chat_display}({chat_id}), æ¶ˆæ¯ID={msg_id}")
            grouped_id = payload.get('grouped_id') # è·å– grouped_id
            
            if not chat_id or not msg_id:
                log.error("task_invalid_payload", task_data=task.task_data)
                await self.repo.fail(task.id, "Invalid Payload")
                return

            if group_tasks:
                log.info(f"aggregated_group_tasks", count=len(group_tasks), grouped_id=grouped_id)
            else:
                group_tasks = []
            
            # æ”¶é›†æ‰€æœ‰ç›¸å…³ä»»åŠ¡ï¼ˆå½“å‰ä»»åŠ¡ + åŒç»„ä»»åŠ¡ï¼‰
            all_related_tasks = [task] + group_tasks
            all_message_ids = [msg_id]
            
            # è§£æåŒç»„ä»»åŠ¡çš„ message_id
            if group_tasks:
                for t in group_tasks:
                    try:
                        p = json.loads(t.task_data)
                        if p.get('message_id'):
                            all_message_ids.append(p.get('message_id'))
                    except Exception as ex:
                        logger.warning(f"Failed to parse group task data: {ex}")
            
            # å…³é”®ç‚¹ï¼šä» Telethon è·å–çœŸå®æ¶ˆæ¯å¯¹è±¡ (æ‰¹é‡è·å–)
            # å¦‚æœæ¶ˆæ¯å·²è¿‡æœŸæˆ–è¢«åˆ ï¼Œè¿™é‡Œä¼šè¿”å› None
            messages = await get_messages_queued(self.client, chat_id, ids=all_message_ids)
            
            # è¿‡æ»¤æ‰ None (æœ‰äº›æ¶ˆæ¯å¯èƒ½å·²è¢«åˆ )
            valid_messages = []
            if isinstance(messages, list):
                valid_messages = [m for m in messages if m]
            elif messages:
                    valid_messages = [messages]

            if not valid_messages:
                log.debug("task_source_message_not_found", chat_id=chat_id, message_ids=all_message_ids)
                # æ¶ˆæ¯ä¸å­˜åœ¨ï¼Œæ ‡è®°ä¸ºå¤±è´¥
                await self.repo.fail(task.id, "Source message not found")
                for t in group_tasks:
                    await self.repo.fail(t.id, "Source message not found (Group)")
                return
            
            primary_message = valid_messages[0]
            log.info(f"ğŸ“¥ [Worker] æˆåŠŸè·å–æ¶ˆæ¯å¯¹è±¡: ID={primary_message.id}, å†…å®¹é¢„è§ˆ={primary_message.text[:20] if primary_message.text else 'No Text'}")
            
            # === è¿›å…¥å¤„ç†ç®¡é“ ===
            if task.task_type == "process_message":
                # èµ°å®Œæ•´ç®¡é“
                ctx = MessageContext(
                    client=self.client,
                    task_id=task.id,
                    chat_id=chat_id,
                    message_id=msg_id,
                    message_obj=primary_message,
                    # æ³¨å…¥åª’ä½“ç»„ä¿¡æ¯
                    is_group=bool(grouped_id),
                    group_messages=valid_messages if grouped_id else [],
                    related_tasks=group_tasks
                )
                # [å…³é”®] æ³¨å…¥ç›®æ ‡è§„åˆ™ ID (ç”¨äºå†å²ä»»åŠ¡æˆ–è½¬å‘å†å²)
                if payload.get('rule_id'):
                    ctx.metadata['target_rule_id'] = int(payload['rule_id'])
                
                # æ³¨å…¥å†å²ä»»åŠ¡æ ‡è®°
                if payload.get('is_history'):
                    ctx.metadata['is_history'] = True
                # æ‰§è¡Œç®¡é“ (Middleware Chain)
                try:
                    await self.pipeline.execute(ctx)
                except FloodWaitException as e:
                    # æ•è·FloodWaitExceptionï¼Œå°†å…¶è½¬åŒ–ä¸ºæˆ‘ä»¬å®šä¹‰çš„ TransientError
                    await self._retry_group(all_related_tasks, e, log)
                    return
                except TransientError as e:
                    # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                    await self._retry_group(all_related_tasks, e, log)
                    return
                except PermanentError as e:
                    # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                    log.error("task_permanent_error", error=str(e), error_type="Permanent")
                    await self.repo.fail(task.id, str(e))
                    for t in group_tasks:
                        await self.repo.fail(t.id, str(e))
                    return
            
            elif task.task_type == "download_file":
                # ç›´æ¥è°ƒç”¨ä¸‹è½½æœåŠ¡ï¼Œç»•è¿‡ RuleLoader å’Œ Filter
                # è¿™æ˜¯ä¸€ä¸ª"ç‰¹æƒ"ä»»åŠ¡
                if not self.downloader:
                    log.error("downloader_not_initialized")
                    await self.repo.fail(task.id, "Downloader not initialized")
                    return
                
                sub_folder = str(chat_id)
                try:
                    await self.downloader.push_to_queue(primary_message, sub_folder)
                except FloodWaitException as e:
                    # æ•è·FloodWaitExceptionï¼Œå°†å…¶è½¬åŒ–ä¸ºæˆ‘ä»¬å®šä¹‰çš„ TransientError
                    await self._retry_task(task, e, log)
                    return
                except TransientError as e:
                    # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                    await self._retry_task(task, e, log)
                    return
                except PermanentError as e:
                    # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                    log.error("task_permanent_error", error=str(e), error_type="Permanent")
                    await self.repo.fail(task.id, str(e))
                    return
            
            elif task.task_type == "manual_download":
                # å¤„ç†æ‰‹åŠ¨ä¸‹è½½ä»»åŠ¡ï¼Œç›´æ¥è°ƒç”¨DownloadService
                # å¯ä»¥æŒ‡å®šä¸€ä¸ªç‰¹æ®Šçš„ä¸‹è½½ç›®å½•ï¼Œå¦‚ "./downloads/manual"
                if not self.downloader:
                    log.error("downloader_not_initialized")
                    await self.repo.fail(task.id, "Downloader not initialized")
                    return
                
                # ä½¿ç”¨"manual"ä½œä¸ºå­æ–‡ä»¶å¤¹ï¼ŒåŒºåˆ†æ‰‹åŠ¨ä¸‹è½½å’Œè‡ªåŠ¨ä¸‹è½½
                try:
                    path = await self.downloader.push_to_queue(
                        primary_message, 
                        sub_folder="manual"
                    )
                    log.info("manual_download_completed", path=path)
                    
                    # [Scheme 7 Feature] å¦‚æœæœ‰ç›®æ ‡IDï¼Œåˆ™æ‰§è¡Œè½¬å‘
                    target_id = payload.get('target_chat_id')
                    if target_id:
                        try:
                            await send_file_queued(
                                self.client,
                                target_id,
                                path,
                                caption=primary_message.text or ""
                            )
                            log.info(f"manual_forward_completed", target_id=target_id)
                        except Exception as e:
                            log.error(f"manual_forward_failed", target_id=target_id, error=str(e))
                            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªè®°å½•é”™è¯¯ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºä¸‹è½½å·²ç»æˆåŠŸäº†
                except FloodWaitException as e:
                    # æ•è·FloodWaitExceptionï¼Œä½¿ç”¨ç»Ÿä¸€çš„é‡è¯•é€»è¾‘
                    await self._retry_task(task, e, log)
                    return
                except TransientError as e:
                    # å¤„ç†è‡ªå®šä¹‰ç¬æ€é”™è¯¯
                    await self._retry_task(task, e, log)
                    return
                except PermanentError as e:
                    # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                    log.error("task_permanent_error", error=str(e), error_type="Permanent")
                    await self.repo.fail(task.id, str(e))
                    return
            
            # === ä»»åŠ¡æˆåŠŸ ===
            # [Fix] å¿…é¡»å®Œæˆæ‰€æœ‰ç›¸å…³çš„åª’ä½“ç»„ä»»åŠ¡ï¼Œå¦åˆ™å®ƒä»¬ä¼šè¢«å…¶ä»– Worker é‡å¤è·å–
            await self.repo.complete(task.id)
            if group_tasks:
                for t in group_tasks:
                    await self.repo.complete(t.id)
                log.info(f"task_completed_with_group", count=len(group_tasks))
            else:
                log.info("task_completed")

        except Exception as e:
            if isinstance(e, RescheduleTaskException):
                    # [éé˜»å¡å»¶è¿Ÿå¤„ç†]
                    # æ•è· RescheduleTaskExceptionï¼Œå°†ä»»åŠ¡ä»¥æŒ‡å®šå»¶è¿Ÿé‡æ–°æ”¾å…¥é˜Ÿåˆ—
                    log.info("task_delay_requested", delay_seconds=e.delay_seconds)
                    
                    next_run = datetime.utcnow() + timedelta(seconds=e.delay_seconds)
                    await self.repo.reschedule(task.id, next_run)
                    
                    # å¦‚æœæœ‰åŒç»„ä»»åŠ¡ï¼Œä¹Ÿä¸€èµ·å»¶è¿Ÿ
                    if group_tasks and 'group_tasks' in locals():
                        for t in group_tasks:
                            await self.repo.reschedule(t.id, next_run)
                    return
                    
            if isinstance(e, (FloodWaitException, TransientError)):
                # æ•è·FloodWaitExceptionæˆ–TransientErrorï¼Œä½¿ç”¨ç»Ÿä¸€çš„é‡è¯•é€»è¾‘
                log.warning(f"ä»»åŠ¡é‡åˆ°ç¬æ€é”™è¯¯ï¼Œå°†é‡è¯•: ç±»å‹={type(e).__name__}, é”™è¯¯={str(e)}")
                await self._retry_task(task, e, log)
            elif isinstance(e, PermanentError):
                # å¤„ç†è‡ªå®šä¹‰æ°¸ä¹…é”™è¯¯
                log.error(f"ä»»åŠ¡æ°¸ä¹…å¤±è´¥: é”™è¯¯={str(e)}, ç±»å‹=Permanent, è§„åˆ™ID={task.rule_id if hasattr(task, 'rule_id') else 'N/A'}", exc_info=True)
                await self.repo.fail(task.id, str(e))
            else:
                from core.helpers.id_utils import get_display_name_async
                chat_display = await get_display_name_async(chat_id)
                log.exception(f"ä»»åŠ¡æœªå¤„ç†é”™è¯¯: é”™è¯¯={str(e)}, ä»»åŠ¡ID={short_id(task.id)}, ä»»åŠ¡ç±»å‹={task.task_type}, æ¥æº={chat_display}({chat_id}), æ¶ˆæ¯ID={msg_id}")
                # è®°å½•å…·ä½“çš„é”™è¯¯ä¿¡æ¯åˆ°æ•°æ®åº“
                await self.repo.fail(task.id, f"Unhandled: {str(e)}")

    # ... Helper methods stay same ...

    async def stop(self):
        """ä¼˜é›…åœæ­¢ Worker"""
        logger.info("worker_stopping")
        self.running = False
        if getattr(self, '_monitor_task', None):
            self._monitor_task.cancel()
        
        # Cancel all workers
        for task in list(self.workers.keys()):
            task.cancel()
        
        if self.workers:
            await asyncio.gather(*self.workers.keys(), return_exceptions=True)
            
        logger.info("worker_stopped_completely")


    async def _adaptive_sleep(self):
        """è‡ªé€‚åº”ä¼‘çœ ï¼šå¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œé€æ­¥å¢åŠ ä¼‘çœ æ—¶é—´ï¼Œå‡å°‘èµ„æºæ¶ˆè€—"""
        # [Phase 13 Optimization] å¦‚æœè¿›å…¥æ·±åº¦ä¼‘çœ  (current_sleep å·²ç»è¾¾åˆ°è¾ƒå¤§å€¼)ï¼Œè§¦å‘ GC
        if self.current_sleep >= self.max_sleep:
             import gc
             collected = gc.collect()
             if collected > 0:
                 logger.debug(f"[GC] Idle cleanup collected {collected} objects")
                 
        await asyncio.sleep(self.current_sleep)
        if self.current_sleep < self.max_sleep:
            self.current_sleep = min(self.current_sleep + self.sleep_increment, self.max_sleep)

    def _reset_sleep(self):
        """é‡ç½®ä¼‘çœ æ—¶é—´"""
        self.current_sleep = self.min_sleep

    async def _ensure_connected(self):
        """ç¡®ä¿ Telethon å®¢æˆ·ç«¯å·²è¿æ¥"""
        if not self.client.is_connected():
            logger.warning("Client disconnected. Attempting to reconnect...")
            try:
                await self.client.connect()
            except Exception as e:
                logger.error(f"Reconnection failed: {e}")
                # ç­‰å¾…ä¸€ä¼šå„¿å†é‡è¯•ï¼Œé¿å…æ­»å¾ªç¯å†²å‡»
                await asyncio.sleep(5)
    
    def _calculate_backoff(self, retry_count: int) -> float:
        """
        è®¡ç®—æŒ‡æ•°é€€é¿æ—¶é—´
        å…¬å¼: min(base * (factor ^ retries), max) + jitter
        """
        # é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸
        safe_retries = min(retry_count, 10)
        
        delay = settings.RETRY_BASE_DELAY * (settings.RETRY_BACKOFF_FACTOR ** safe_retries)
        
        # æˆªæ–­åˆ°æœ€å¤§å»¶è¿Ÿ
        delay = min(delay, settings.RETRY_MAX_DELAY)
        
        # æ·»åŠ  0-10% çš„éšæœºæŠ–åŠ¨ï¼Œé˜²æ­¢æƒŠç¾¤æ•ˆåº” (Thundering Herd)
        jitter = delay * random.uniform(0, 0.1)
        
        return delay + jitter
    
    async def _retry_task(self, task, error, log):
        """
        å¤„ç†ä»»åŠ¡é‡è¯•ï¼Œæ ¹æ®é”™è¯¯ç±»å‹å’Œé‡è¯•æ¬¡æ•°å†³å®šåç»­æ“ä½œ
        """
        current_retries = task.retry_count + 1
        
        # å¦‚æœè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå‡çº§ä¸ºæ°¸ä¹…å¤±è´¥
        if current_retries > settings.MAX_RETRIES:
            log.error("task_max_retries_exceeded", retry_count=current_retries, max_retries=settings.MAX_RETRIES, error=str(error))
            await self.repo.fail(task.id, f"Max retries exceeded: {str(error)}")
            return

        # è®¡ç®—ç­‰å¾…æ—¶é—´
        if isinstance(error, FloodWaitException):
            wait_seconds = error.seconds + 1 # é¢å¤–å¤šç­‰1ç§’ä¿é™©
        else:
            wait_seconds = self._calculate_backoff(current_retries)
            
        next_run = datetime.utcnow() + timedelta(seconds=wait_seconds)
        
        log.warning(
            "task_rescheduled", 
            retry_count=current_retries,
            max_retries=settings.MAX_RETRIES,
            wait_seconds=wait_seconds,
            next_run=next_run.isoformat(),
            error_type=type(error).__name__,
            error=str(error)
        )
        
        # è°ƒç”¨rescheduleæ–¹æ³•ï¼Œæ›´æ–°task.next_retry_atå­—æ®µ
        await self.repo.reschedule(
            task.id, 
            next_run
        )
        
    async def _retry_group(self, tasks, error, log):
        """
        æ‰¹é‡å¤„ç†ä»»åŠ¡é‡è¯•
        """
        for task in tasks:
            await self._retry_task(task, error, log)