import logging
import asyncio
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any
from core.config import settings

logger = logging.getLogger(__name__)

class SystemService:
    """
    Service for system-wide configurations and state.
    """
    def __init__(self):
        pass # Settings managed via core.config.settings and config_service
        
    @property
    def container(self):
        from core.container import container
        return container

    async def is_maintenance_mode(self) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å¤„äºç»´æŠ¤æ¨¡å¼"""
        try:
            from models.system import SystemConfiguration
            from sqlalchemy import select
            
            async with self.container.db.get_session() as s:
                result = await s.execute(select(SystemConfiguration).filter_by(key="maintenance_mode"))
                config = result.scalar_one_or_none()
                return config and config.value.lower() == "true"
        except Exception as e:
            logger.error(f"Failed to check maintenance mode: {e}")
            return False

    async def set_maintenance_mode(self, enabled: bool) -> bool:
        """è®¾ç½®ç³»ç»Ÿç»´æŠ¤æ¨¡å¼"""
        try:
            from models.system import SystemConfiguration
            from sqlalchemy import select
            
            async with self.container.db.get_session() as s:
                result = await s.execute(select(SystemConfiguration).filter_by(key="maintenance_mode"))
                config = result.scalar_one_or_none()
                
                value = "true" if enabled else "false"
                if config:
                    config.value = value
                else:
                    config = SystemConfiguration(key="maintenance_mode", value=value)
                    s.add(config)
                
                await s.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to set maintenance mode: {e}")
            return False
        
    def get_allow_registration(self) -> bool:
        return settings.ALLOW_REGISTRATION
        
    async def set_allow_registration(self, value: bool):
        from services.config_service import config_service
        # æŒä¹…åŒ–åˆ°æ•°æ®åº“
        await config_service.set("ALLOW_REGISTRATION", value, data_type="boolean")
        # åŒæ­¥æ›´æ–°å†…å­˜ä¸­çš„ settings å¯¹è±¡ï¼Œç¡®ä¿åç»­è¯·æ±‚ç«‹å³ç”Ÿæ•ˆ
        settings.ALLOW_REGISTRATION = value
        logger.info(f"Registration allowed set to: {value}")

    async def get_system_configurations(self, limit: int = 20):
        """è·å–ç³»ç»Ÿé…ç½®åˆ—è¡¨"""
        try:
            from models.system import SystemConfiguration
            from sqlalchemy import select
            async with self.container.db.get_session() as s:
                result = await s.execute(select(SystemConfiguration).limit(limit))
                return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to get system configurations: {e}")
            return []

    async def get_error_logs(self, limit: int = 5):
        """è·å–æœ€è¿‘çš„æ¶ˆæ¯é”™è¯¯æ—¥å¿—"""
        try:
            from models.models import ErrorLog
            from sqlalchemy import select, desc
            async with self.container.db.get_session() as s:
                result = await s.execute(
                    select(ErrorLog).order_by(desc(ErrorLog.created_at)).limit(limit)
                )
                return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to get error logs: {e}")
            return []

    def get_logs(self, lines: int = 50, log_type: str = "app") -> str:
        """è¯»å–ç³»ç»Ÿæ—¥å¿—æœ€è¿‘ N è¡Œ (ä¼˜åŒ–ç‰ˆï¼Œé˜²æ­¢å¤§æ–‡ä»¶ OOM)"""
        if log_type == "error":
            log_file = settings.LOG_DIR / "error.log"
        else:
            log_file = settings.LOG_DIR / "app.log"
            
        if not log_file.exists():
            return "Log file not found."
            
        try:
            file_size = log_file.stat().st_size
            # ä¼˜åŒ–ç­–ç•¥ï¼šä»…è¯»å–æœ«å°¾ 200KB (çº¦ 1000-2000 è¡Œ)ï¼Œè¶³ä»¥è¦†ç›– N=50/100/200 çš„è¯·æ±‚
            # é¿å… readlines() åŠ è½½å‡ ç™¾ MB ç”šè‡³ GB çš„å…¨é‡æ—¥å¿—å¯¼è‡´ OOM
            read_bytes = min(file_size, 200 * 1024) 
            
            with open(log_file, "rb") as f:
                if file_size > read_bytes:
                    f.seek(file_size - read_bytes)
                
                content = f.read().decode("utf-8", errors="ignore")
                all_lines = content.splitlines()
                
                # å¦‚æœæ–‡ä»¶è¢«æˆªæ–­äº†ï¼Œç¬¬ä¸€è¡Œå¯èƒ½ä¸å®Œæ•´ï¼Œä¸¢å¼ƒå®ƒ
                if file_size > read_bytes and len(all_lines) > 1:
                    all_lines = all_lines[1:]
                
                return "\n".join(all_lines[-lines:])
        except Exception as e:
            logger.error(f"Read log failed: {e}")
            return f"Failed to read log: {e}"
    
    def get_log_file_path(self, log_type: str = "app") -> Optional[Path]:
        """è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„ (ç”¨äºä¸‹è½½)"""
        if log_type == "error":
            log_file = settings.LOG_DIR / "error.log"
        else:
            log_file = settings.LOG_DIR / "app.log"
        if log_file.exists():
            return log_file
        return None

    async def backup_database(self) -> Dict:
        """
        å¼‚æ­¥æ‰§è¡Œæ•°æ®åº“å¤‡ä»½
        """
        try:
            from repositories.backup import backup_database as _backup
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥å¤‡ä»½æ“ä½œ
            loop = asyncio.get_running_loop()
            backup_path = await loop.run_in_executor(None, _backup)
            
            if backup_path and os.path.exists(backup_path):
                size = os.path.getsize(backup_path) / (1024 * 1024)
                return {
                    "success": True, 
                    "path": backup_path, 
                    "size_mb": size
                }
            else:
                return {
                    "success": False, 
                    "error": "Backup function returned empty path"
                }
        except Exception as e:
            logger.error(f"Backup failed: {e}")
            return {
                "success": False, 
                "error": str(e)
            }

    async def get_backup_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯"""
        backup_dir = settings.BACKUP_DIR
        try:
            if not backup_dir.exists():
                return {"last_backup": "ä»æœª", "backup_count": 0}
            
            backups = sorted(
                [f for f in backup_dir.iterdir() if f.suffix == ".bak" or f.suffix == ".zip"],
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            
            if not backups:
                return {"last_backup": "ä»æœª", "backup_count": 0}
            
            last_backup_time = datetime.fromtimestamp(backups[0].stat().st_mtime).strftime("%Y-%m-%d %H:%M")
            return {
                "last_backup": last_backup_time,
                "backup_count": len(backups)
            }
        except Exception as e:
            logger.error(f"Failed to get backup info: {e}")
            return {"last_backup": "æœªçŸ¥", "backup_count": 0}

    async def get_cleanup_info(self) -> Dict[str, Any]:
        """è·å–ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜å¤§å°ä¿¡æ¯"""
        try:
            temp_dir = settings.TEMP_DIR
            log_dir = settings.LOG_DIR
            
            def get_dir_size(path: Path) -> float:
                if not path.exists(): return 0
                return sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / (1024 * 1024)

            tmp_size = get_dir_size(temp_dir)
            log_size = get_dir_size(log_dir)
            
            # è·å–å»é‡ç¼“å­˜å¤§å°
            from services.dedup.engine import smart_deduplicator
            dedup_stats = smart_deduplicator.get_stats()
            
            return {
                "tmp_size_mb": f"{round(tmp_size, 1)}MB",
                "log_size_mb": f"{round(log_size, 1)}MB",
                "dedup_cache_size": f"{round(dedup_stats.get('memory_mb', 0), 1)}MB" if 'memory_mb' in dedup_stats else f"{dedup_stats.get('cached_signatures', 0)}æ¡"
            }
        except Exception as e:
            logger.error(f"Failed to get cleanup info: {e}")
            return {"tmp_size_mb": "0MB", "log_size_mb": "0MB", "dedup_cache_size": "0æ¡"}

    async def restore_database(self, backup_path: str) -> Dict:
        """
        å¼‚æ­¥æ¢å¤æ•°æ®åº“å¤‡ä»½
        """
        try:
            if not os.path.exists(backup_path):
                return {"success": False, "error": "Backup file not found"}

            if settings.DATABASE_URL.startswith("sqlite"):
                path_str = settings.DATABASE_URL.split("///")[-1]
                db_path = str(Path(path_str).resolve())
                
                # åœæ­¢æ‰€æœ‰æ´»åŠ¨è¿æ¥å¯ä»¥é€šè¿‡å…³é—­å¼•æ“æˆ–æ˜¯è®©ç”¨æˆ·æ‰‹åŠ¨é‡å¯
                # è¿™é‡Œç®€å•å¤„ç†ï¼šç›´æ¥æ–‡ä»¶å¤åˆ¶ï¼ˆæ³¨æ„ï¼šè¿™åœ¨æ´»è·ƒè¿æ¥ä¸‹å¯èƒ½å¯¼è‡´æŸåæˆ–é”é”™è¯¯ï¼‰
                import shutil
                shutil.copy2(backup_path, db_path)
                return {"success": True}
            else:
                return {"success": False, "error": "Only SQLite restore is supported"}
        except Exception as e:
            logger.error(f"Restore failed: {e}")
            return {"success": False, "error": str(e)}

    async def run_db_optimization(self, deep: bool = False) -> Dict[str, Any]:
        """è¿è¡Œæ•°æ®åº“ä¼˜åŒ– (SQLite PRAGMA optimize/VACUUM)"""
        try:
            from core.db_factory import async_vacuum_database, async_analyze_database
            import time
            
            start_time = time.time()
            
            if deep:
                # Deep æ¨¡å¼ï¼šæ‰§è¡Œ VACUUM (æ— æ³•åœ¨äº‹åŠ¡å†…è¿è¡Œï¼Œä½¿ç”¨ factory æä¾›çš„ç‹¬ç«‹è¿æ¥)
                logger.info("Executing deep database optimization (VACUUM)...")
                await async_vacuum_database()
            else:
                # Standard æ¨¡å¼ï¼šæ‰§è¡Œ PRAGMA optimize
                from core.container import container
                from sqlalchemy import text
                async with container.db.get_session() as session:
                    await session.execute(text("PRAGMA optimize;"))
                    await session.commit()
            
            # æ— è®ºå“ªç§æ¨¡å¼éƒ½æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            await async_analyze_database()
            
            # æ¸…ç†æ—§æ—¥å¿— (æš‚å®š 30 å¤©)
            deleted_logs = 0
            if deep:
                from models.models import async_cleanup_old_logs
                deleted_logs = await async_cleanup_old_logs(30)
            
            return {
                "success": True,
                "duration": round(time.time() - start_time, 2),
                "deep": deep,
                "deleted_logs": deleted_logs,
                "message": f"Database optimization ({'VACUUM' if deep else 'PRAGMA optimize'} + ANALYZE) completed."
            }
        except Exception as e:
            logger.error(f"DB Optimization failed: {e}")
            return {"success": False, "error": str(e)}

    async def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
        import psutil
        from version import get_version
        from services.update_service import update_service
        
        try:
            process = psutil.Process()
            with process.oneshot():
                cpu_percent = process.cpu_percent()
                memory_info = process.memory_info()
                create_time = process.create_time()
                
            uptime_seconds = time.time() - create_time
            uptime_str = self._format_uptime(uptime_seconds)
            
            # ç‰ˆæœ¬ä¿¡æ¯
            version_str = get_version()
            git_ver = await update_service.get_current_version()
            if git_ver:
                version_str += f" ({git_ver})"
            elif not update_service._is_git_repo:
                version_str += " (é Git ä»“åº“)"

            disk = psutil.disk_usage(str(settings.BASE_DIR))
            
            return {
                "cpu_percent": cpu_percent,
                "memory_percent": round(process.memory_percent(), 1),
                "memory_used_mb": round(memory_info.rss / 1024 / 1024, 1),
                "disk_percent": disk.percent,
                "uptime": uptime_str,
                "version": version_str
            }
        except Exception as e:
            logger.error(f"Get system status failed: {e}")
            return {
                "cpu_percent": 0,
                "memory_percent": 0,
                "memory_used_mb": 0,
                "disk_percent": 0,
                "uptime": "Unknown",
                "version": "Unknown"
            }

    def _format_uptime(self, seconds):
        m, s = divmod(seconds, 60)
        h, m = divmod(m, 60)
        d, h = divmod(h, 24)
        if d > 0:
            return f"{int(d)}å¤© {int(h)}å°æ—¶"
        elif h > 0:
            return f"{int(h)}å°æ—¶ {int(m)}åˆ†"
        else:
            return f"{int(m)}åˆ† {int(s)}ç§’"

    async def get_advanced_stats(self) -> Dict[str, Any]:
        """è·å–æ·±åº¦æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯ (æ•´åˆå®˜æ–¹APIå’ŒHLL)"""
        from sqlalchemy import func, select
        from models.models import ForwardRule, Chat, MediaSignature, ErrorLog
        from services.network.api_optimization import get_api_optimizer
        from core.algorithms.hll import GlobalHLL
        
        async with self.container.db.get_session() as s:
            # 1. åŸºç¡€è§„åˆ™ç»Ÿè®¡
            stmt_count = select(func.count()).select_from(ForwardRule)
            rule_count = (await s.execute(stmt_count)).scalar() or 0
            
            stmt_active = select(func.count()).select_from(ForwardRule).where(ForwardRule.enable_rule.is_(True))
            active_rules = (await s.execute(stmt_active)).scalar() or 0
            
            # 2. å…¶ä»–åŸºç¡€è®¡æ•°
            chat_count = (await s.execute(select(func.count()).select_from(Chat))).scalar() or 0
            media_count = (await s.execute(select(func.count()).select_from(MediaSignature))).scalar() or 0
            error_count = (await s.execute(select(func.count()).select_from(ErrorLog))).scalar() or 0
            total_processed = (await s.execute(select(func.sum(ForwardRule.message_count)).select_from(ForwardRule))).scalar() or 0
            
            # 3. HLL ä¼°å€¼
            unique_today = 0
            hll = GlobalHLL.get_hll("unique_messages_today")
            if hll: unique_today = hll.count()
            
            # 4. å®˜æ–¹ API å®æ—¶ç»Ÿè®¡
            realtime_data = {}
            api_optimizer = get_api_optimizer()
            if api_optimizer:
                active_chat_ids = (await s.execute(select(Chat.telegram_chat_id).where(Chat.is_active == True).limit(5))).scalars().all()
                if active_chat_ids:
                    realtime_data = await api_optimizer.get_multiple_chat_statistics([cid for cid in active_chat_ids if cid])
            
            return {
                "base": {
                    "total_rules": rule_count,
                    "active_rules": active_rules,
                    "chat_count": chat_count,
                    "media_count": media_count,
                    "error_count": error_count,
                    "total_processed": total_processed,
                },
                "unique_today": unique_today,
                "realtime": realtime_data,
                "api_enabled": api_optimizer is not None
            }

    async def run_anomaly_detection(self) -> Dict[str, Any]:
        """æ‰«ææ¶ˆæ¯æ—¥å¿—ä¸­çš„å¼‚å¸¸æ¨¡å¼"""
        try:
            from models.models import RuleLog
            from sqlalchemy import select, func
            
            async with self.container.db.get_session() as session:
                # æ£€æŸ¥æœ€è¿‘ 1 å°æ—¶çš„å¤±è´¥æ•°
                one_hour_ago = datetime.now() - timedelta(hours=1)
                stmt_total = select(func.count(RuleLog.id)).where(RuleLog.created_at >= one_hour_ago)
                total_recent = (await session.execute(stmt_total)).scalar() or 0
                
                if total_recent == 0:
                    return {"status": "normal", "message": "æœ€è¿‘ 1 å°æ—¶ç”±äºæ— è½¬å‘æ•°æ®ï¼Œæš‚æœªå‘ç°å¼‚å¸¸ã€‚"}
                
                stmt_err = select(func.count(RuleLog.id)).where(
                    RuleLog.status == "error",
                    RuleLog.created_at >= one_hour_ago
                )
                recent_errors = (await session.execute(stmt_err)).scalar() or 0
                error_rate = (recent_errors / total_recent) * 100
                
                if error_rate > 30: # å¤±è´¥ç‡è¶…è¿‡ 30%
                    return {
                        "status": "warning", 
                        "message": f"ğŸš¨ [é«˜å¤±è´¥ç‡å‘Šè­¦] æœ€è¿‘ 1 å°æ—¶å¤±è´¥ç‡è¾¾ {error_rate:.1f}% ({recent_errors}/{total_recent})ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç›®æ ‡é¢‘é“æƒé™ã€‚"
                    }
                
                return {"status": "healthy", "message": "âœ… æœªæ£€æµ‹åˆ°å¤§è§„æ¨¡è½¬å‘å¼‚å¸¸ï¼Œç³»ç»Ÿè¿è¡Œå¹³ç¨³ã€‚"}
        except Exception as e:
            logger.error(f"Anomaly detection failed: {e}")
            return {"status": "error", "message": str(e)}

    async def export_analytics_csv(self) -> Optional[Path]:
        """å¯¼å‡ºæœ€è¿‘ 24 å°æ—¶çš„è½¬å‘ç»Ÿè®¡åˆ° CSV"""
        try:
            import csv
            from models.models import RuleLog
            from sqlalchemy import select
            
            os.makedirs(settings.TEMP_DIR, exist_ok=True)
            export_path = settings.TEMP_DIR / f"analytics_export_{int(time.time())}.csv"
            one_day_ago = datetime.now() - timedelta(days=1)
            
            async with self.container.db.get_session() as session:
                stmt = select(RuleLog).where(RuleLog.created_at >= one_day_ago).order_by(RuleLog.created_at.desc()).limit(1000)
                logs = (await session.execute(stmt)).scalars().all()
                
                if not logs:
                    return None
                    
                with open(export_path, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    writer.writerow(["ID", "Time", "RuleID", "Type", "Status", "Latency"])
                    for log in logs:
                        writer.writerow([
                            log.id, 
                            log.created_at.strftime("%Y-%m-%d %H:%M:%S"),
                            log.rule_id,
                            log.message_type,
                            log.status,
                            f"{log.latency:.3f}s" if log.latency else "0s"
                        ])
                
                return export_path
        except Exception as e:
            logger.error(f"Export CSV failed: {e}")
            return None

    async def get_db_pragma_info(self) -> Dict[str, Any]:
        """è·å– SQLite çš„ PRAGMA é…ç½®ä¿¡æ¯"""
        try:
            from sqlalchemy import text
            async with self.container.db.get_session() as session:
                # è·å– auto_vacuum
                res_v = await session.execute(text("PRAGMA auto_vacuum;"))
                auto_vacuum = res_v.scalar()
                
                # è·å– journal_mode
                res_j = await session.execute(text("PRAGMA journal_mode;"))
                journal_mode = res_j.scalar()
                
                # è·å– synchronous
                res_s = await session.execute(text("PRAGMA synchronous;"))
                synchronous = res_s.scalar()
                
                # è·å– mmap_size
                res_m = await session.execute(text("PRAGMA mmap_size;"))
                mmap_size = res_m.scalar()
                
                return {
                    "auto_vacuum": bool(auto_vacuum) if isinstance(auto_vacuum, int) else auto_vacuum != 'NONE',
                    "wal_mode": str(journal_mode).upper() == 'WAL',
                    "sync_mode": 'NORMAL' if synchronous == 1 else 'FULL' if synchronous == 2 else 'OFF' if synchronous == 0 else str(synchronous),
                    "mmap_size": f"{int(mmap_size) / 1024 / 1024:.0f}MB" if mmap_size and int(mmap_size) > 0 else "0 (Disabled)"
                }
        except Exception as e:
            logger.error(f"Failed to get DB PRAGMA info: {e}")
            return {'auto_vacuum': False, 'wal_mode': False, 'sync_mode': 'Unknown'}

class GuardService:
    """
    Guard service for hot-reloading and system health monitoring.
    Fully asynchronous implementation.
    """
    def __init__(self):
        self._stop_event = asyncio.Event()
        self._last_mtimes = {}
        self._watch_paths = [
            settings.BASE_DIR / ".env",
            settings.BASE_DIR / "main.py",
            settings.BASE_DIR / "core",
            settings.BASE_DIR / "services",
            settings.BASE_DIR / "web_admin"
        ]
        # Maintenance settings
        self._temp_guard_max = settings.TEMP_GUARD_MAX
        self._temp_guard_path = settings.TEMP_DIR
        self._memory_limit_mb = 500 # Default limit

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å®ˆæŠ¤æœåŠ¡çš„å½“å‰ç»Ÿè®¡çŠ¶æ€"""
        import psutil
        try:
            process = psutil.Process()
            return {
                "memory_mb": round(process.memory_info().rss / 1024 / 1024, 2),
                "cpu_percent": process.cpu_percent(),
                "uptime_seconds": round(time.time() - process.create_time(), 1),
                "tasks_active": len(asyncio.all_tasks()),
                "guard_status": "active" if not self._stop_event.is_set() else "stopped"
            }
        except Exception:
            return {"status": "error"}

    def start_guards(self):
        """Deprecated: Use start_guards_async instead."""
        
    async def start_guards_async(self):
        """å¯åŠ¨æ‰€æœ‰å¼‚æ­¥å®ˆæŠ¤ä»»åŠ¡"""
        logger.info("ğŸš€ Initializing All System Guards (Async)...")
        self._stop_event.clear()
        
        # è®°å½•åˆå§‹æ–‡ä»¶æ—¶é—´
        self._update_mtimes()
        
        # ä½¿ç”¨ exception_handler æˆ–è€… gather å¯åŠ¨æ‰€æœ‰èƒŒæ™¯ä»»åŠ¡
        # æˆ‘ä»¬è¿™é‡Œè®©å®ƒä»¬ä½œä¸ºé•¿é©»ä»»åŠ¡è¿è¡Œ
        tasks = [
            self.start_config_guard(),
            self.start_memory_guard(),
            self.start_db_health_guard(),
            self.start_temp_guard(),
            self.start_file_watcher_guard()
        ]
        
        # å¯åŠ¨èƒŒæ™¯ä»»åŠ¡
        for task in tasks:
            asyncio.create_task(task)
            
        logger.info("âœ… All Guards initiated.")

    def stop_guards(self):
        """åœæ­¢æ‰€æœ‰å®ˆæŠ¤é€»è¾‘ä¿¡å·"""
        logger.info("Stopping System Guards...")
        self._stop_event.set()

    async def start_config_guard(self):
        """å¼‚æ­¥é…ç½®åŒæ­¥å®ˆæŠ¤ä»»åŠ¡"""
        logger.info("[guard] Config hot-load guard initiated.")
        while not self._stop_event.is_set():
            try:
                await asyncio.sleep(60)
                from core.config_initializer import load_dynamic_config_from_db
                await load_dynamic_config_from_db(settings)
            except Exception as e:
                logger.error(f"[guard-config] Error: {e}")

    async def start_memory_guard(self):
        """å¼‚æ­¥å†…å­˜åŠå¢“ç¢‘åŒ–ç»´æŠ¤ä»»åŠ¡"""
        logger.info("[guard] Memory guard initiated (Limit: {}MB).".format(self._memory_limit_mb))
        import gc
        import psutil
        from core.helpers.tombstone import tombstone
        
        check_interval = 30
        gc_interval = 1800
        last_gc = time.time()
        
        while not self._stop_event.is_set():
            try:
                now = time.time()
                # 1. å®šæ—¶ GC
                if now - last_gc > gc_interval:
                    unreachable = gc.collect()
                    if unreachable > 0:
                        logger.debug(f"[guard-mem] GC collected {unreachable} objects")
                    last_gc = now
                
                # 2. å†…å­˜é˜ˆå€¼æ£€æŸ¥
                try:
                    process = psutil.Process()
                    rss_mb = process.memory_info().rss / 1024 / 1024
                    
                    if rss_mb > self._memory_limit_mb and not tombstone._is_frozen:
                        logger.warning(f"[guard-mem] Memory threshold exceeded ({rss_mb:.2f}MB > {self._memory_limit_mb}MB)")
                        await tombstone.freeze()
                    elif rss_mb < (self._memory_limit_mb * 0.7) and tombstone._is_frozen:
                        # å†…å­˜é™ä¸‹æ¥åå°è¯•å¤è‹
                        await tombstone.resurrect()
                except Exception as e:
                    logger.error(f"[guard-mem] Memory check error: {e}")
                
                await asyncio.sleep(check_interval)
            except Exception as e:
                logger.error(f"[guard-mem] Error: {e}")
                await asyncio.sleep(60)

    async def start_temp_guard(self):
        """å¼‚æ­¥ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®ˆæŠ¤ä»»åŠ¡"""
        logger.info("[guard] Temp directory guard initiated (Limit: {}GB).".format(self._temp_guard_max // 1024**3))
        while not self._stop_event.is_set():
            try:
                if self._temp_guard_path.exists():
                    files = []
                    total_size = 0
                    for f in self._temp_guard_path.rglob('*'):
                        if f.is_file():
                            try:
                                stat = f.stat()
                                total_size += stat.st_size
                                files.append((stat.st_mtime, stat.st_size, f))
                            except OSError as e:
                                logger.debug(f'å·²å¿½ç•¥é¢„æœŸå†…çš„å¼‚å¸¸: {e}' if 'e' in locals() else 'å·²å¿½ç•¥é™é»˜å¼‚å¸¸')
                    
                    if total_size > self._temp_guard_max:
                        # æŒ‰æ—¶é—´å‡åºæ’åºï¼ˆæœ€æ—§çš„åœ¨å‰ï¼‰
                        files.sort(key=lambda x: x[0])
                        deleted_size = 0
                        target_size = total_size - self._temp_guard_max
                        deleted_count = 0
                        
                        for _, size, f in files:
                            if deleted_size >= target_size:
                                break
                            try:
                                if f.exists():
                                    f.unlink()
                                    deleted_size += size
                                    deleted_count += 1
                            except Exception as e:
                                logger.warning(f'å·²å¿½ç•¥é¢„æœŸå†…çš„å¼‚å¸¸: {e}' if 'e' in locals() else 'å·²å¿½ç•¥é™é»˜å¼‚å¸¸')
                        
                        if deleted_count > 0:
                            logger.info(f"[guard-temp] Cleaned {deleted_count} files, freed {deleted_size/1024/1024:.2f}MB")
                
                await asyncio.sleep(3600) # æ¯å°æ—¶æ£€æŸ¥
            except Exception as e:
                logger.error(f"[guard-temp] Error: {e}")
                await asyncio.sleep(3600)

    async def start_db_health_guard(self):
        """å¼‚æ­¥æ•°æ®åº“å¥åº·æ£€æŸ¥"""
        logger.info("[guard] DB health monitor initiated.")
        from repositories.health_check import DatabaseHealthManager, settings as db_settings
        if not db_settings.ENABLE_DB_HEALTH_CHECK:
            return

        while not self._stop_event.is_set():
            try:
                await asyncio.sleep(4 * 3600)
                db_url = db_settings.DATABASE_URL
                if db_url.startswith("sqlite"):
                    path_str = db_url.split("///")[-1]
                    db_path = Path(db_settings.BASE_DIR) / path_str
                    manager = DatabaseHealthManager(str(db_path))
                    if not manager.check_health():
                        logger.critical("[guard-db] RUNTIME DB CORRUPTION DETECTED!")
            except Exception as e:
                logger.error(f"[guard-db] Error: {e}")

    async def start_file_watcher_guard(self):
        """å¼‚æ­¥æ–‡ä»¶å˜åŒ–ç›‘æ§ (çƒ­é‡å¯)"""
        logger.info("[guard] File watcher guard initiated.")
        while not self._stop_event.is_set():
            try:
                changed = await asyncio.to_thread(self._check_changes)
                if changed:
                    # [ä¿®å¤] å¦‚æœç³»ç»Ÿå½“å‰æ­£å¤„äºæ›´æ–°åçš„â€œè§‚å¯ŸæœŸâ€ï¼Œåˆ™å±è”½çƒ­é‡å¯ï¼Œé˜²æ­¢é€ æˆå¯åŠ¨å¾ªç¯å¯¼è‡´è¯¯åˆ¤å›æ»š
                    from services.update_service import update_service
                    update_state = update_service._get_state()
                    if update_state.get("status") == "restarting":
                        logger.warning(f"âš ï¸ [guard-watcher] ç³»ç»Ÿå¤„äºæ›´æ–°è§‚å¯ŸæœŸ (Observation Period)ï¼Œå·²å¿½ç•¥æ–‡ä»¶å˜æ›´ä»¥é˜²æ­¢å¯åŠ¨å¾ªç¯: {changed}")
                    else:
                        logger.info(f"[guard-watcher] Detected change: {changed}. Triggering hot-restart...")
                        await asyncio.sleep(1)
                        await self._restart_process_async()
                await asyncio.sleep(5) # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logger.error(f"[guard-watcher] Error: {e}")
                await asyncio.sleep(10)

    def _update_mtimes(self):
        for path in self._watch_paths:
            if not path.exists(): continue
            if path.is_file():
                self._last_mtimes[str(path)] = path.stat().st_mtime
            else:
                for p in path.glob("**/*.py"):
                    self._last_mtimes[str(p)] = p.stat().st_mtime

    def _check_changes(self) -> Optional[str]:
        for path in self._watch_paths:
            if not path.exists(): continue
            if path.is_file():
                mtime = path.stat().st_mtime
                if str(path) not in self._last_mtimes or mtime > self._last_mtimes[str(path)]:
                    self._last_mtimes[str(path)] = mtime
                    return str(path)
            else:
                for p in path.glob("**/*.py"):
                    mtime = p.stat().st_mtime
                    if str(p) not in self._last_mtimes or mtime > self._last_mtimes[str(p)]:
                        self._last_mtimes[str(p)] = mtime
                        return str(p)
        return None

    async def _restart_process_async(self):
        """å¼‚æ­¥è§¦å‘é‡å¯"""
        logger.info("Triggering graceful restart...")
        from core.shutdown import get_shutdown_coordinator
        coordinator = get_shutdown_coordinator()
        try:
            await coordinator.shutdown()
            
            # ç‰©ç†é‡å¯å°è¯•
            logger.info("Attempting physical process re-exec...")
            import os
            import sys
            import time
            
            # è·å–å½“å‰è¿è¡Œçš„ Python è§£é‡Šå™¨å’Œå‚æ•°
            executable = sys.executable
            args = sys.argv[:]
            
            # [æˆç†Ÿæ–¹æ¡ˆ] ç»™äºˆä¸€å°æ®µæ—¶é—´é‡Šæ”¾èµ„æºå’Œæ—¥å¿—å¥æŸ„
            # è™½ç„¶ coordinator.shutdown å·²å®Œæˆï¼Œä½†åœ¨ Windows ä¸Šæ–‡ä»¶é”é‡Šæ”¾æœ‰æ—¶æœ‰å»¶è¿Ÿ
            time.sleep(0.5)
            
            if os.name == 'nt':
                # Windows ä¸‹ os.execl æ˜¯é€šè¿‡ spawn æ¨¡æ‹Ÿçš„ï¼Œä¼šç”Ÿæˆæ–°è¿›ç¨‹å¹¶ç»ˆæ­¢å½“å‰è¿›ç¨‹
                # è¿™ç§æ–¹å¼åœ¨ Windows ä¸‹æ˜¯é«˜åº¦å¯é çš„ï¼Œèƒ½ç¡®ä¿é”è¢«é‡Šæ”¾
                os.execl(executable, executable, *args)
            else:
                # Unix ä¸‹ os.execl æ›¿æ¢å½“å‰è¿›ç¨‹æ˜ åƒï¼Œæè‡´å¹³æ»‘
                os.execl(executable, executable, *args)
                
            # ç†è®ºä¸Š execl ä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œï¼Œé™¤éæ‰¾ä¸åˆ°æ–‡ä»¶
            sys.exit(0)
        except Exception as e:
            logger.error(f"Graceful restart failed: {e}")
            # æœ€åçš„ä¿åº•é€€å‡º
            sys.exit(1)

    def trigger_restart(self):
        asyncio.create_task(self._restart_process_async())

    async def cleanup_old_logs(self, days: int) -> Dict[str, Any]:
        """æ¸…ç†æ—§æ—¥å¿— (Handler Purity å…¼å®¹)"""
        try:
            from core.db_factory import async_cleanup_old_logs
            deleted_count = await async_cleanup_old_logs(days)
            logger.info(f"æˆåŠŸæ¸…ç† {days} å¤©å‰çš„æ—¥å¿—ï¼Œåˆ é™¤ {deleted_count} æ¡è®°å½•")
            return {
                'success': True,
                'deleted_count': deleted_count
            }
        except Exception as e:
            logger.error(f"æ¸…ç†æ—¥å¿—å¤±è´¥: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'deleted_count': 0
            }

    async def get_db_health(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“å¥åº·çŠ¶æ€ (Handler Purity å…¼å®¹)"""
        try:
            from sqlalchemy import text
            async with self.container.db.get_session() as session:
                # ç®€å•çš„è¿æ¥æµ‹è¯•
                await session.execute(text("SELECT 1"))
                return {
                    'connected': True,
                    'status': 'healthy'
                }
        except Exception as e:
            logger.error(f"æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {
                'connected': False,
                'status': 'error',
                'error': str(e)
            }


system_service = SystemService()
guard_service = GuardService()
