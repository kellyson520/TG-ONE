"""
æ™ºèƒ½å»é‡ç³»ç»Ÿ
å®ç°å†…å®¹ç›¸ä¼¼åº¦æ£€æµ‹å’Œæ—¶é—´çª—å£å»é‡
"""

import hashlib
from collections import OrderedDict
from difflib import SequenceMatcher

import asyncio
import logging
import os
import re
import time
from typing import Dict, List, Optional, Tuple, Any

from core.helpers.tombstone import tombstone

try:
    # å¯é€‰å¿«é€Ÿæ–‡æœ¬ç›¸ä¼¼åº¦åº“ï¼ˆæ›´å¿«æ›´å‡†ï¼‰
    from rapidfuzz import fuzz  # type: ignore

    _HAS_RAPIDFUZZ = True
except Exception:
    _HAS_RAPIDFUZZ = False

try:
    import xxhash

    _HAS_XXHASH = True
except ImportError:
    _HAS_XXHASH = False

try:
    from numba import jit

    _HAS_NUMBA = True
except ImportError:
    _HAS_NUMBA = False

    # å®šä¹‰ä¸€ä¸ªç©ºè£…é¥°å™¨ä½œä¸ºå›é€€
    def jit(*args, **kwargs):
        def decorator(func):
            return func

        return decorator


logger = logging.getLogger(__name__)


# Numba ä¼˜åŒ–çš„æ±‰æ˜è·ç¦»è®¡ç®—
@jit(nopython=True, cache=True)
def _fast_hamming_64(a: int, b: int) -> int:
    # Numba ä¼šå°†å…¶ç¼–è¯‘ä¸ºæå…¶é«˜æ•ˆçš„æœºå™¨ç 
    x = (a ^ b) & 0xFFFFFFFFFFFFFFFF
    # Kernighan ç®—æ³•åœ¨ JIT ä¸‹æ¯” bit_count æ›´é€šç”¨ä¸”æå¿«
    c = 0
    while x:
        x &= x - 1
        c += 1
    return c


class SmartDeduplicator:
    """æ™ºèƒ½å»é‡å™¨"""

    def __init__(self):
        # æ—¶é—´çª—å£ç¼“å­˜ (chat_id -> {signature: timestamp})
        self.time_window_cache = {}
        # å†…å®¹å“ˆå¸Œç¼“å­˜ (chat_id -> {content_hash: timestamp})
        self.content_hash_cache = {}
        # æ–‡æœ¬ç¼“å­˜ (chat_id -> [ {'text': cleaned_text, 'ts': timestamp}, ... ])
        self.text_cache = {}
        # æ–‡æœ¬æŒ‡çº¹ç¼“å­˜ï¼ˆSimHash 64bitï¼‰ï¼š(chat_id -> [ {'fp': int, 'ts': timestamp}, ... ])
        self.text_fp_cache = {}
        # å†™ç¼“å†²é˜Ÿåˆ—ï¼šç”¨äºæ‰¹é‡å†™å…¥æ•°æ®åº“
        self._write_buffer = []
        self._buffer_lock = asyncio.Lock()
        self._flush_task = None  # åå°åˆ·å†™ä»»åŠ¡
        self._bg_tasks = set()   # åå°è®¡ç®—ä»»åŠ¡é›†åˆ
        # é»˜è®¤é…ç½®
        self.config = {
            "enable_time_window": True,
            "time_window_hours": 24,
            "similarity_threshold": 0.85,
            "enable_content_hash": True,
            "enable_smart_similarity": True,
            "cache_cleanup_interval": 3600,  # 1å°æ—¶æ¸…ç†ä¸€æ¬¡
            "enable_persistent_cache": True,  # ä½¿ç”¨æŒä¹…åŒ–ç¼“å­˜è·¨é‡å¯ä¿ç•™çª—å£å‘½ä¸­
            "persistent_cache_ttl_seconds": int(
                os.getenv("DEDUP_PERSIST_TTL_SECONDS", "2592000")
            ),  # 30å¤©ä¸Šé™
            # æ–°å¢é…ç½®é¡¹
            "max_text_cache_size": 300,  # æ¯ä¸ªä¼šè¯æœ€å¤šç¼“å­˜å¤šå°‘æ¡æ–‡æœ¬ç”¨äºç›¸ä¼¼åº¦æ£€æŸ¥
            "min_text_length": 10,  # è§¦å‘ç›¸ä¼¼åº¦æ£€æŸ¥çš„æœ€å°æ¸…æ´—åæ–‡æœ¬é•¿åº¦
            "strip_numbers": True,  # æ¸…æ´—æ–‡æœ¬æ—¶æ˜¯å¦ç§»é™¤æ•°å­—
            # æ–‡æœ¬ç›¸ä¼¼åº¦é¢„ç­›ï¼ˆå›ºå®šé•¿åº¦æŒ‡çº¹/SimHashï¼Œç”¨äºå¿«é€Ÿè¿‡æ»¤ï¼‰
            "enable_text_fingerprint": True,
            "fingerprint_ngram": 3,  # è¯çº§ n-gram å¤§å°
            "fingerprint_hamming_threshold": 3,  # æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼ˆ0 ä¸ºå®Œå…¨ä¸€è‡´ï¼‰
            "max_text_fp_cache_size": 500,  # æ¯ä¸ªä¼šè¯æœ€å¤šç¼“å­˜çš„æ–‡æœ¬æŒ‡çº¹æ¡æ•°
            "max_similarity_checks": 50,  # æœ€å¤šåšå¤šå°‘æ¬¡ç²¾ç¡®ç›¸ä¼¼åº¦æ¯”è¾ƒ
            # æ–‡æœ¬ç›¸ä¼¼åº¦åœ¨å«è§†é¢‘æ¶ˆæ¯ä¸­çš„åº”ç”¨ï¼ˆé»˜è®¤å…³é—­ï¼Œé¿å…ä¸åŒè§†é¢‘åŒæ ‡é¢˜è¢«è¯¯æ€ï¼‰
            "enable_text_similarity_for_video": False,
            # è§†é¢‘ä¸“ç”¨
            "enable_video_file_id_check": True,  # åŸºäº telegram file id çš„å¿«é€Ÿåˆ¤é‡
            "enable_video_partial_hash_check": True,  # åŸºäºè§†é¢‘éƒ¨åˆ†å­—èŠ‚å“ˆå¸Œçš„åˆ¤é‡
            "video_partial_hash_bytes": 262144,  # æ¯æ®µè¯»å–çš„å­—èŠ‚æ•°ï¼ˆé»˜è®¤256KBï¼‰
            "video_partial_hash_on_fileid_miss_only": True,  # ä»…åœ¨ file_id æœªå‘½ä¸­é‡å¤æ—¶å†è®¡ç®—éƒ¨åˆ†å“ˆå¸Œ
            "video_partial_hash_min_size_bytes": 5
            * 1024
            * 1024,  # å°è§†é¢‘ä¸åšéƒ¨åˆ†å“ˆå¸Œï¼ˆé»˜è®¤>=5MBï¼‰
            # è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜ï¼ˆé¿å…é‡å¤ä¸‹è½½/é‡å¤è®¡ç®—ï¼‰
            "video_hash_persist_ttl_seconds": int(
                os.getenv("VIDEO_HASH_PERSIST_TTL_SECONDS", "15552000")
            ),  # 180å¤©
            # è§†é¢‘ä¸¥æ ¼å¤æ ¸ï¼šå“ˆå¸Œå‘½ä¸­åå¯¹æ—¶é•¿/åˆ†è¾¨ç‡/å¤§å°èŒƒå›´åšé˜ˆå€¼æ ¡éªŒ
            "video_strict_verify": True,
            "video_duration_tolerance_sec": 2,
            "video_resolution_tolerance_px": 8,
            "video_size_bucket_tolerance": 1,
        }
        self.last_cleanup = time.time()
        # é…ç½®åŠ è½½æ ‡è®°ï¼šä½¿ç”¨æ‡’åŠ è½½æ¨¡å¼ï¼Œé¿å…åœ¨æ¨¡å—åˆå§‹åŒ–é˜¶æ®µæ‰§è¡Œæ•°æ®åº“æ“ä½œ
        self._config_loaded = False

        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        # åŸºç¡€æ–‡æœ¬æ¸…æ´—æ­£åˆ™ï¼šåŒ¹é… URLã€@æåŠã€#æ ‡ç­¾
        self._re_basic_clean = re.compile(r"http[s]?://\S+|@\w+|#\w+", re.I)
        # ç”¨äº URL/Mention å»é™¤çš„ç®€å•æ­£åˆ™
        self._re_complex_patterns = re.compile(r"http\S+|@\w+|#\w+", re.I)
        # ä½¿ç”¨str.translateçš„è½¬æ¢è¡¨ï¼Œé¢„è®¡ç®—ä»¥æé«˜æ€§èƒ½
        import string

        # å®šä¹‰è¦åˆ é™¤çš„å­—ç¬¦ï¼šæ ‡ç‚¹ç¬¦å· + ä¸å¯è§å­—ç¬¦
        self.trans_table_keep_nums = str.maketrans("", "")
        self.trans_table_no_nums = str.maketrans("", "")

        # æ±‰æ˜è·ç¦»è½¬æ¢è¡¨å·²é¢„è®¡ç®—
        self.trans_table_keep_nums = str.maketrans({c: None for c in string.punctuation + "\n\r\t"})
        self.trans_table_no_nums = str.maketrans({c: None for c in string.punctuation + string.digits + "\n\r\t"})

        # åˆå§‹åŒ– Bloom Filter (L0 ç¼“å­˜)
        try:
            from utils.processing.bloom_filter import GlobalBloomFilter
            self.bloom_filter = GlobalBloomFilter.get_filter("smart_dedup")
            logger.info("Bloom Filter (L0) åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Bloom Filter åˆå§‹åŒ–å¤±è´¥: {e}")
            self.bloom_filter = None

        # åˆå§‹åŒ– HLL (HyperLogLog) ç”¨äºåŸºæ•°ç»Ÿè®¡
        try:
            from utils.processing.hll import GlobalHLL
            self.hll = GlobalHLL.get_hll("unique_messages_today")
            logger.info("HLL (HyperLogLog) åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"HLL åˆå§‹åŒ–å¤±è´¥: {e}")
            self.hll = None

        # åˆå§‹åŒ– LSH Forest (ç”¨äºè¯­ä¹‰å»é‡)
        try:
            from utils.processing.simhash import SimHash
            from utils.algorithm.lsh_forest import LSHForest
            self.simhash_engine = SimHash()
            # åˆå§‹åŒ–ç´¢å¼• (chat_id -> LSHForest)
            self.lsh_forests = {}
            logger.info("SimHash å¼•æ“ä¸ LSH Forest ç´¢å¼•ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"LSH Forest åˆå§‹åŒ–å¤±è´¥: {e}")
            self.simhash_engine = None
            self.lsh_forests = {}

        # âœ… æ³¨å†Œåˆ°å¢“ç¢‘ç®¡ç†å™¨
        tombstone.register(
            name="smart_dedup",
            get_state_func=self._hibernate_state,
            restore_state_func=self._wakeup_state,
        )

    @property
    def repo(self):
        from core.container import container
        return container.dedup_repo

    # --- æ–°å¢ï¼šä¼‘çœ é€»è¾‘ (å¯¼å‡ºæ•°æ®å¹¶æ¸…ç©ºè‡ªå·±) ---
    def _hibernate_state(self):
        """å¯¼å‡ºçŠ¶æ€å¹¶æ¸…ç©ºå†…å­˜"""
        state = {
            "time_window": self.time_window_cache,
            "content_hash": self.content_hash_cache,
            "text": self.text_cache,
            "text_fp": self.text_fp_cache,
            "lsh_forests": self.lsh_forests,
        }
        # ğŸš¨ å…³é”®ï¼šå½»åº•æ¸…ç©ºå†…å­˜ä¸­çš„å­—å…¸
        self.time_window_cache = {}
        self.content_hash_cache = {}
        self.text_cache = {}
        self.text_fp_cache = {}
        self.lsh_forests = {}
        return state

    # --- æ–°å¢ï¼šå”¤é†’é€»è¾‘ (æ¢å¤æ•°æ®) ---
    def _wakeup_state(self, state):
        """æ¢å¤çŠ¶æ€"""
        if not state:
            return
        self.time_window_cache = state.get("time_window", {})
        self.content_hash_cache = state.get("content_hash", {})
        self.text_cache = state.get("text", {})
        self.text_fp_cache = state.get("text_fp", {})
        self.lsh_forests = state.get("lsh_forests", {})

    async def _lazy_load_config(self):
        """æ‡’åŠ è½½é…ç½®ï¼šåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶ä»æ•°æ®åº“åŠ è½½é…ç½®"""
        if self._config_loaded:
            return
        logger.debug("å¼€å§‹æ‡’åŠ è½½å»é‡é…ç½®...")
        try:
            await asyncio.to_thread(self._load_config_from_db)
            self._config_loaded = True
            logger.debug("æ‡’åŠ è½½é…ç½®å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ‡’åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ç»§ç»­è¿è¡Œ")
            self._config_loaded = True  # é¿å…é‡å¤å°è¯•åŠ è½½

    async def _compute_and_save_video_hash_bg(self, message_obj, partial_bytes, file_id, target_chat_id, config):
        """åå°è®¡ç®—è§†é¢‘å“ˆå¸Œå¹¶ä¿å­˜åˆ°DB/Cache"""
        try:
            logger.info(f"åå°å¼€å§‹è®¡ç®—è§†é¢‘å“ˆå¸Œ: {file_id}")
            vhash = await self._compute_video_partial_hash(message_obj, partial_bytes)
            if vhash:
                logger.info(f"åå°å“ˆå¸Œè®¡ç®—å®Œæˆ: {file_id} -> {vhash}")
                
                # 1. å†™å…¥æŒä¹…åŒ–ç¼“å­˜
                try:
                    ttl = int(config.get("video_hash_persist_ttl_seconds", 15552000))
                    await self._write_video_hash_pcache(str(file_id), vhash, ttl)
                except Exception as e:
                    logger.error(f"åå°å†™å…¥PCacheå¤±è´¥: {e}")

                # 2. å†™å…¥æ•°æ®åº“
                # ä½¿ç”¨ç‰¹æ®Šçš„ signature "video_hash:{hash}" ä»¥é¿å…ä¸ä¸»æµç¨‹çš„ "video:{file_id}" å†²çª
                try:
                    await self._record_message(
                        message_obj, 
                        target_chat_id, 
                        signature=f"video_hash:{vhash}", 
                        content_hash=vhash
                    )
                    logger.debug(f"åå°å“ˆå¸Œè®°å½•å·²å­˜å…¥DBç¼“å†²: {vhash}")
                except Exception as e:
                    logger.error(f"åå°å†™å…¥DBå¤±è´¥: {e}")
            else:
                logger.warning(f"åå°è®¡ç®—å“ˆå¸Œè¿”å›ç©º: {file_id}")
        except Exception as e:
            logger.error(f"åå°è§†é¢‘å¤„ç†ä»»åŠ¡å¼‚å¸¸: {e}", exc_info=True)

    async def check_duplicate(
        self,
        message_obj,
        target_chat_id: int,
        rule_config: Dict = None,
        *,
        readonly: bool = False,
    ) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä¸ºé‡å¤
        è¿”å›: (is_duplicate, reason)
        """
        start_ts = time.time()
        logger.debug(
            f"å¼€å§‹å»é‡æ£€æŸ¥ï¼Œç›®æ ‡chat_id: {target_chat_id}, æ¶ˆæ¯ç±»å‹: {type(message_obj).__name__}"
        )
        try:
            # âœ… å…³é”®ï¼šæ¯æ¬¡ä½¿ç”¨å‰æ£€æŸ¥æ˜¯å¦å¤„äºå†·å†»çŠ¶æ€
            # å¦‚æœå·²å†»ç»“ï¼Œå…ˆå¤è‹ (Lazy Loading)
            logger.debug("æ£€æŸ¥å†·å†»çŠ¶æ€...")
            if tombstone._is_frozen:
                logger.debug("æ£€æµ‹åˆ°å†·å†»çŠ¶æ€ï¼Œå°è¯•å¤è‹...")
                try:
                    await tombstone.resurrect()
                    logger.debug("å¤è‹æˆåŠŸ")
                except Exception as e:
                    logger.error(f"è‡ªåŠ¨å¤è‹å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç©ºç¼“å­˜ç»§ç»­è¿è¡Œ")
                    # å¼ºåˆ¶è§£é™¤å†»ç»“çŠ¶æ€ï¼Œé¿å…æ­»å¾ªç¯
                    tombstone._is_frozen = False
                    # è¿™é‡Œä¸éœ€è¦åšé¢å¤–æ“ä½œï¼Œå› ä¸º _wakeup_state æ²¡è¢«è°ƒç”¨çš„è¯
                    # ç¼“å­˜å°±æ˜¯ç©ºçš„ï¼Œç¨‹åºä¼šæ­£å¸¸è¿è¡Œï¼ˆåªæ˜¯æš‚æ—¶æ— æ³•å»é‡æ—§æ¶ˆæ¯ï¼‰

            # æ‡’åŠ è½½é…ç½®
            await self._lazy_load_config()

            # å®šæœŸæ¸…ç†ç¼“å­˜
            logger.debug("æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†ç¼“å­˜...")
            await self._cleanup_cache_if_needed()
            logger.debug("ç¼“å­˜æ¸…ç†æ£€æŸ¥å®Œæˆ")

            # åˆå¹¶é…ç½®
            config = {**self.config, **(rule_config or {})}
            logger.debug(f"ä½¿ç”¨é…ç½®: {config}")

            # 1. ä¼ ç»Ÿç­¾åå»é‡
            logger.debug("å¼€å§‹ç”Ÿæˆæ¶ˆæ¯ç­¾å...")
            signature = self._generate_signature(message_obj)
            logger.debug(f"ç”Ÿæˆç­¾å: {signature}")
            if signature:
                # L0: Bloom Filter é¢„åˆ¤
                if self.bloom_filter:
                    bloom_key = f"sig:{target_chat_id}:{signature}"
                    if bloom_key not in self.bloom_filter:
                        # 100% ç¡®å®šä¸é‡å¤ï¼Œç›´æ¥è·³è¿‡åç»­æ˜‚è´µçš„ DB/PCache æ£€æŸ¥
                        logger.debug(f"Bloom Filter (L0) ç¡®è®¤ç­¾åä¸é‡å¤: {signature}")
                        # ä»…è®°å½•åˆ° Bloom Filter (å®é™…è®°å½•åˆ° DB ä¼šåœ¨æµç¨‹ç»“æŸæ—¶è°ƒç”¨ _record_message)
                        # è¿™é‡Œæˆ‘ä»¬è¿”å› Falseï¼Œè¿›å…¥åç»­æµç¨‹
                        pass
                    else:
                        logger.debug(f"Bloom Filter (L0) å‘½ä¸­ï¼Œå¯èƒ½é‡å¤: {signature}")
                
                # å…ˆæŸ¥æŒä¹…åŒ–ç¼“å­˜ï¼ˆè·¨é‡å¯çƒ­å‘½ä¸­ï¼‰ï¼Œå‘½ä¸­å³è¿”å›
                if await self._check_pcache_hit("sig", target_chat_id, signature):
                    logger.debug(f"æŒä¹…åŒ–ç¼“å­˜ç­¾åå‘½ä¸­: {signature}")
                    try:
                        from core.helpers.metrics import (
                            DEDUP_DECISIONS_TOTAL,
                            DEDUP_HITS_TOTAL,
                        )

                        DEDUP_HITS_TOTAL.labels(method="signature_pcache").inc()
                        DEDUP_DECISIONS_TOTAL.labels(
                            result="duplicate", method="signature_pcache"
                        ).inc()
                    except Exception as e:
                        logger.debug(f"Metrics record failed: {e}")
                        pass
                    return True, "ç­¾åé‡å¤: persistent cache å‘½ä¸­"
                logger.debug(f"æ£€æŸ¥ç­¾åé‡å¤: {signature}")
                is_dup, reason = await self._check_signature_duplicate(
                    signature, target_chat_id, config
                )
                if is_dup:
                    logger.debug(f"ç­¾åé‡å¤å‘½ä¸­: {reason}")
                    try:
                        from core.helpers.metrics import (
                            DEDUP_DECISIONS_TOTAL,
                            DEDUP_HITS_TOTAL,
                        )

                        DEDUP_HITS_TOTAL.labels(method="signature").inc()
                        DEDUP_DECISIONS_TOTAL.labels(
                            result="duplicate", method="signature"
                        ).inc()
                        from core.helpers.metrics import DEDUP_CHECK_SECONDS

                        DEDUP_CHECK_SECONDS.observe(max(0.0, time.time() - start_ts))
                    except Exception as e:
                        logger.debug(f"Metrics record failed: {e}")
                        pass
                    return True, f"ç­¾åé‡å¤: {reason}"

            # 2. è§†é¢‘ä¼˜å…ˆåˆ¤é‡ï¼ˆå°†è§†é¢‘ç›¸å…³æ£€æŸ¥æå‰ï¼Œé¿å…è¢«å†…å®¹å“ˆå¸Œ/æ–‡æœ¬ç›¸ä¼¼åº¦è¯¯æ€ï¼‰
            logger.debug("æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ¶ˆæ¯...")
            is_video = self._is_video(message_obj)
            logger.debug(f"è§†é¢‘æ¶ˆæ¯æ£€æŸ¥ç»“æœ: {is_video}")

            if is_video:
                # file_id åˆ¤é‡
                logger.debug("å¼€å§‹è§†é¢‘file_idåˆ¤é‡...")
                file_id_checked = False
                file_id_found_duplicate = False
                if config.get("enable_video_file_id_check", True):
                    try:
                        file_id = self._extract_video_file_id(message_obj)
                        logger.debug(f"æå–åˆ°è§†é¢‘file_id: {file_id}")
                        if file_id:
                            file_id_checked = True
                            is_dup = await self._check_video_duplicate_by_file_id(
                                file_id, target_chat_id
                            )
                            logger.debug(f"è§†é¢‘file_idé‡å¤æ£€æŸ¥ç»“æœ: {is_dup}")
                            if is_dup:
                                file_id_found_duplicate = True
                                try:
                                    from core.helpers.metrics import (
                                        DEDUP_DECISIONS_TOTAL,
                                        DEDUP_HITS_TOTAL,
                                    )

                                    DEDUP_HITS_TOTAL.labels(
                                        method="video_file_id"
                                    ).inc()
                                    DEDUP_DECISIONS_TOTAL.labels(
                                        result="duplicate", method="video_file_id"
                                    ).inc()
                                    from core.helpers.metrics import DEDUP_CHECK_SECONDS

                                    DEDUP_CHECK_SECONDS.observe(
                                        max(0.0, time.time() - start_ts)
                                    )
                                except Exception:
                                    pass
                                return True, "è§†é¢‘file_idé‡å¤"
                            try:
                                setattr(message_obj, "_tf_file_id", str(file_id))
                            except Exception:
                                pass
                    except Exception as _ve:
                        logger.debug(f"è§†é¢‘ file_id åˆ¤é‡å¤±è´¥: {_ve}")
                # éƒ¨åˆ†å“ˆå¸Œåˆ¤é‡ï¼ˆå¯é€‰ï¼šä»…åœ¨ file_id æœªå‘½ä¸­é‡å¤æ—¶æ‰§è¡Œï¼›å¯é…ç½®æœ€å°æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼‰
                logger.debug("å¼€å§‹è§†é¢‘éƒ¨åˆ†å“ˆå¸Œåˆ¤é‡...")
                if config.get("enable_video_partial_hash_check", True):
                    only_on_miss = bool(
                        config.get("video_partial_hash_on_fileid_miss_only", True)
                    )
                    # é€»è¾‘ï¼šå¦‚æœä¸æ˜¯"ä»…é”™è¿‡æ—¶"æ¨¡å¼ï¼Œæ€»æ˜¯è¿è¡Œï¼›å¦‚æœæ˜¯"ä»…é”™è¿‡æ—¶"æ¨¡å¼ï¼Œåªåœ¨file_idæ£€æŸ¥äº†ä½†æ²¡æ‰¾åˆ°é‡å¤æ—¶è¿è¡Œ
                    should_run = (not only_on_miss) or (
                        only_on_miss and file_id_checked and not file_id_found_duplicate
                    )
                    logger.debug(
                        f"è§†é¢‘éƒ¨åˆ†å“ˆå¸Œåˆ¤é‡æ¡ä»¶: should_run={should_run}, only_on_miss={only_on_miss}, file_id_checked={file_id_checked}, file_id_found_duplicate={file_id_found_duplicate}"
                    )
                    if should_run:
                        try:
                            min_size = int(
                                config.get(
                                    "video_partial_hash_min_size_bytes", 5 * 1024 * 1024
                                )
                            )
                            # è‹¥å¯è·å–æ–‡ä»¶å¤§å°ï¼Œåšé˜ˆå€¼è¿‡æ»¤
                            doc = getattr(message_obj, "document", None)
                            if doc is None and hasattr(message_obj, "video"):
                                doc = getattr(message_obj, "video")
                            size_ok = True
                            if doc is not None:
                                try:
                                    size_val = int(getattr(doc, "size", 0) or 0)
                                    if size_val and size_val < min_size:
                                        size_ok = False
                                    logger.debug(
                                        f"è§†é¢‘å¤§å°æ£€æŸ¥: size_val={size_val}, min_size={min_size}, size_ok={size_ok}"
                                    )
                                except Exception:
                                    size_ok = True
                            if size_ok:
                                partial_bytes = int(
                                    config.get("video_partial_hash_bytes", 262144)
                                )
                                # å…ˆæŸ¥æŒä¹…åŒ–ç¼“å­˜ï¼ˆä»¥ file_id ä¸ºé”®ï¼‰
                                vhash = None
                                try:
                                    file_id_for_hash = getattr(
                                        getattr(message_obj, "video", None), "id", None
                                    ) or getattr(
                                        getattr(message_obj, "document", None),
                                        "id",
                                        None,
                                    )
                                    if file_id_for_hash:
                                        logger.debug(
                                            f"æ£€æŸ¥è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜: file_id={file_id_for_hash}"
                                        )
                                        vhash = await self._check_video_hash_pcache(
                                            str(file_id_for_hash)
                                        )
                                        logger.debug(f"è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜ç»“æœ: {vhash}")
                                        if vhash:
                                            try:
                                                from core.helpers.metrics import (
                                                    VIDEO_HASH_PCACHE_HITS_TOTAL,
                                                )

                                                VIDEO_HASH_PCACHE_HITS_TOTAL.labels(
                                                    algo="partial_md5"
                                                ).inc()
                                            except Exception:
                                                pass
                                except Exception:
                                    pass

                                if not vhash:
                                    # [Optimization] å¼‚æ­¥è®¡ç®—è§†é¢‘å“ˆå¸Œï¼Œé¿å…é˜»å¡è½¬å‘æµç¨‹
                                    # é¦–æ¬¡è§åˆ°çš„è§†é¢‘ï¼ˆä¸”PCacheæœªå‘½ä¸­ï¼‰ï¼Œæ”¾è¡Œå¹¶åå°è®°å½•
                                    logger.info(f"è§†é¢‘FileIDæœªå‘½ä¸­ä¸”æ— Hashç¼“å­˜ï¼Œå¯åŠ¨åå°è®¡ç®—ä»»åŠ¡å¹¶æ”¾è¡Œ: {file_id_for_hash}")
                                    
                                    task = asyncio.create_task(
                                        self._compute_and_save_video_hash_bg(
                                            message_obj, partial_bytes, file_id_for_hash, target_chat_id, config
                                        )
                                    )
                                    self._bg_tasks.add(task)
                                    task.add_done_callback(self._bg_tasks.discard)
                                    
                                    # è¿”å› False (ä¸é‡å¤) å¹¶ä¸ç­‰å¾…å“ˆå¸Œç»“æœ
                                    return False, "æ–°è§†é¢‘(å¼‚æ­¥è®°å½•)"
                                if vhash:
                                    logger.debug(f"æ£€æŸ¥è§†é¢‘å“ˆå¸Œé‡å¤: {vhash}")
                                    is_dup = await self._check_video_duplicate_by_hash(
                                        vhash, target_chat_id
                                    )
                                    logger.debug(f"è§†é¢‘å“ˆå¸Œé‡å¤æ£€æŸ¥ç»“æœ: {is_dup}")
                                    if is_dup:
                                        try:
                                            from core.helpers.metrics import (
                                                DEDUP_DECISIONS_TOTAL,
                                                DEDUP_HITS_TOTAL,
                                            )

                                            DEDUP_HITS_TOTAL.labels(
                                                method="video_partial_hash"
                                            ).inc()
                                            DEDUP_DECISIONS_TOTAL.labels(
                                                result="duplicate",
                                                method="video_partial_hash",
                                            ).inc()
                                            from core.helpers.metrics import (
                                                DEDUP_CHECK_SECONDS,
                                            )

                                            DEDUP_CHECK_SECONDS.observe(
                                                max(0.0, time.time() - start_ts)
                                            )
                                        except Exception:
                                            pass
                                        # ä¸¥æ ¼å¤æ ¸
                                        try:
                                            if config.get("video_strict_verify", True):
                                                logger.debug("å¼€å§‹è§†é¢‘ç‰¹å¾ä¸¥æ ¼å¤æ ¸...")
                                                strict_ok = await self._strict_verify_video_features(
                                                    target_chat_id,
                                                    message_obj,
                                                    file_id_for_hash,
                                                    vhash,
                                                    config,
                                                )
                                                logger.debug(
                                                    f"è§†é¢‘ç‰¹å¾ä¸¥æ ¼å¤æ ¸ç»“æœ: {strict_ok}"
                                                )
                                                if not strict_ok:
                                                    return (
                                                        False,
                                                        "è§†é¢‘ç‰¹å¾ä¸ä¸€è‡´ï¼Œå¿½ç•¥å“ˆå¸Œå‘½ä¸­",
                                                    )
                                        except Exception:
                                            pass
                                        return True, "è§†é¢‘å†…å®¹å“ˆå¸Œé‡å¤"
                                    try:
                                        setattr(message_obj, "_tf_content_hash", vhash)
                                    except Exception:
                                        pass
                        except Exception as _ve:
                            logger.debug(f"è§†é¢‘éƒ¨åˆ†å“ˆå¸Œåˆ¤é‡å¤±è´¥: {_ve}")

            # 3. å†…å®¹å“ˆå¸Œå»é‡ï¼ˆå¯¹è§†é¢‘é»˜è®¤å…³é—­ï¼Œä»¥é¿å…è¯¯æ€ï¼›å¯é€šè¿‡é…ç½®å¼€å¯ï¼‰
            logger.debug("å¼€å§‹å†…å®¹å“ˆå¸Œå»é‡...")
            content_hash = None
            if config.get("enable_content_hash") and (
                not is_video or config.get("enable_content_hash_for_video", False)
            ):
                content_hash = self._generate_content_hash(message_obj)
                logger.debug(f"ç”Ÿæˆå†…å®¹å“ˆå¸Œ: {content_hash}")
                if content_hash:
                    # å…ˆæŸ¥æŒä¹…åŒ–ç¼“å­˜
                    logger.debug(f"æ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜å†…å®¹å“ˆå¸Œ: {content_hash}")
                    if await self._check_pcache_hit(
                        "hash", target_chat_id, content_hash
                    ):
                        logger.debug(f"æŒä¹…åŒ–ç¼“å­˜å†…å®¹å“ˆå¸Œå‘½ä¸­: {content_hash}")
                        try:
                            from core.helpers.metrics import (
                                DEDUP_DECISIONS_TOTAL,
                                DEDUP_HITS_TOTAL,
                            )

                            DEDUP_HITS_TOTAL.labels(method="content_hash_pcache").inc()
                            DEDUP_DECISIONS_TOTAL.labels(
                                result="duplicate", method="content_hash_pcache"
                            ).inc()
                        except Exception:
                            pass
                        return True, "å†…å®¹é‡å¤: persistent cache å‘½ä¸­"
                    logger.debug(f"æ£€æŸ¥å†…å®¹å“ˆå¸Œé‡å¤: {content_hash}")
                    is_dup, reason = await self._check_content_hash_duplicate(
                        content_hash, target_chat_id, config
                    )
                    if is_dup:
                        logger.debug(f"å†…å®¹å“ˆå¸Œé‡å¤å‘½ä¸­: {reason}")
                        try:
                            from core.helpers.metrics import (
                                DEDUP_DECISIONS_TOTAL,
                                DEDUP_HITS_TOTAL,
                            )

                            DEDUP_HITS_TOTAL.labels(method="content_hash").inc()
                            DEDUP_DECISIONS_TOTAL.labels(
                                result="duplicate", method="content_hash"
                            ).inc()
                            from core.helpers.metrics import DEDUP_CHECK_SECONDS

                            DEDUP_CHECK_SECONDS.observe(
                                max(0.0, time.time() - start_ts)
                            )
                        except Exception:
                            pass
                        return True, f"å†…å®¹é‡å¤: {reason}"

            # 4. æ™ºèƒ½ç›¸ä¼¼åº¦ï¼ˆè§†é¢‘æˆ–ç›¸å†Œé»˜è®¤è·³è¿‡ï¼‰
            logger.debug("å¼€å§‹æ™ºèƒ½ç›¸ä¼¼åº¦æ£€æŸ¥...")
            if config.get("enable_smart_similarity"):
                # è§†é¢‘é»˜è®¤è·³è¿‡æ–‡æœ¬ç›¸ä¼¼åº¦
                if not (
                    is_video
                    and not config.get("enable_text_similarity_for_video", False)
                ):
                    # ç›¸å†Œ/ç»„æ¶ˆæ¯é»˜è®¤è·³è¿‡
                    if not (
                        getattr(message_obj, "grouped_id", None)
                        and config.get("disable_similarity_for_grouped", True)
                    ):
                        logger.debug("æ‰§è¡Œç›¸ä¼¼åº¦æ£€æŸ¥...")
                        is_dup, reason = await self._check_similarity_duplicate(
                            message_obj, target_chat_id, config
                        )
                        logger.debug(f"ç›¸ä¼¼åº¦æ£€æŸ¥ç»“æœ: {is_dup}, {reason}")
                        if is_dup:
                            try:
                                from core.helpers.metrics import (
                                    DEDUP_DECISIONS_TOTAL,
                                    DEDUP_HITS_TOTAL,
                                )

                                DEDUP_HITS_TOTAL.labels(method="similarity").inc()
                                DEDUP_DECISIONS_TOTAL.labels(
                                    result="duplicate", method="similarity"
                                ).inc()
                                from core.helpers.metrics import DEDUP_CHECK_SECONDS

                                DEDUP_CHECK_SECONDS.observe(
                                    max(0.0, time.time() - start_ts)
                                )
                            except Exception:
                                pass
                            return True, f"ç›¸ä¼¼é‡å¤: {reason}"

            # å¦‚æœæ£€æŸ¥é€šè¿‡ï¼Œè®°å½•åˆ°ç¼“å­˜ï¼ˆåªè¯»æ¨¡å¼ä¸è®°å½•ï¼‰
            if not readonly:
                logger.debug("è®°å½•æ¶ˆæ¯åˆ°ç¼“å­˜...")
                await self._record_message(
                    message_obj, target_chat_id, signature, content_hash
                )
                # è®°å½•åˆ° HLL (ç»Ÿè®¡ç‹¬ç«‹æ¶ˆæ¯)
                if self.hll:
                    msg_id = getattr(message_obj, "id", None)
                    chat_id = getattr(message_obj, "chat_id", None)
                    if msg_id and chat_id:
                        self.hll.add(f"{chat_id}:{msg_id}")

                # åŒæ—¶è®°å½•åˆ° Bloom Filter
                # åŒæ—¶è®°å½•åˆ° Bloom Filter
                if self.bloom_filter:
                    if signature: self.bloom_filter.add(f"sig:{target_chat_id}:{signature}")
                    if content_hash: self.bloom_filter.add(f"hash:{target_chat_id}:{content_hash}")

            try:
                from core.helpers.metrics import DEDUP_DECISIONS_TOTAL

                DEDUP_DECISIONS_TOTAL.labels(result="unique", method="final").inc()
                from core.helpers.metrics import DEDUP_CHECK_SECONDS

                DEDUP_CHECK_SECONDS.observe(max(0.0, time.time() - start_ts))
            except Exception:
                pass
            logger.debug(
                f"å»é‡æ£€æŸ¥å®Œæˆï¼Œè€—æ—¶: {time.time() - start_ts:.3f}sï¼Œç»“æœ: ä¸é‡å¤"
            )
            return False, "æ— é‡å¤"

        except Exception as e:
            logger.error(f"æ™ºèƒ½å»é‡æ£€æŸ¥å¤±è´¥: {e}")
            try:
                from core.helpers.metrics import DEDUP_CHECK_SECONDS, DEDUP_DECISIONS_TOTAL

                DEDUP_DECISIONS_TOTAL.labels(result="error", method="final").inc()
                DEDUP_CHECK_SECONDS.observe(max(0.0, time.time() - start_ts))
            except Exception:
                pass
            return False, f"æ£€æŸ¥å¤±è´¥: {e}"

    def _generate_signature(self, message_obj) -> Optional[str]:
        """ç”Ÿæˆæ¶ˆæ¯ç­¾åï¼ˆä¸ç°æœ‰ç³»ç»Ÿå…¼å®¹ï¼‰"""
        try:
            if hasattr(message_obj, "photo") and message_obj.photo:
                # ç…§ç‰‡ç­¾å
                photo = message_obj.photo
                if hasattr(photo, "sizes") and photo.sizes:
                    largest = max(photo.sizes, key=lambda x: getattr(x, "size", 0))
                    w = getattr(largest, "w", 0)
                    h = getattr(largest, "h", 0)
                    size = getattr(largest, "size", 0)
                    return f"photo:{w}x{h}:{size}"

            elif hasattr(message_obj, "document") and message_obj.document:
                # æ–‡æ¡£ç­¾å
                doc = message_obj.document
                doc_id = getattr(doc, "id", "")
                size = getattr(doc, "size", 0)
                mime_type = getattr(doc, "mime_type", "")
                return f"document:{doc_id}:{size}:{mime_type}"

            elif hasattr(message_obj, "video") and message_obj.video:
                # è§†é¢‘ç­¾å
                video = message_obj.video
                # ä¼˜å…ˆä½¿ç”¨ telegram file idï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¦åˆ™å›é€€åˆ°åŸæœ‰è§„åˆ™
                file_id = getattr(video, "id", "") or getattr(
                    video, "file_reference", ""
                )
                duration = getattr(video, "duration", 0)
                return f"video:{file_id or video}:{duration}"

            # æŸäº›å®¢æˆ·ç«¯å°†è§†é¢‘æš´éœ²åœ¨ document ä¸­ï¼Œè¿™é‡Œå…œåº•
            elif (
                hasattr(message_obj, "document")
                and message_obj.document
                and str(getattr(message_obj.document, "mime_type", "")).startswith(
                    "video/"
                )
            ):
                file_id = getattr(message_obj.document, "id", "") or getattr(
                    message_obj.document, "file_reference", ""
                )
                duration = int(
                    getattr(getattr(message_obj, "video", None), "duration", 0) or 0
                )
                return f"video:{file_id}:{duration}"

            return None

        except Exception as e:
            logger.debug(f"ç”Ÿæˆç­¾åå¤±è´¥: {e}")
            return None

    def _generate_content_hash(self, message_obj) -> Optional[str]:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        try:
            content_parts = []

            # æ–‡æœ¬å†…å®¹
            if hasattr(message_obj, "message") and message_obj.message:
                # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤æ ¼å¼ã€é“¾æ¥ã€æåŠç­‰ï¼‰
                text = message_obj.message
                cleaned_text = self._clean_text_for_hash(
                    text, strip_numbers=self.config.get("strip_numbers", True)
                )
                if cleaned_text:
                    content_parts.append(f"text:{cleaned_text}")

            # åª’ä½“ç‰¹å¾
            if hasattr(message_obj, "media") and message_obj.media:
                media_info = self._extract_media_features(message_obj.media)
                if media_info:
                    content_parts.append(f"media:{media_info}")

            if content_parts:
                combined = "|".join(content_parts)
                return hashlib.md5(combined.encode()).hexdigest()

            return None

        except Exception as e:
            logger.debug(f"ç”Ÿæˆå†…å®¹å“ˆå¸Œå¤±è´¥: {e}")
            return None

    def _clean_text_for_hash(self, text: str, strip_numbers: bool = False) -> str:
        """æ¸…ç†æ–‡æœ¬ç”¨äºå“ˆå¸Œè®¡ç®—"""
        if not text:
            return ""

        # 1. å…ˆç”¨æ­£åˆ™å¿«é€Ÿå‰”é™¤å¤æ‚çš„è¯­ä¹‰å— (URL, Mention)
        text = self._re_complex_patterns.sub(" ", text.lower())

        # 2. ä½¿ç”¨ C è¯­è¨€å±‚é¢çš„ translate ä¸€æ¬¡æ€§å‰”é™¤æ‰€æœ‰æ ‡ç‚¹/æ•°å­—
        table = (
            self.trans_table_no_nums if strip_numbers else self.trans_table_keep_nums
        )
        text = text.translate(table)

        # 3. åˆå¹¶ç©ºæ ¼ (split + join æ˜¯æœ€å¿«çš„æ ‡å‡†åŒ–ç©ºæ ¼æ–¹æ³•)
        return " ".join(text.split())

    def _is_video(self, message_obj) -> bool:
        """åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦å«è§†é¢‘ï¼ˆåŸç”Ÿè§†é¢‘æˆ–è§†é¢‘æ–‡æ¡£ï¼‰ã€‚"""
        try:
            if hasattr(message_obj, "video") and getattr(message_obj, "video"):
                return True
            if hasattr(message_obj, "document") and getattr(message_obj, "document"):
                mime = str(
                    getattr(getattr(message_obj, "document"), "mime_type", "") or ""
                )
                return mime.startswith("video/")
        except Exception:
            return False
        return False

    def _extract_video_file_id(self, message_obj) -> Optional[str]:
        """ä»è§†é¢‘æ¶ˆæ¯ä¸­æå–æ–‡ä»¶IDç”¨äºå»é‡å¤æ£€æŸ¥"""
        try:
            # æ£€æŸ¥åŸç”Ÿè§†é¢‘
            if hasattr(message_obj, "video") and getattr(message_obj, "video"):
                video = message_obj.video
                file_id = getattr(video, "id", None) or getattr(
                    video, "file_reference", None
                )
                if file_id:
                    return str(file_id)

            # æ£€æŸ¥è§†é¢‘æ–‡æ¡£
            if hasattr(message_obj, "document") and getattr(message_obj, "document"):
                doc = message_obj.document
                mime = str(getattr(doc, "mime_type", "") or "")
                if mime.startswith("video/"):
                    file_id = getattr(doc, "id", None) or getattr(
                        doc, "file_reference", None
                    )
                    if file_id:
                        return str(file_id)

            return None

        except Exception as e:
            logger.debug(f"æå–è§†é¢‘æ–‡ä»¶IDå¤±è´¥: {e}")
            return None

    def _extract_media_features(self, media) -> Optional[str]:
        """æå–åª’ä½“ç‰¹å¾"""
        try:
            features = []

            if hasattr(media, "photo"):
                features.append("type:photo")
                photo = media.photo
                if hasattr(photo, "sizes") and photo.sizes:
                    # ä½¿ç”¨å°ºå¯¸ç‰¹å¾è€ŒéID
                    largest = max(photo.sizes, key=lambda x: getattr(x, "size", 0))
                    w = getattr(largest, "w", 0)
                    h = getattr(largest, "h", 0)
                    features.append(f"size:{w}x{h}")

            elif hasattr(media, "document"):
                doc = media.document
                features.append("type:document")

                # æ–‡ä»¶å¤§å°
                if hasattr(doc, "size"):
                    # ä½¿ç”¨å¤§å°èŒƒå›´è€Œéç²¾ç¡®å€¼
                    size_range = self._get_size_range(doc.size)
                    features.append(f"size_range:{size_range}")

                # MIMEç±»å‹
                if hasattr(doc, "mime_type"):
                    features.append(f"mime:{doc.mime_type}")

                # æ–‡ä»¶åæ¨¡å¼ï¼ˆç§»é™¤æ•°å­—ã€æ—¥æœŸç­‰å˜åŒ–éƒ¨åˆ†ï¼‰
                if hasattr(doc, "file_name") and doc.file_name:
                    name_pattern = self._extract_name_pattern(doc.file_name)
                    if name_pattern:
                        features.append(f"pattern:{name_pattern}")

                # è‹¥ä¸ºè§†é¢‘æ–‡æ¡£ï¼ŒåŠ å…¥æ›´ç¨³å®šçš„ç»´åº¦
                try:
                    if getattr(doc, "mime_type", "").startswith("video/"):
                        duration = getattr(
                            getattr(media, "video", None), "duration", None
                        )
                        if duration:
                            features.append(f"duration:{int(duration)}")
                except Exception:
                    pass

            return "|".join(features) if features else None

        except Exception as e:
            logger.debug(f"æå–åª’ä½“ç‰¹å¾å¤±è´¥: {e}")
            return None

    def _get_size_range(self, size: int) -> str:
        """è·å–æ–‡ä»¶å¤§å°èŒƒå›´"""
        if size < 1024:
            return "tiny"
        elif size < 1024 * 1024:
            return "small"
        elif size < 10 * 1024 * 1024:
            return "medium"
        elif size < 100 * 1024 * 1024:
            return "large"
        else:
            return "huge"

    def _size_bucket_index(self, bucket: str) -> int:
        order = ["tiny", "small", "medium", "large", "huge"]
        try:
            return order.index(bucket)
        except Exception:
            return -1

    def _extract_name_pattern(self, filename: str) -> str:
        """æå–æ–‡ä»¶åæ¨¡å¼"""
        # ç§»é™¤æ—¥æœŸæ—¶é—´
        pattern = re.sub(r"\d{4}[-_]\d{2}[-_]\d{2}", "DATE", filename)
        pattern = re.sub(r"\d{2}[-_:]\d{2}[-_:]\d{2}", "TIME", pattern)

        # ç§»é™¤æ•°å­—åºåˆ—
        pattern = re.sub(r"\d{3,}", "NUM", pattern)

        # ä¿ç•™æ‰©å±•å
        if "." in pattern:
            name, ext = pattern.rsplit(".", 1)
            pattern = re.sub(r"[^\w\.]", "_", name) + "." + ext

        return pattern.lower()

    async def _check_signature_duplicate(
        self, signature: str, target_chat_id: int, config: Dict
    ) -> Tuple[bool, str]:
        """æ£€æŸ¥ç­¾åé‡å¤"""
        try:
            # æ—¶é—´çª—å£æ£€æŸ¥
            if config.get("enable_time_window"):
                cache_key = str(target_chat_id)
                if cache_key in self.time_window_cache:
                    if signature in self.time_window_cache[cache_key]:
                        last_seen = self.time_window_cache[cache_key][signature]
                        window_hours = config.get("time_window_hours", 24)
                        # æ°¸ä¹…çª—å£ï¼š<=0 è§†ä¸ºæ°¸ä¹…
                        if (
                            window_hours <= 0
                            or time.time() - last_seen < window_hours * 3600
                        ):
                            return (
                                True,
                                f"æ—¶é—´çª—å£å†…é‡å¤ ({'æ°¸ä¹…' if window_hours <= 0 else str(window_hours)+'å°æ—¶'})",
                            )

            # æ•°æ®åº“æ£€æŸ¥
            exists = await self.repo.exists_media_signature(str(target_chat_id), signature)
            if exists: return True, "æ•°æ®åº“ä¸­å­˜åœ¨"
            # å†·åŒºå…œåº•ï¼šè‹¥å¼€å¯æ°¸ä¹…çª—å£ï¼ˆtime_window_hours<=0ï¼‰æˆ–çƒ­åŒºæœªå‘½ä¸­æ—¶å¯è¿›ä¸€æ­¥æŸ¥è¯¢å½’æ¡£
            try:
                if config.get("time_window_hours", 24) <= 0:
                    from utils.bloom_index import bloom

                    # å…ˆç”¨ Bloom åˆ¤æ–­å¯èƒ½å­˜åœ¨ï¼Œå†åšå†·æŸ¥ç¡®è®¤
                    if bloom.probably_contains(
                        "media_signatures", str(target_chat_id), str(signature)
                    ):
                        from utils.archive_store import query_parquet_duckdb
                        from core.helpers.metrics import DEDUP_HITS_TOTAL, DEDUP_QUERIES_TOTAL

                        DEDUP_QUERIES_TOTAL.labels(method="signature").inc()
                        rows = query_parquet_duckdb(
                            "media_signatures",
                            "chat_id = ? AND signature = ?",
                            [str(target_chat_id), str(signature)],
                            columns=["chat_id"],
                            order_by="created_at DESC",
                            limit=1,
                            max_days=int(os.getenv("ARCHIVE_COLD_LOOKBACK_DAYS", "30")),
                        )
                        if rows:
                            DEDUP_HITS_TOTAL.labels(method="signature").inc()
                            return True, "å½’æ¡£å†·åŒºå‘½ä¸­"
            except Exception:
                pass
            return False, ""

        except Exception as e:
            logger.debug(f"ç­¾åé‡å¤æ£€æŸ¥å¤±è´¥: {e}")
            return False, ""

    async def _check_content_hash_duplicate(
        self, content_hash: str, target_chat_id: int, config: Dict
    ) -> Tuple[bool, str]:
        """æ£€æŸ¥å†…å®¹å“ˆå¸Œé‡å¤"""
        try:
            cache_key = str(target_chat_id)
            if cache_key in self.content_hash_cache:
                if content_hash in self.content_hash_cache[cache_key]:
                    last_seen = self.content_hash_cache[cache_key][content_hash]
                    window_hours = config.get("time_window_hours", 24)
                    # æ°¸ä¹…çª—å£ï¼š<=0 è§†ä¸ºæ°¸ä¹…
                    if (
                        window_hours <= 0
                        or time.time() - last_seen < window_hours * 3600
                    ):
                        return (
                            True,
                            f"å†…å®¹å“ˆå¸Œé‡å¤ ({'æ°¸ä¹…' if window_hours <= 0 else str(window_hours)+'å°æ—¶å†…'})",
                        )
            # å†·åŒºå…œåº•ï¼šæ°¸ä¹…çª—å£æˆ–çƒ­åŒºæœªå‘½ä¸­æ—¶æŸ¥è¯¢å½’æ¡£
            try:
                if config.get("time_window_hours", 24) <= 0:
                    from utils.bloom_index import bloom

                    if bloom.probably_contains(
                        "media_signatures", str(target_chat_id), str(content_hash)
                    ):
                        from utils.archive_store import query_parquet_duckdb
                        from core.helpers.metrics import DEDUP_HITS_TOTAL, DEDUP_QUERIES_TOTAL

                        DEDUP_QUERIES_TOTAL.labels(method="content_hash").inc()
                        rows = query_parquet_duckdb(
                            "media_signatures",
                            "chat_id = ? AND content_hash = ?",
                            [str(target_chat_id), str(content_hash)],
                            columns=["chat_id"],
                            order_by="created_at DESC",
                            limit=1,
                            max_days=int(os.getenv("ARCHIVE_COLD_LOOKBACK_DAYS", "30")),
                        )
                        if rows:
                            DEDUP_HITS_TOTAL.labels(method="content_hash").inc()
                            return True, "å½’æ¡£å†·åŒºå‘½ä¸­"
            except Exception:
                pass
            return False, ""

        except Exception as e:
            logger.debug(f"å†…å®¹å“ˆå¸Œæ£€æŸ¥å¤±è´¥: {e}")
            return False, ""

    def _get_lsh_forest(self, chat_id: str) -> Any:
        # å†…éƒ¨æ–¹æ³•è·å–å¯¹åº”ä¼šè¯çš„ç´¢å¼•
        if chat_id not in self.lsh_forests:
            try:
                from utils.algorithm.lsh_forest import LSHForest
                # ä½¿ç”¨é»˜è®¤ 8 æ£µæ ‘ï¼Œå‰ç¼€é•¿åº¦æ ¹æ® Hamming é˜ˆå€¼è°ƒæ•´
                # è¿™é‡Œæˆ‘ä»¬ä¿æŒé»˜è®¤ 64bit å¤„ç†ï¼ŒLSHForest å†…éƒ¨å¤„ç†æ’åˆ—
                self.lsh_forests[chat_id] = LSHForest(num_trees=8, prefix_length=64)
            except Exception:
                return None
        return self.lsh_forests[chat_id]

    async def _check_similarity_duplicate(
        self, message_obj, target_chat_id: int, config: Dict
    ) -> Tuple[bool, str]:
        """æ£€æŸ¥ç›¸ä¼¼åº¦é‡å¤"""
        try:
            if not hasattr(message_obj, "message") or not message_obj.message:
                return False, ""

            current_text = self._clean_text_for_hash(
                message_obj.message,
                strip_numbers=self.config.get("strip_numbers", True),
            )
            min_len = int(self.config.get("min_text_length", 10))
            if len(current_text) < min_len:  # å¤ªçŸ­çš„æ–‡æœ¬ä¸æ£€æŸ¥ç›¸ä¼¼åº¦
                return False, ""

            # ä»æ–‡æœ¬ç¼“å­˜ä¸­æŸ¥æ‰¾ç›¸ä¼¼æ–‡æœ¬
            cache_key = str(target_chat_id)
            if cache_key not in self.text_cache:
                return False, ""

            threshold = config.get("similarity_threshold", 0.85)
            window_hours = config.get("time_window_hours", 24)
            current_time = time.time()

            # å¯é€‰ï¼šå…ˆç”¨å›ºå®šé•¿åº¦æŒ‡çº¹åšé¢„ç­›ï¼ŒO(N) æ±‰æ˜è·ç¦»ï¼Œæ¯”ç²¾é…æ›´å¿«
            current_fp = None
            comparisons = 0
            if config.get("enable_text_fingerprint", True):
                try:
                    current_fp = self._compute_text_fingerprint(
                        current_text, int(config.get("fingerprint_ngram", 3))
                    )
                    idx = self._get_lsh_forest(cache_key)
                    if idx and current_fp is not None:
                        # ä½¿ç”¨ LSHForest è¿›è¡Œè¿‘ä¼¼æŸ¥è¯¢
                        # è¿”å› doc_id åˆ—è¡¨ï¼Œè¿™é‡Œæˆ‘ä»¬å­˜çš„æ˜¯ timestamp
                        hits = idx.query(current_fp, top_k=5)
                        if hits:
                            for ts_str in hits:
                                try:
                                    ts = float(ts_str)
                                except ValueError:
                                    continue
                                
                                if window_hours > 0 and current_time - ts > window_hours * 3600:
                                    continue
                                
                                # LSH å‘½ä¸­å³è§†ä¸ºç›¸ä¼¼ (Phase 5 ç­–ç•¥ï¼šä¿¡ä»» SimHash ä»¥æ”¯æŒç™¾ä¸‡çº§)
                                # å¦‚æœéœ€è¦æ›´ç²¾ç¡®ï¼Œå¯ä»¥å» text_cache æå– (ä½† text_cache å¯èƒ½å·²è¢«æˆªæ–­)
                                
                                # å°è¯•åœ¨ text_cache ä¸­æ‰¾å›åŸæ–‡è¿›è¡Œæ ¸å¯¹ (Best Effort)
                                # å¦‚æœæ‰¾ä¸åˆ°åŸæ–‡ï¼Œé‰´äº LSH/SimHash çš„å¼ºå»é‡æ€§è´¨ï¼Œæˆ‘ä»¬ä¹Ÿè®¤ä½œé‡å¤
                                prev_text = None
                                if cache_key in self.text_cache:
                                    for item in self.text_cache[cache_key]:
                                        if abs(item['ts'] - ts) < 0.001:
                                            prev_text = item['text']
                                            break
                                
                                if prev_text:
                                    # æœ‰åŸæ–‡ï¼Œè¿›è¡Œç²¾ç¡®æ¯”å¯¹
                                    sim = self._calculate_text_similarity(current_text, prev_text)
                                    if sim >= config.get("similarity_threshold", 0.85):
                                        try:
                                            from core.helpers.metrics import DEDUP_FP_HITS_TOTAL
                                            DEDUP_FP_HITS_TOTAL.labels(algo="lsh_forest").inc()
                                        except Exception:
                                            pass
                                        return True, f"æŒ‡çº¹ç´¢å¼•å‘½ä¸­ä¸”å†…å®¹æ ¡éªŒé€šè¿‡ ({sim:.2f})"
                                else:
                                    # åŸæ–‡å·²ä¸¢å¤±ï¼Œä½† LSH å¼ºåŒ¹é… -> åˆ¤å®šé‡å¤ (ä¿¡ä»» SimHash)
                                    # è¿™é‡Œå‡è®¾ LSH çš„ recall ä¸»è¦æ˜¯çœŸé˜³æ€§
                                    return True, "LSHç´¢å¼•å‘½ä¸­ (åŸæ–‡å·²å½’æ¡£)"

                except Exception as e:
                    logger.debug(f"SimHashIndex æ£€æŸ¥å¤±è´¥: {e}")

            # æ£€æŸ¥æœ€è¿‘çš„æ¶ˆæ¯ï¼ˆå€’åºï¼Œä¼˜å…ˆæ¯”è¾ƒæœ€æ–°çš„ï¼‰
            comparisons = 0
            curr_len = len(current_text)

            for item in reversed(self.text_cache[cache_key]):
                ts = item.get("ts")
                # æ°¸ä¹…çª—å£ï¼šä¸ä¼šå› æ—¶é—´è¿‡æœŸè€Œè·³è¿‡
                if window_hours > 0 and current_time - ts > window_hours * 3600:
                    continue
                prev_text = item.get("text", "")
                prev_len = len(prev_text)
                if not prev_len:
                    continue
                if prev_text == current_text:
                    return True, "æ–‡æœ¬å®Œå…¨ä¸€è‡´"

                # âœ… ä¼˜åŒ–ï¼šæ•°å­¦å‰ªæ
                # è®¡ç®—é•¿åº¦å·®å¼‚æ¯”ç‡ã€‚å¦‚æœé•¿åº¦å·®å æ¯”è¶…è¿‡ (1 - é˜ˆå€¼)ï¼Œåˆ™ä¸å¯èƒ½åŒ¹é…ã€‚
                # ä¸¾ä¾‹ï¼šé˜ˆå€¼ 0.8ï¼Œcurr=100ã€‚å¦‚æœ prev < 80 æˆ– prev > 125ï¼Œåˆ™å¿…ä¸åŒ¹é…ã€‚
                # Jaccard ä¸Šé™ä¼°ç®—ï¼šmin_len / max_len < threshold
                if prev_len < curr_len:
                    upper_bound = prev_len / curr_len
                else:
                    upper_bound = curr_len / prev_len

                if upper_bound < threshold:
                    continue  # è·³è¿‡æ˜‚è´µçš„è¯¦ç»†æ¯”å¯¹

                # æ§åˆ¶ç²¾ç¡®æ¯”è¾ƒçš„ä¸Šé™ï¼Œé¿å… O(N) è¿‡å¤§
                if comparisons >= int(config.get("max_similarity_checks", 50)):
                    break
                similarity = self._calculate_text_similarity(current_text, prev_text)
                comparisons += 1
                if similarity >= threshold:
                    return True, f"æ–‡æœ¬ç›¸ä¼¼åº¦ {similarity:.2f} â‰¥ {threshold}"

            try:
                from core.helpers.metrics import DEDUP_SIMILARITY_COMPARISONS

                DEDUP_SIMILARITY_COMPARISONS.observe(float(comparisons))
            except Exception:
                pass
            return False, ""
        except Exception as e:
            logger.debug(f"ç›¸ä¼¼åº¦æ£€æŸ¥å¤±è´¥: {e}")
            return False, ""

    async def _check_video_duplicate_by_file_id(self, file_id: str, target_chat_id: int) -> bool:
        try:
            res = await self.repo.find_by_file_id_or_hash(str(target_chat_id), file_id=file_id)
            return res is not None
        except Exception: return False

    async def _check_video_duplicate_by_hash(self, vhash: str, target_chat_id: int) -> bool:
        try:
            res = await self.repo.find_by_file_id_or_hash(str(target_chat_id), content_hash=vhash)
            return res is not None
        except Exception: return False

    async def _compute_video_partial_hash(
        self, message_obj, partial_bytes: int
    ) -> Optional[str]:
        """ä¼˜åŒ–ç‰ˆï¼šæµå¼ä¸‹è½½è§†é¢‘å¤´å°¾éƒ¨åˆ†å­—èŠ‚å¹¶è®¡ç®—ç»„åˆå“ˆå¸Œï¼Œé¿å…å…¨é‡ä¸‹è½½"""
        logger.debug("å¼€å§‹è®¡ç®—è§†é¢‘éƒ¨åˆ†å“ˆå¸Œ...")
        try:
            if not getattr(message_obj, "media", None):
                logger.debug("æ¶ˆæ¯æ— åª’ä½“å¯¹è±¡ï¼Œè·³è¿‡å“ˆå¸Œè®¡ç®—")
                return None
            from core.helpers.metrics import VIDEO_PARTIAL_HASH_SECONDS

            _start = time.time()
            # è·å–æ–‡ä»¶æ€»å¤§å°
            doc = getattr(message_obj, "document", None)
            if not doc and hasattr(message_obj, "video"):
                logger.debug("ä»videoå­—æ®µè·å–åª’ä½“å¯¹è±¡")
                doc = message_obj.video
            if not doc:
                logger.debug("æ— æ³•è·å–åª’ä½“å¯¹è±¡ï¼Œè·³è¿‡å“ˆå¸Œè®¡ç®—")
                return None
            total_size = getattr(doc, "size", 0)
            logger.debug(f"è§†é¢‘æ€»å¤§å°: {total_size}å­—èŠ‚")
            if total_size == 0:
                logger.debug("è§†é¢‘å¤§å°ä¸º0ï¼Œè·³è¿‡å“ˆå¸Œè®¡ç®—")
                return None

            # âœ… ä¼˜åŒ–ï¼šä½¿ç”¨ xxh64 æ›¿ä»£ md5
            if _HAS_XXHASH:
                logger.debug("ä½¿ç”¨xxh64ç®—æ³•è®¡ç®—å“ˆå¸Œ")
                h = xxhash.xxh64()
            else:
                logger.debug("ä½¿ç”¨md5ç®—æ³•è®¡ç®—å“ˆå¸Œ")
                import hashlib as _hash

                h = _hash.md5()

            read_len = min(partial_bytes, total_size)
            logger.debug(f"æ¯æ¬¡è¯»å–å­—èŠ‚æ•°: {read_len}")
            try:
                client = getattr(message_obj, "client", None)
                if not client:
                    logger.debug("æ— æ³•è·å–å®¢æˆ·ç«¯å¯¹è±¡ï¼Œè·³è¿‡å“ˆå¸Œè®¡ç®—")
                    return None
                logger.debug("å¼€å§‹ä¸‹è½½è§†é¢‘å¤´éƒ¨æ•°æ®...")
                # å¤´éƒ¨
                head_data = bytearray()
                async for chunk in client.iter_download(doc, limit=read_len):
                    head_data.extend(chunk)
                    logger.debug(f"å·²ä¸‹è½½å¤´éƒ¨æ•°æ®: {len(head_data)}/{read_len}å­—èŠ‚")
                logger.debug(f"å¤´éƒ¨æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(head_data)} å­—èŠ‚")
                h.update(head_data)
                # å°¾éƒ¨æˆ–ä¸­é—´æ®µ
                if total_size > read_len * 2:
                    logger.debug("è§†é¢‘è¾ƒå¤§ï¼Œä¸‹è½½å°¾éƒ¨æ•°æ®...")
                    offset = total_size - read_len
                    tail_data = bytearray()
                    async for chunk in client.iter_download(
                        doc, offset=offset, limit=read_len
                    ):
                        tail_data.extend(chunk)
                        logger.debug(f"å·²ä¸‹è½½å°¾éƒ¨æ•°æ®: {len(tail_data)}/{read_len}å­—èŠ‚")
                    logger.debug(f"å°¾éƒ¨æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(tail_data)} å­—èŠ‚")
                    h.update(tail_data)
                elif total_size > read_len:
                    logger.debug("è§†é¢‘ä¸­ç­‰å¤§å°ï¼Œä¸‹è½½ä¸­é—´æ®µæ•°æ®...")
                    mid_offset = total_size // 2
                    mid_data = bytearray()
                    async for chunk in client.iter_download(
                        doc, offset=mid_offset, limit=read_len
                    ):
                        mid_data.extend(chunk)
                        logger.debug(
                            f"å·²ä¸‹è½½ä¸­é—´æ®µæ•°æ®: {len(mid_data)}/{read_len}å­—èŠ‚"
                        )
                    logger.debug(f"ä¸­é—´æ®µæ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(mid_data)} å­—èŠ‚")
                    h.update(mid_data)
            except Exception as e:
                logger.error(f"æµå¼ä¸‹è½½éƒ¨åˆ†å†…å®¹å¤±è´¥: {e}")
                return None
            try:
                VIDEO_PARTIAL_HASH_SECONDS.observe(max(0.0, time.time() - _start))
            except Exception:
                pass
            hash_result = h.hexdigest()
            logger.debug(
                f"è§†é¢‘éƒ¨åˆ†å“ˆå¸Œè®¡ç®—å®Œæˆï¼Œç»“æœ: {hash_result}ï¼Œè€—æ—¶: {time.time() - _start:.3f}s"
            )
            return hash_result
        except Exception as e:
            logger.error(f"è®¡ç®—è§†é¢‘éƒ¨åˆ†å“ˆå¸Œå¤±è´¥: {e}")
            return None

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        # 1. ä¼˜å…ˆä½¿ç”¨ SimHash (O(1) è®°å¿†å‹ç®—æ³•)
        if self.simhash_engine:
            try:
                # æ³¨æ„ï¼šè¿™é‡Œå¦‚æœèƒ½æå‰ç®—å¥½ fp æ›´å¥½ï¼Œä½†åœ¨æ¯”è¾ƒæ—¶ç®—ä¹Ÿè¡Œ
                fp1 = self.simhash_engine.build_fingerprint(text1)
                fp2 = self.simhash_engine.build_fingerprint(text2)
                return self.simhash_engine.similarity(fp1, fp2)
            except Exception as e:
                logger.debug(f"SimHash è®¡ç®—å¤±è´¥: {e}")

        # 2. å¤‡é€‰æ–¹æ¡ˆ
        try:
            if _HAS_RAPIDFUZZ:
                return float(fuzz.token_set_ratio(text1, text2)) / 100.0

            # ä½¿ç”¨ Token-based Jaccard Similarityï¼Œå¤æ‚åº¦ O(N + M)
            # ç®€å•çš„ç©ºæ ¼åˆ†è¯ï¼ˆå› ä¸º _clean_text_for_hash å·²ç»å¤„ç†è¿‡æ ‡ç‚¹ï¼‰
            set1 = set(text1.split())
            set2 = set(text2.split())

            if not set1 or not set2:
                return 0.0

            intersection = len(set1 & set2)
            union = len(set1 | set2)

            return intersection / union if union > 0 else 0.0

        except Exception:
            return 0.0

    async def _record_message(
        self,
        message_obj,
        target_chat_id: int,
        signature: Optional[str],
        content_hash: Optional[str],
    ):
        """è®°å½•æ¶ˆæ¯åˆ°ç¼“å­˜"""
        try:
            current_time = time.time()
            cache_key = str(target_chat_id)

            # è®°å½•ç­¾å
            if signature:
                if cache_key not in self.time_window_cache:
                    self.time_window_cache[cache_key] = OrderedDict()
                self.time_window_cache[cache_key][signature] = current_time
                self.time_window_cache[cache_key].move_to_end(signature)

            # è®°å½•å†…å®¹å“ˆå¸Œ
            if content_hash:
                if cache_key not in self.content_hash_cache:
                    self.content_hash_cache[cache_key] = OrderedDict()
                self.content_hash_cache[cache_key][content_hash] = current_time
                self.content_hash_cache[cache_key].move_to_end(content_hash)

            # [Optimization] æ–‡æœ¬ SimHash æŒ‡çº¹ç´¢å¼•åŒ–
            if hasattr(message_obj, "message") and message_obj.message:
                text = message_obj.message
                cleaned = self._clean_text_for_hash(text)
                if cleaned:
                    fp = self._compute_text_fingerprint(cleaned)
                    if fp is not None:
                        idx = self._get_simhash_index(cache_key)
                        if idx:
                            # åœ¨ç´¢å¼•ä¸­å­˜å‚¨ (text, timestamp)
                            idx.add((cleaned, current_time), fp)

            # å†™å…¥æŒä¹…åŒ–ç¼“å­˜ï¼ˆç”¨äºè·¨é‡å¯å»é‡çƒ­å‘½ä¸­ï¼‰
            try:
                await self._write_pcache(signature, content_hash, cache_key)
            except Exception:
                pass

            # è§†é¢‘ï¼šæŒä¹…åŒ–è®°å½• file_id ä¸å†…å®¹å“ˆå¸Œï¼ˆè‹¥æœ‰ï¼‰ï¼Œä¾¿äºåç»­åˆ¤é‡
            try:
                from datetime import datetime

                is_video = (hasattr(message_obj, "video") and message_obj.video) or (
                    hasattr(message_obj, "document")
                    and message_obj.document
                    and getattr(getattr(message_obj, "document"), "mime_type", "")
                    and str(message_obj.document.mime_type).startswith("video/")
                )
                if is_video:
                    file_id = getattr(message_obj, "_tf_file_id", None)
                    vhash = getattr(message_obj, "_tf_content_hash", None)
                    # æ„å»ºå°½é‡ç¨³å®šçš„ç­¾å
                    stable_sig = (
                        signature
                        or (f"video:{file_id}" if file_id else None)
                        or (f"video_hash:{vhash}" if vhash else None)
                    )
                    if stable_sig:
                        # æå–ä¸€äº›é™„åŠ å±æ€§
                        duration = int(
                            getattr(getattr(message_obj, "video", None), "duration", 0)
                            or 0
                        )
                        width = int(
                            getattr(getattr(message_obj, "video", None), "w", 0) or 0
                        )
                        height = int(
                            getattr(getattr(message_obj, "video", None), "h", 0) or 0
                        )
                        mime_type = None
                        file_size = None
                        file_name = None
                        if hasattr(message_obj, "document") and getattr(
                            message_obj, "document"
                        ):
                            mime_type = getattr(message_obj.document, "mime_type", None)
                            file_size = getattr(message_obj.document, "size", None)
                            file_name = getattr(message_obj.document, "file_name", None)

                        # âœ… ä¼˜åŒ–ï¼šä»…åŠ å…¥å†…å­˜ Bufferï¼Œä¸ç«‹å³å†™åº“
                        payload = {
                            "chat_id": str(target_chat_id),
                            "signature": stable_sig,
                            "file_id": str(file_id) if file_id else None,
                            "content_hash": str(vhash) if vhash else None,
                            "message_id": getattr(message_obj, "id", None),
                            "media_type": "video",
                            "file_size": file_size,
                            "file_name": file_name,
                            "mime_type": mime_type,
                            "duration": duration,
                            "width": width,
                            "height": height,
                            "created_at": datetime.utcnow().isoformat(),
                            "updated_at": datetime.utcnow().isoformat(),
                            "last_seen": datetime.utcnow().isoformat(),
                            "count": 1,
                        }
                        async with self._buffer_lock:
                            self._write_buffer.append(payload)

                        # ç¡®ä¿åå°ä»»åŠ¡åœ¨è¿è¡Œ
                        await self._ensure_flush_task()
            except Exception as pe:
                logger.debug(f"æŒä¹…åŒ–è§†é¢‘ç­¾åå¤±è´¥: {pe}")

            # è®°å½•æ–‡æœ¬ï¼ˆç”¨äºç›¸ä¼¼åº¦åˆ¤é‡ï¼‰
            if hasattr(message_obj, "message") and message_obj.message:
                cleaned_text = self._clean_text_for_hash(
                    message_obj.message,
                    strip_numbers=self.config.get("strip_numbers", True),
                )
                min_len = int(self.config.get("min_text_length", 10))
                if len(cleaned_text) >= min_len:
                    if cache_key not in self.text_cache:
                        self.text_cache[cache_key] = []
                    self.text_cache[cache_key].append(
                        {"text": cleaned_text, "ts": current_time}
                    )
                    # æ§åˆ¶æ¯ä¸ªä¼šè¯çš„æ–‡æœ¬ç¼“å­˜ä¸Šé™
                    max_size = int(self.config.get("max_text_cache_size", 300))
                    if len(self.text_cache[cache_key]) > max_size:
                        overflow = len(self.text_cache[cache_key]) - max_size
                        if overflow > 0:
                            self.text_cache[cache_key] = self.text_cache[cache_key][
                                overflow:
                            ]
                    # è®°å½•æ–‡æœ¬æŒ‡çº¹ï¼ˆSimHashï¼‰
                    try:
                        if self.config.get("enable_text_fingerprint", True):
                            fp = self._compute_text_fingerprint(
                                cleaned_text,
                                int(self.config.get("fingerprint_ngram", 3)),
                            )
                            if fp is not None:
                                if cache_key not in self.text_fp_cache:
                                    self.text_fp_cache[cache_key] = []
                                self.text_fp_cache[cache_key].append(
                                    {"fp": fp, "ts": current_time}
                                )
                                fp_max = int(
                                    self.config.get("max_text_fp_cache_size", 500)
                                )
                                self.text_fp_cache[cache_key] = self.text_fp_cache[
                                        cache_key
                                    ][-fp_max:]
                                
                                # âœ… å°†æŒ‡çº¹åŠ å…¥ LSH Forest
                                forest = self._get_lsh_forest(cache_key)
                                if forest:
                                    # doc_id å­˜ä¸º timestamp å­—ç¬¦ä¸²
                                    forest.add(str(current_time), fp)
                                    
                    except Exception:
                        pass

        except Exception as e:
            logger.debug(f"è®°å½•æ¶ˆæ¯å¤±è´¥: {e}")

    async def _check_pcache_hit(
        self, kind: str, target_chat_id: int, value: str
    ) -> bool:
        """æ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜æ˜¯å¦å‘½ä¸­ã€‚kind: 'sig' | 'hash'"""
        try:
            if not self.config.get("enable_persistent_cache", True):
                logger.debug("æŒä¹…åŒ–ç¼“å­˜å·²ç¦ç”¨")
                return False
            from repositories.persistent_cache import get_persistent_cache

            pc = get_persistent_cache()
            key = f"dedup:{kind}:{target_chat_id}:{value}"
            logger.debug(f"æ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜ï¼Œkey: {key}")
            result = pc.get(key) is not None
            logger.debug(f"æŒä¹…åŒ–ç¼“å­˜æ£€æŸ¥ç»“æœ: {result}")
            return result
        except Exception as e:
            logger.debug(f"æ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")
            return False

    async def _write_pcache(
        self, signature: Optional[str], content_hash: Optional[str], cache_chat_key: str
    ) -> None:
        """å°†ç­¾åæˆ–å†…å®¹å“ˆå¸Œå†™å…¥æŒä¹…åŒ–ç¼“å­˜ï¼Œå¸¦ TTLã€‚"""
        if not self.config.get("enable_persistent_cache", True):
            logger.debug("æŒä¹…åŒ–ç¼“å­˜å·²ç¦ç”¨ï¼Œè·³è¿‡å†™å…¥")
            return
        try:
            from repositories.persistent_cache import dumps_json, get_persistent_cache

            pc = get_persistent_cache()
            ttl = int(self.config.get("persistent_cache_ttl_seconds", 30 * 24 * 3600))
            logger.debug(f"å¼€å§‹å†™å…¥æŒä¹…åŒ–ç¼“å­˜ï¼ŒTTL: {ttl}ç§’")
            # cache_chat_key å·²æ˜¯ str(target_chat_id)
            if signature:
                key = f"dedup:sig:{cache_chat_key}:{signature}"
                logger.debug(f"å†™å…¥æŒä¹…åŒ–ç¼“å­˜ï¼Œkey: {key}")
                pc.set(key, dumps_json({"ts": int(time.time())}), ttl)
            if content_hash:
                key = f"dedup:hash:{cache_chat_key}:{content_hash}"
                logger.debug(f"å†™å…¥æŒä¹…åŒ–ç¼“å­˜ï¼Œkey: {key}")
                pc.set(key, dumps_json({"ts": int(time.time())}), ttl)
            logger.debug("æŒä¹…åŒ–ç¼“å­˜å†™å…¥å®Œæˆ")
        except Exception as e:
            logger.debug(f"å†™å…¥æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")
            pass

    async def _ensure_flush_task(self):
        """ç¡®ä¿åå°åˆ·å†™ä»»åŠ¡åœ¨è¿è¡Œ"""
        if self._flush_task is None or self._flush_task.done():
            self._flush_task = asyncio.create_task(self._flush_worker())

    async def _flush_worker(self):
        """åå°åˆ·å†™ä»»åŠ¡ï¼šå®šæœŸå°†å†…å­˜ç¼“å†²åŒºä¸­çš„æ•°æ®æ‰¹é‡å†™å…¥æ•°æ®åº“"""
        while True:
            await asyncio.sleep(2.0)  # æ¯2ç§’åˆ·å†™ä¸€æ¬¡
            async with self._buffer_lock:
                if not self._write_buffer:
                    continue
                batch = self._write_buffer[:]
                self._write_buffer.clear()

            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            try: await self.repo.batch_add(batch)
            except Exception as e: logger.error(f"æ‰¹é‡å†™å…¥æŒ‡çº¹å¤±è´¥: {e}")

    async def _cleanup_cache_if_needed(self):
        """å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            current_time = time.time()
            if current_time - self.last_cleanup < self.config["cache_cleanup_interval"]:
                return

            max_age = self.config["time_window_hours"] * 3600 * 2  # ä¿ç•™2å€æ—¶é—´çª—å£

            # æ¸…ç†æ—¶é—´çª—å£ç¼“å­˜
            for chat_id in list(self.time_window_cache.keys()):
                cache = self.time_window_cache[chat_id]
                # åˆ©ç”¨æœ‰åºæ€§ï¼Œä»…å¤„ç†å¤´éƒ¨è¿‡æœŸé¡¹
                while cache:
                    # peek å¤´éƒ¨å…ƒç´  (key, ts)
                    key, timestamp = next(iter(cache.items()))
                    if current_time - timestamp > max_age:
                        cache.popitem(last=False)  # å¼¹å‡ºå¤´éƒ¨
                    else:
                        break  # é‡åˆ°ç¬¬ä¸€ä¸ªæœªè¿‡æœŸçš„ï¼Œåç»­éƒ½ä¸ç”¨æ£€æŸ¥

                if not cache:
                    del self.time_window_cache[chat_id]

            # æ¸…ç†å†…å®¹å“ˆå¸Œç¼“å­˜
            for chat_id in list(self.content_hash_cache.keys()):
                cache = self.content_hash_cache[chat_id]
                # åˆ©ç”¨æœ‰åºæ€§ï¼Œä»…å¤„ç†å¤´éƒ¨è¿‡æœŸé¡¹
                while cache:
                    # peek å¤´éƒ¨å…ƒç´  (key, ts)
                    key, timestamp = next(iter(cache.items()))
                    if current_time - timestamp > max_age:
                        cache.popitem(last=False)  # å¼¹å‡ºå¤´éƒ¨
                    else:
                        break  # é‡åˆ°ç¬¬ä¸€ä¸ªæœªè¿‡æœŸçš„ï¼Œåç»­éƒ½ä¸ç”¨æ£€æŸ¥

                if not cache:
                    del self.content_hash_cache[chat_id]

            # æ¸…ç†æ–‡æœ¬ç¼“å­˜
            for chat_id in list(self.text_cache.keys()):
                items = self.text_cache[chat_id]
                # ä»…ä¿ç•™åœ¨æœ‰æ•ˆæœŸå†…çš„
                items = [
                    it for it in items if current_time - it.get("ts", 0) <= max_age
                ]
                if items:
                    # å†æ¬¡ç¡®ä¿ä¸è¶…è¿‡ä¸Šé™
                    max_size = int(self.config.get("max_text_cache_size", 300))
                    if len(items) > max_size:
                        items = items[-max_size:]
                    self.text_cache[chat_id] = items
                else:
                    del self.text_cache[chat_id]
            # æ¸…ç†æ–‡æœ¬æŒ‡çº¹ç¼“å­˜
            for chat_id in list(self.text_fp_cache.keys()):
                items = self.text_fp_cache[chat_id]
                items = [
                    it for it in items if current_time - it.get("ts", 0) <= max_age
                ]
                if items:
                    fp_max = int(self.config.get("max_text_fp_cache_size", 500))
                    if len(items) > fp_max:
                        items = items[-fp_max:]
                    self.text_fp_cache[chat_id] = items
                else:
                    del self.text_fp_cache[chat_id]

            self.last_cleanup = current_time
            logger.debug("æ™ºèƒ½å»é‡ç¼“å­˜æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    def get_stats(self) -> Dict:
        """è·å–å»é‡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            total_signatures = sum(
                len(cache) for cache in self.time_window_cache.values()
            )
            total_hashes = sum(len(cache) for cache in self.content_hash_cache.values())
            total_texts = sum(len(items) for items in self.text_cache.values())

            return {
                "cached_signatures": total_signatures,
                "cached_content_hashes": total_hashes,
                "cached_texts": total_texts,
                "cached_text_fps": sum(
                    len(items) for items in self.text_fp_cache.values()
                ),
                "tracked_chats": len(self.time_window_cache),
                "config": self.config.copy(),
                "last_cleanup": self.last_cleanup,
            }
        except Exception:
            return {}

    def update_config(self, new_config: Dict):
        """æ›´æ–°é…ç½®å¹¶æŒä¹…åŒ–"""
        self.config.update(new_config)
        logger.info(f"æ™ºèƒ½å»é‡é…ç½®å·²æ›´æ–°: {self.config}")

        # æŒä¹…åŒ–é…ç½®åˆ°æ•°æ®åº“
        try:
            self._save_config_to_db()
        except Exception as e:
            logger.warning(f"ä¿å­˜å»é‡é…ç½®åˆ°æ•°æ®åº“å¤±è´¥: {e}")

    def _save_config_to_db(self):
        """ä¿å­˜é…ç½®åˆ°æ•°æ®åº“"""
        try:
            import json

            from models.models import SessionManager, SystemConfiguration

            with SessionManager() as session:
                # æŸ¥æ‰¾æˆ–åˆ›å»ºé…ç½®è®°å½•
                config_record = (
                    session.query(SystemConfiguration)
                    .filter_by(key="smart_dedup_config")
                    .first()
                )

                if not config_record:
                    config_record = SystemConfiguration(
                        key="smart_dedup_config", value=json.dumps(self.config)
                    )
                    session.add(config_record)
                else:
                    config_record.value = json.dumps(self.config)

                session.commit()
                logger.debug("æ™ºèƒ½å»é‡é…ç½®å·²ä¿å­˜åˆ°æ•°æ®åº“")

        except Exception as e:
            logger.error(f"ä¿å­˜å»é‡é…ç½®å¤±è´¥: {e}")

    def _load_config_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½é…ç½®"""
        try:
            import json

            from models.models import SessionManager, SystemConfiguration

            with SessionManager() as session:
                config_record = (
                    session.query(SystemConfiguration)
                    .filter_by(key="smart_dedup_config")
                    .first()
                )

                if config_record and config_record.value:
                    db_config = json.loads(config_record.value)
                    # åˆå¹¶æ•°æ®åº“é…ç½®å’Œé»˜è®¤é…ç½®
                    self.config.update(db_config)
                    logger.info(f"ä»æ•°æ®åº“åŠ è½½æ™ºèƒ½å»é‡é…ç½®: {self.config}")

        except Exception as e:
            logger.warning(f"ä»æ•°æ®åº“åŠ è½½å»é‡é…ç½®å¤±è´¥: {e}")

    def reset_to_defaults(self):
        """é‡ç½®ä¸ºé»˜è®¤é…ç½®"""
        self.config = {
            "enable_time_window": True,
            "time_window_hours": 24,
            "similarity_threshold": 0.85,
            "enable_content_hash": True,
            "enable_smart_similarity": True,
            "cache_cleanup_interval": 3600,
            "max_text_cache_size": 300,
            "min_text_length": 10,
            "strip_numbers": True,
            "enable_text_fingerprint": True,
            "fingerprint_ngram": 3,
            "fingerprint_hamming_threshold": 3,
            "max_text_fp_cache_size": 500,
            "max_similarity_checks": 50,
            "enable_text_similarity_for_video": False,
            "enable_video_file_id_check": True,
            "enable_video_partial_hash_check": True,
            "video_partial_hash_bytes": 262144,
            "disable_similarity_for_grouped": True,
        }
        self._save_config_to_db()
        logger.info("æ™ºèƒ½å»é‡é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")

    def _compute_text_fingerprint(
        self, cleaned_text: str, ngram: int = 3
    ) -> Optional[int]:
        """åŸºäºè¯çº§ n-gram çš„ç®€æ˜“ SimHashï¼ˆ64ä½ï¼‰ã€‚"""
        try:
            tokens = cleaned_text.split()
            if not tokens:
                return None
            shingles = [
                " ".join(tokens[i : i + ngram])
                for i in range(max(1, len(tokens) - ngram + 1))
            ]
            if not shingles:
                shingles = tokens
            vector = [0] * 64

            # âœ… ä¼˜åŒ–ï¼šä½¿ç”¨ xxHash æ›¿ä»£ MD5
            if _HAS_XXHASH:
                for s in shingles:
                    # xxh64 ç›´æ¥è¿”å› intï¼Œé€Ÿåº¦æå¿«
                    h = xxhash.xxh64(s.encode("utf-8")).intdigest()
                    for i in range(64):
                        if (h >> i) & 1:
                            vector[i] += 1
                        else:
                            vector[i] -= 1
            else:
                # åŸæœ‰é€»è¾‘...
                for s in shingles:
                    h = int(hashlib.md5(s.encode("utf-8")).hexdigest(), 16)
                    for i in range(64):
                        if (h >> i) & 1:
                            vector[i] += 1
                        else:
                            vector[i] -= 1

            fp = 0
            for i, v in enumerate(vector):
                if v > 0:
                    fp |= 1 << i
            return fp
        except Exception:
            return None

    def _hamming_distance64(self, a: int, b: int) -> int:
        if _HAS_NUMBA:
            return _fast_hamming_64(a, b)
        xor_val = (a ^ b) & 0xFFFFFFFFFFFFFFFF

        # Python 3.10+ åŸç”Ÿæ”¯æŒ (æé€Ÿ)
        if hasattr(int, "bit_count"):
            return xor_val.bit_count()

        # å›é€€ç®—æ³• (Kernighan's Algorithm / Brian Kernighan's way)
        # å¯¹äºå·®å¼‚è¾ƒå°çš„æŒ‡çº¹ï¼ˆå»é‡åœºæ™¯ï¼‰ï¼Œæ­¤ç®—æ³•åªéœ€å¾ªç¯ "å·®å¼‚ä½æ•°" æ¬¡ï¼Œè¿œå°‘äº 64 æ¬¡
        count = 0
        while xor_val:
            xor_val &= xor_val - 1
            count += 1
        return count

    async def _strict_verify_video_features(
        self,
        target_chat_id: int,
        message_obj,
        file_id: Optional[str],
        vhash: Optional[str],
        config: Dict,
    ) -> bool:
        """åœ¨å“ˆå¸Œå‘½ä¸­åè¿›è¡Œä¸¥æ ¼å¤æ ¸ï¼šæ¯”è¾ƒ duration/åˆ†è¾¨ç‡/å¤§å°èŒƒå›´ ç­‰ç‰¹å¾ã€‚

        å®¹å¿åº¦é€šè¿‡é…ç½®æ§åˆ¶ï¼š
        - video_duration_tolerance_sec
        - video_resolution_tolerance_px
        - video_size_bucket_tolerance
        """
        try:
            # è¯»å–å½“å‰æ¶ˆæ¯çš„ç‰¹å¾
            duration = int(
                getattr(getattr(message_obj, "video", None), "duration", 0) or 0
            )
            width = int(getattr(getattr(message_obj, "video", None), "w", 0) or 0)
            height = int(getattr(getattr(message_obj, "video", None), "h", 0) or 0)
            size_val = None
            if hasattr(message_obj, "document") and getattr(message_obj, "document"):
                try:
                    size_val = int(getattr(message_obj.document, "size", 0) or 0)
                except Exception:
                    size_val = None
            size_bucket = self._get_size_range(size_val or 0)
            # æŸ¥æ‰¾å†å²ä¸€æ¡åŒ¹é…è®°å½•ç”¨äºå¯¹æ¯”
            from repositories.db_operations import DBOperations
            from models.models import AsyncSessionManager

            # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            async with AsyncSessionManager() as session:
                db_ops = await DBOperations.create()
                rec = await db_ops.find_media_record_by_fileid_or_hash(
                    session, str(target_chat_id), file_id=file_id, content_hash=vhash
                )
                if not rec:
                    return True  # æ²¡æœ‰å¯ä»¥å¯¹æ¯”çš„è®°å½•æ—¶ï¼Œä¸é˜»æ–­
                tol_d = int(config.get("video_duration_tolerance_sec", 2))
                tol_r = int(config.get("video_resolution_tolerance_px", 8))
                tol_s = int(config.get("video_size_bucket_tolerance", 1))
                # å†å²ç‰¹å¾
                h_d = int(getattr(rec, "duration", 0) or 0)
                h_w = int(getattr(rec, "width", 0) or 0)
                h_h = int(getattr(rec, "height", 0) or 0)
                h_bucket = self._get_size_range(int(getattr(rec, "file_size", 0) or 0))
                # æ¯”è¾ƒ
                if abs(duration - h_d) > tol_d:
                    return False
                if (width and h_w) and abs(width - h_w) > tol_r:
                    return False
                if (height and h_h) and abs(height - h_h) > tol_r:
                    return False
                # bucket å®¹å¿ 1 çº§ï¼ˆå¯é…ç½®ï¼‰
                if size_bucket and h_bucket:
                    if (
                        abs(
                            self._size_bucket_index(size_bucket)
                            - self._size_bucket_index(h_bucket)
                        )
                        > tol_s
                    ):
                        return False
                return True
        except Exception:
            return True

    async def _check_video_hash_pcache(self, file_id: str) -> Optional[str]:
        """ä»æŒä¹…åŒ–ç¼“å­˜ä¸­è¯»å–è§†é¢‘ partial-hashã€‚"""
        try:
            from repositories.persistent_cache import get_persistent_cache, loads_json

            pc = get_persistent_cache()
            key = f"video:hash:{file_id}"
            logger.debug(f"æ£€æŸ¥è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜ï¼Œkey: {key}")
            raw = pc.get(key)
            if raw:
                logger.debug(f"è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜å‘½ä¸­ï¼Œkey: {key}")
                data = loads_json(raw)
                if isinstance(data, dict):
                    hash_value = data.get("hash")
                    logger.debug(f"ä»ç¼“å­˜ä¸­è·å–åˆ°è§†é¢‘å“ˆå¸Œ: {hash_value}")
                    return hash_value
            logger.debug(f"è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜æœªå‘½ä¸­ï¼Œkey: {key}")
        except Exception as e:
            logger.debug(f"æ£€æŸ¥è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")
            return None
        return None

    async def _write_video_hash_pcache(
        self, file_id: str, vhash: str, ttl_seconds: int
    ) -> None:
        """å†™å…¥è§†é¢‘ partial-hash åˆ°æŒä¹…åŒ–ç¼“å­˜ã€‚"""
        try:
            from repositories.persistent_cache import dumps_json, get_persistent_cache

            pc = get_persistent_cache()
            key = f"video:hash:{file_id}"
            ttl = max(60, int(ttl_seconds))
            logger.debug(
                f"å†™å…¥è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜ï¼Œkey: {key}, hash: {vhash}, TTL: {ttl}ç§’"
            )
            pc.set(key, dumps_json({"hash": vhash, "ts": int(time.time())}), ttl)
            logger.debug(f"è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜å†™å…¥å®Œæˆï¼Œkey: {key}")
        except Exception as e:
            logger.debug(f"å†™å…¥è§†é¢‘å“ˆå¸ŒæŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")
            pass


    async def remove_message(self, message_obj, target_chat_id: int):
        """Remove message from cache (Rollback)"""
        try:
            cache_key = str(target_chat_id)
            signature = self._generate_signature(message_obj)
            content_hash = self._generate_content_hash(message_obj)
            
            # Remove from Memory Cache
            if signature and cache_key in self.time_window_cache:
                self.time_window_cache[cache_key].pop(signature, None)
            
            if content_hash and cache_key in self.content_hash_cache:
                self.content_hash_cache[cache_key].pop(content_hash, None)
                
            # Remove from Persistent Cache
            if self.config.get("enable_persistent_cache", True):
                try:
                    from repositories.persistent_cache import get_persistent_cache
                    pc = get_persistent_cache()
                    if signature:
                        pc.delete(f"dedup:sig:{target_chat_id}:{signature}")
                    if content_hash:
                        pc.delete(f"dedup:hash:{target_chat_id}:{content_hash}")
                except Exception:
                    pass
            
            # Remove from Write Buffer (if not flushed yet)
            async with self._buffer_lock:
                 self._write_buffer = [
                     item for item in self._write_buffer 
                     if not (item.get('signature') == signature and item.get('content_hash') == content_hash)
                 ]
                 
            logger.debug(f"Rolled back dedup status for chat {target_chat_id}")
        except Exception as e:
            logger.error(f"Failed to rollback dedup: {e}")

# å…¨å±€æ™ºèƒ½å»é‡å™¨å®ä¾‹
smart_deduplicator = SmartDeduplicator()
