"""
æ•°æ®åº“ä¼˜åŒ–å¥—ä»¶ - é›†æˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½çš„ä¸»æ¨¡å—
æä¾›ä¸€é”®ä¼˜åŒ–ã€æ€§èƒ½ç›‘æ§ã€é…ç½®ç®¡ç†ç­‰å®Œæ•´è§£å†³æ–¹æ¡ˆ
"""

from datetime import datetime

import asyncio
import json
import logging
from typing import Any, Dict, List, Optional

from repositories.batch_repo import (
    get_batch_processor_stats,
    get_connection_pool_status,
    start_batch_processing,
    stop_batch_processing,
)
from repositories.db_index_optimizer import (
    db_optimizer,
    get_database_performance_metrics,
    optimize_database_performance,
)
from repositories.db_monitor import (
    get_performance_dashboard,
    reset_monitoring_data,
    start_database_monitoring,
    stop_database_monitoring,
)
from repositories.db_sharding import (
    get_sharding_statistics,
    optimize_query_with_sharding,
    setup_database_sharding,
)
from core.logging import get_logger
from repositories.query_optimizer import (
    CacheInvalidationManager,
    OptimizedQueries,
    get_query_performance_stats,
    start_query_optimization,
)

logger = get_logger(__name__)


class DatabaseOptimizationSuite:
    """æ•°æ®åº“ä¼˜åŒ–å¥—ä»¶"""

    def __init__(self):
        self.is_initialized = False
        self.optimization_config = {
            "enable_query_cache": True,
            "enable_monitoring": True,
            "enable_sharding": True,
            "enable_batch_processing": True,
            "enable_index_optimization": True,
            "auto_optimize": True,
        }

    async def initialize(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–ä¼˜åŒ–å¥—ä»¶"""
        if self.is_initialized:
            logger.warning("Database optimization suite already initialized")
            return

        if config:
            self.optimization_config.update(config)

        logger.info("Initializing Database Optimization Suite...")

        try:
            # 1. å¯åŠ¨æŸ¥è¯¢ä¼˜åŒ–
            if self.optimization_config.get("enable_query_cache", True):
                await start_query_optimization()
                logger.info("âœ“ Query optimization enabled")

            # 2. å¯åŠ¨æ€§èƒ½ç›‘æ§
            if self.optimization_config.get("enable_monitoring", True):
                await start_database_monitoring()
                logger.info("âœ“ Performance monitoring enabled")

            # 3. è®¾ç½®åˆ†ç‰‡å’Œåˆ†åŒº
            if self.optimization_config.get("enable_sharding", True):
                sharding_result = setup_database_sharding(enable_partitioning=True)
                logger.info(
                    f"âœ“ Sharding setup completed: {len(sharding_result.get('partitions_created', []))} partitions"
                )

            # 4. å¯åŠ¨æ‰¹é‡å¤„ç†
            if self.optimization_config.get("enable_batch_processing", True):
                await start_batch_processing()
                logger.info("âœ“ Batch processing enabled")

            # 5. ç´¢å¼•ä¼˜åŒ–
            if self.optimization_config.get("enable_index_optimization", True):
                if self.optimization_config.get("auto_optimize", True):
                    index_result = optimize_database_performance(apply_changes=True)
                    created_indexes = len(index_result.get("changes_applied", []))
                    logger.info(
                        f"âœ“ Database optimization completed: {created_indexes} changes applied"
                    )
                else:
                    logger.info("âœ“ Index optimization configured (manual mode)")

            self.is_initialized = True
            logger.info("ğŸš€ Database Optimization Suite initialized successfully!")

        except Exception as e:
            logger.error(f"Failed to initialize Database Optimization Suite: {e}")
            raise

    async def shutdown(self):
        """å…³é—­ä¼˜åŒ–å¥—ä»¶"""
        if not self.is_initialized:
            return

        logger.info("Shutting down Database Optimization Suite...")

        try:
            # åœæ­¢å„ç§æœåŠ¡
            stop_database_monitoring()
            await stop_batch_processing()

            self.is_initialized = False
            logger.info("Database Optimization Suite shutdown completed")

        except Exception as e:
            logger.error(f"Error during shutdown: {e}")

    def get_comprehensive_status(self) -> Dict[str, Any]:
        """è·å–å…¨é¢çš„çŠ¶æ€ä¿¡æ¯"""
        if not self.is_initialized:
            return {"status": "not_initialized"}

        try:
            status = {
                "timestamp": datetime.utcnow().isoformat(),
                "suite_status": "active",
                "config": self.optimization_config,
                "components": {},
            }

            # æŸ¥è¯¢ä¼˜åŒ–çŠ¶æ€
            if self.optimization_config.get("enable_query_cache"):
                try:
                    query_stats = get_query_performance_stats()
                    status["components"]["query_optimization"] = {
                        "status": "active",
                        "stats": query_stats,
                    }
                except Exception as e:
                    status["components"]["query_optimization"] = {
                        "status": "error",
                        "error": str(e),
                    }

            # ç›‘æ§çŠ¶æ€
            if self.optimization_config.get("enable_monitoring"):
                try:
                    monitoring_data = get_performance_dashboard()
                    status["components"]["monitoring"] = {
                        "status": "active",
                        "dashboard": monitoring_data,
                    }
                except Exception as e:
                    status["components"]["monitoring"] = {
                        "status": "error",
                        "error": str(e),
                    }

            # åˆ†ç‰‡çŠ¶æ€
            if self.optimization_config.get("enable_sharding"):
                try:
                    sharding_stats = get_sharding_statistics()
                    status["components"]["sharding"] = {
                        "status": "active",
                        "statistics": sharding_stats,
                    }
                except Exception as e:
                    status["components"]["sharding"] = {
                        "status": "error",
                        "error": str(e),
                    }

            # æ‰¹é‡å¤„ç†çŠ¶æ€
            if self.optimization_config.get("enable_batch_processing"):
                try:
                    batch_stats = get_batch_processor_stats()
                    pool_status = get_connection_pool_status()
                    status["components"]["batch_processing"] = {
                        "status": "active",
                        "batch_stats": batch_stats,
                        "pool_status": pool_status,
                    }
                except Exception as e:
                    status["components"]["batch_processing"] = {
                        "status": "error",
                        "error": str(e),
                    }

            # æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡
            try:
                db_metrics = get_database_performance_metrics()
                status["database_metrics"] = db_metrics
            except Exception as e:
                status["database_metrics"] = {"error": str(e)}

            return status

        except Exception as e:
            logger.error(f"Failed to get comprehensive status: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat(),
            }

    def get_optimization_recommendations(self) -> List[Dict[str, Any]]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        try:
            # åŸºäºå½“å‰çŠ¶æ€åˆ†æ
            status = self.get_comprehensive_status()

            # æŸ¥è¯¢ä¼˜åŒ–å»ºè®®
            if "query_optimization" in status.get("components", {}):
                query_component = status["components"]["query_optimization"]
                if query_component.get("status") == "active":
                    hot_queries = query_component.get("stats", {}).get(
                        "hot_queries", []
                    )
                    if len(hot_queries) > 5:
                        recommendations.append(
                            {
                                "category": "query_optimization",
                                "priority": "high",
                                "title": "çƒ­ç‚¹æŸ¥è¯¢ä¼˜åŒ–",
                                "description": f"æ£€æµ‹åˆ° {len(hot_queries)} ä¸ªçƒ­ç‚¹æŸ¥è¯¢ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–",
                                "action": "æ£€æŸ¥æŸ¥è¯¢ç¼“å­˜é…ç½®ï¼Œè€ƒè™‘æ·»åŠ ä¸“é—¨çš„ç´¢å¼•",
                            }
                        )

            # ç›‘æ§å»ºè®®
            if "monitoring" in status.get("components", {}):
                monitoring_component = status["components"]["monitoring"]
                if monitoring_component.get("status") == "active":
                    alerts = monitoring_component.get("dashboard", {}).get("alerts", [])
                    if alerts:
                        for alert in alerts:
                            recommendations.append(
                                {
                                    "category": "performance_alert",
                                    "priority": alert.get("severity", "medium"),
                                    "title": f'æ€§èƒ½å‘Šè­¦: {alert.get("type", "unknown")}',
                                    "description": alert.get("message", ""),
                                    "action": "æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µï¼Œè€ƒè™‘æ‰©å®¹æˆ–ä¼˜åŒ–",
                                }
                            )

            # æ•°æ®åº“å¤§å°å»ºè®®
            db_metrics = status.get("database_metrics", {})
            if "db_size" in db_metrics:
                db_size_mb = db_metrics["db_size"] / (1024 * 1024)
                if db_size_mb > 500:  # 500MB
                    recommendations.append(
                        {
                            "category": "storage_optimization",
                            "priority": "medium",
                            "title": "æ•°æ®åº“å¤§å°ä¼˜åŒ–",
                            "description": f"æ•°æ®åº“å¤§å°å·²è¾¾åˆ° {db_size_mb:.1f}MB",
                            "action": "è€ƒè™‘å¯ç”¨æ•°æ®å½’æ¡£ï¼Œæ¸…ç†è¿‡æœŸæ•°æ®",
                        }
                    )

            # è¿æ¥æ± å»ºè®®
            if "batch_processing" in status.get("components", {}):
                batch_component = status["components"]["batch_processing"]
                if batch_component.get("status") == "active":
                    pool_status = batch_component.get("pool_status", {})
                    checked_out = pool_status.get("checked_out", 0)
                    pool_size = pool_status.get("pool_size", 0)

                    if isinstance(checked_out, int) and isinstance(pool_size, int):
                        if checked_out > pool_size * 0.8:  # 80%ä»¥ä¸Šä½¿ç”¨ç‡
                            recommendations.append(
                                {
                                    "category": "connection_pool",
                                    "priority": "medium",
                                    "title": "è¿æ¥æ± ä¼˜åŒ–",
                                    "description": f"è¿æ¥æ± ä½¿ç”¨ç‡è¾ƒé«˜ ({checked_out}/{pool_size})",
                                    "action": "è€ƒè™‘å¢åŠ è¿æ¥æ± å¤§å°æˆ–ä¼˜åŒ–è¿æ¥ä½¿ç”¨",
                                }
                            )

            # é€šç”¨å»ºè®®
            if not recommendations:
                recommendations.append(
                    {
                        "category": "general",
                        "priority": "low",
                        "title": "ç³»ç»Ÿè¿è¡Œè‰¯å¥½",
                        "description": "å½“å‰æ•°æ®åº“æ€§èƒ½çŠ¶æ€è‰¯å¥½ï¼Œå»ºè®®å®šæœŸæ£€æŸ¥",
                        "action": "ç»§ç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼Œå®šæœŸç»´æŠ¤",
                    }
                )

        except Exception as e:
            logger.error(f"Failed to generate recommendations: {e}")
            recommendations.append(
                {
                    "category": "error",
                    "priority": "high",
                    "title": "æ— æ³•ç”Ÿæˆå»ºè®®",
                    "description": f"ç”Ÿæˆä¼˜åŒ–å»ºè®®æ—¶å‡ºé”™: {str(e)}",
                    "action": "æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å’Œæ—¥å¿—",
                }
            )

        return recommendations

    async def run_optimization_check(self) -> Dict[str, Any]:
        """è¿è¡Œä¼˜åŒ–æ£€æŸ¥"""
        logger.info("Running database optimization check...")

        check_result = {
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "checks": {},
            "recommendations": [],
            "actions_taken": [],
        }

        try:
            # 1. æ€§èƒ½æŒ‡æ ‡æ£€æŸ¥
            db_metrics = get_database_performance_metrics()
            check_result["checks"]["performance_metrics"] = {
                "status": "passed" if db_metrics else "failed",
                "metrics": db_metrics,
            }

            # 2. æŸ¥è¯¢æ€§èƒ½æ£€æŸ¥
            query_stats = get_query_performance_stats()
            slow_queries = query_stats.get("slow_queries", [])
            check_result["checks"]["query_performance"] = {
                "status": "warning" if len(slow_queries) > 5 else "passed",
                "slow_query_count": len(slow_queries),
                "details": slow_queries[:3],  # æ˜¾ç¤ºå‰3ä¸ª
            }

            # 3. è¿æ¥æ± æ£€æŸ¥
            pool_status = get_connection_pool_status()
            check_result["checks"]["connection_pool"] = {
                "status": "passed",
                "pool_status": pool_status,
            }

            # 4. æ‰¹é‡å¤„ç†æ£€æŸ¥
            batch_stats = get_batch_processor_stats()
            check_result["checks"]["batch_processing"] = {
                "status": "passed" if batch_stats.get("is_running") else "warning",
                "stats": batch_stats,
            }

            # ç”Ÿæˆå»ºè®®
            check_result["recommendations"] = self.get_optimization_recommendations()

            # è‡ªåŠ¨æ‰§è¡ŒæŸäº›ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.optimization_config.get("auto_optimize", True):
                actions = await self._auto_optimize()
                check_result["actions_taken"] = actions

            logger.info("Database optimization check completed")

        except Exception as e:
            logger.error(f"Optimization check failed: {e}")
            check_result["status"] = "failed"
            check_result["error"] = str(e)

        return check_result

    async def _auto_optimize(self) -> List[str]:
        """è‡ªåŠ¨ä¼˜åŒ–æ“ä½œ"""
        actions = []

        try:
            # æ¸…ç†è¿‡æœŸç›‘æ§æ•°æ®
            reset_monitoring_data()
            actions.append("æ¸…ç†è¿‡æœŸç›‘æ§æ•°æ®")

            # åˆ·æ–°æŸ¥è¯¢ç¼“å­˜ç»Ÿè®¡
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šè‡ªåŠ¨ä¼˜åŒ–é€»è¾‘

        except Exception as e:
            logger.error(f"Auto optimization failed: {e}")
            actions.append(f"è‡ªåŠ¨ä¼˜åŒ–å¤±è´¥: {str(e)}")

        return actions

    def save_optimization_report(
        self, report: Dict[str, Any], filename: Optional[str] = None
    ) -> str:
        """ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filename = f"db_optimization_report_{timestamp}.json"

        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"Optimization report saved to {filename}")
            return filename

        except Exception as e:
            logger.error(f"Failed to save report: {e}")
            raise


# å…¨å±€å®ä¾‹
db_optimization_suite = DatabaseOptimizationSuite()


# ä¾¿æ·å‡½æ•°
async def initialize_database_optimization(config: Optional[Dict[str, Any]] = None):
    """åˆå§‹åŒ–æ•°æ®åº“ä¼˜åŒ–"""
    await db_optimization_suite.initialize(config)


async def shutdown_database_optimization():
    """å…³é—­æ•°æ®åº“ä¼˜åŒ–"""
    await db_optimization_suite.shutdown()


def get_database_optimization_status() -> Dict[str, Any]:
    """è·å–æ•°æ®åº“ä¼˜åŒ–çŠ¶æ€"""
    return db_optimization_suite.get_comprehensive_status()


async def run_database_optimization_check() -> Dict[str, Any]:
    """è¿è¡Œæ•°æ®åº“ä¼˜åŒ–æ£€æŸ¥"""
    return await db_optimization_suite.run_optimization_check()


def get_database_optimization_recommendations() -> List[Dict[str, Any]]:
    """è·å–æ•°æ®åº“ä¼˜åŒ–å»ºè®®"""
    return db_optimization_suite.get_optimization_recommendations()


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    try:
        # åˆå§‹åŒ–ä¼˜åŒ–å¥—ä»¶
        await initialize_database_optimization(
            {
                "enable_query_cache": True,
                "enable_monitoring": True,
                "enable_sharding": True,
                "enable_batch_processing": True,
                "enable_index_optimization": True,
                "auto_optimize": True,
            }
        )

        # è¿è¡Œä¼˜åŒ–æ£€æŸ¥
        check_result = await run_database_optimization_check()
        print(
            "ä¼˜åŒ–æ£€æŸ¥ç»“æœ:",
            json.dumps(check_result, indent=2, ensure_ascii=False, default=str),
        )

        # è·å–çŠ¶æ€
        status = get_database_optimization_status()
        print(
            "å½“å‰çŠ¶æ€:", json.dumps(status, indent=2, ensure_ascii=False, default=str)
        )

        # è·å–å»ºè®®
        recommendations = get_database_optimization_recommendations()
        print(
            "ä¼˜åŒ–å»ºè®®:",
            json.dumps(recommendations, indent=2, ensure_ascii=False, default=str),
        )

    except KeyboardInterrupt:
        print("æ­£åœ¨å…³é—­...")
    finally:
        await shutdown_database_optimization()


if __name__ == "__main__":
    asyncio.run(main())
