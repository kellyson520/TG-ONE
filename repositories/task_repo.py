from sqlalchemy import select, update, func
from models.models import TaskQueue, ForwardRule, Chat
from datetime import datetime, timedelta
import logging
from core.states import validate_transition
from core.config import settings
from core.helpers.db_utils import async_db_retry

logger = logging.getLogger(__name__)

class TaskRepository:
    def __init__(self, db):
        self.db = db

    async def archive_old_tasks(self, hot_days: int = 7, batch_size: int = 10000) -> dict:
        """å½’æ¡£æ—§ä»»åŠ¡è®°å½•ã€‚"""
        from core.archive.engine import UniversalArchiver
        from models.models import TaskQueue
        
        archiver = UniversalArchiver()
        result = await archiver.archive_table(
            model_class=TaskQueue,
            hot_days=hot_days,
            batch_size=batch_size
        )
        return result.to_dict()

    @async_db_retry(max_retries=5)
    async def push(self, task_type: str, payload: dict, priority: int = 0, scheduled_at: datetime = None):
        import json
        from sqlalchemy import insert
        
        # æå–ç”¨äºå»é‡çš„å…³é”®ä¿¡æ¯
        chat_id = payload.get('chat_id')
        message_id = payload.get('message_id')
        grouped_id = payload.get('grouped_id')
        
        unique_key = None
        if chat_id and message_id:
            unique_key = f"{task_type}:{chat_id}:{message_id}"

        from core.db_factory import AsyncSessionManager
        async with AsyncSessionManager() as session:
            # [Scheme 7 Optimization] ä½¿ç”¨åŸå­åŒ– INSERT OR IGNORE æ›¿ä»£å…ˆæŸ¥åå¢
            # å½»åº•è§£å†³å¤šå¹¶å‘åœºæ™¯ä¸‹çš„é‡å¤æ¨é€é—®é¢˜
            stmt = insert(TaskQueue).values(
                task_type=task_type,
                task_data=json.dumps(payload),
                unique_key=unique_key,
                grouped_id=str(grouped_id) if grouped_id else None,
                priority=priority,
                status='pending',
                scheduled_at=scheduled_at,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            ).prefix_with('OR IGNORE')
            
            result = await session.execute(stmt)
            await session.commit()
            
            if result.rowcount > 0:
                logger.info(f"âœ… ä»»åŠ¡å…¥åˆ—æˆåŠŸ (Key: {unique_key})")
            else:
                if unique_key:
                    logger.warning(f"âš ï¸ ä»»åŠ¡å·²å­˜åœ¨ï¼Œè·³è¿‡å…¥åˆ—: {unique_key}")
                else:
                    logger.warning("âš ï¸ ä»»åŠ¡å…¥åˆ—è¢«å¿½ç•¥ (rowcount=0)")

    @async_db_retry(max_retries=5)
    async def push_batch(self, tasks_data: list):
        """
        æ‰¹é‡å†™å…¥ä»»åŠ¡ (Batch Insert)
        Args:
            tasks_data: List[(task_type, payload, priority)]
        """
        import json
        from sqlalchemy import insert
        
        if not tasks_data:
            return
            
        values_list = []
        now = datetime.utcnow()
        
        for task_type, payload, priority in tasks_data:
            chat_id = payload.get('chat_id')
            message_id = payload.get('message_id')
            grouped_id = payload.get('grouped_id')
            
            unique_key = None
            if chat_id and message_id:
                unique_key = f"{task_type}:{chat_id}:{message_id}"
            
            values_list.append({
                "task_type": task_type,
                "task_data": json.dumps(payload),
                "unique_key": unique_key,
                "grouped_id": str(grouped_id) if grouped_id else None,
                "priority": priority,
                "attempts": 0,
                "status": "pending",
                "created_at": now,
                "updated_at": now
            })

        if not values_list:
            return

        from core.db_factory import AsyncSessionManager
        async with AsyncSessionManager() as session:
             # ä½¿ç”¨ Core Insert + OR IGNORE (SQLite) å®ç°é«˜æ€§èƒ½æ‰¹é‡å»é‡å†™å…¥
             stmt = insert(TaskQueue).values(values_list).prefix_with('OR IGNORE')
             await session.execute(stmt)
             # AsyncSessionManager handles commit automatically
             logger.info(f"âœ… æ‰¹é‡èšåˆå†™å…¥: {len(values_list)} æ¡ä»»åŠ¡")

    @async_db_retry(max_retries=5)
    async def fetch_next(self, limit: int = 1):
        """
        è·å–ä¸‹ä¸€æ‰¹å¾…å¤„ç†ä»»åŠ¡ã€‚
        ä¼˜åŒ–ç‚¹ï¼šåˆ†ä¸¤æ­¥æ‰§è¡Œï¼Œå…ˆåªè¯»æŸ¥è¯¢å€™é€‰ IDï¼Œå†åŸå­æ›´æ–°ã€‚è¿™æ ·å¯ä»¥ç¼©çŸ­é”å®šæ—¶é—´ã€‚
        """
        from core.db_factory import AsyncSessionManager
        now = datetime.utcnow()
        # é¢„ç•™ 50ms ç¼“å†²ï¼Œç¡®ä¿ä¸ä¼šå› ä¸ºå¾®å°çš„æ—¶é’Ÿå·®æ¼æ‰ä»»åŠ¡
        buffer_now = now + timedelta(milliseconds=50)
        lease_until = now + timedelta(seconds=settings.TASK_DISPATCHER_MAX_SLEEP + 60)
        
        # 1. ç¬¬ä¸€æ­¥ï¼šè·å–å€™é€‰ä»»åŠ¡ ID (Read Phase - ä¸åŠ é”)
        async with AsyncSessionManager(readonly=True) as session:
            stmt_candidates = (
                select(TaskQueue.id, TaskQueue.grouped_id)
                .where(
                    (TaskQueue.status == 'pending') |
                    (
                        (TaskQueue.status == 'running') & 
                        (TaskQueue.locked_until != None) & 
                        (TaskQueue.locked_until <= now)
                    )
                )
                .where((TaskQueue.scheduled_at == None) | (TaskQueue.scheduled_at <= buffer_now))
                .where((TaskQueue.next_retry_at == None) | (TaskQueue.next_retry_at <= buffer_now))
                .order_by(TaskQueue.priority.desc(), TaskQueue.created_at.asc())
                .limit(limit)
            )
            
            result = await session.execute(stmt_candidates)
            candidates = result.all() # List of Row(id, grouped_id)

        if not candidates:
            return []

        # æ”¶é›†ä¸»ä»»åŠ¡ ID å’Œåˆ†ç»„ ID
        candidate_ids = [c.id for c in candidates]
        group_ids = [c.grouped_id for c in candidates if c.grouped_id]
        
        final_task_ids = set(candidate_ids)
        
        # 2. ç¬¬äºŒæ­¥ï¼šè‹¥å­˜åœ¨ grouped_idï¼Œæ‰©å±•å¹¶ç¡®å®šæœ€ç»ˆä»»åŠ¡é›†åˆ (Read Phase - ä¸åŠ é”)
        if group_ids:
            async with AsyncSessionManager(readonly=True) as session:
                stmt_groups = (
                    select(TaskQueue.id)
                    .where(TaskQueue.grouped_id.in_(group_ids))
                    .where(TaskQueue.status == 'pending')
                )
                group_result = await session.execute(stmt_groups)
                for row in group_result:
                    final_task_ids.add(row[0])

        final_task_ids = list(final_task_ids)
        
        # 3. ç¬¬ä¸‰æ­¥ï¼šåŸå­æ›´æ–°å€™é€‰ä»»åŠ¡çŠ¶æ€ (Write Phase - BEGIN IMMEDIATE)
        async with AsyncSessionManager() as session:
            stmt = (
                update(TaskQueue)
                .where(TaskQueue.id.in_(final_task_ids))
                .where(
                    (TaskQueue.status == 'pending') |
                    (
                        (TaskQueue.status == 'running') & 
                        (TaskQueue.locked_until <= now)
                    )
                )
                .values(
                    status='running',
                    started_at=now,
                    locked_until=lease_until, # è®¾ç½®ç§Ÿçº¦
                    updated_at=now
                )
                .execution_options(synchronize_session=False)
                .returning(TaskQueue)
            )
            
            result = await session.execute(stmt)
            tasks = result.scalars().all()
            
            # [Fix] å¦‚æœåœ¨æ›´æ–°é‚£ä¸€åˆ»ï¼ŒæŸäº›ä»»åŠ¡çŠ¶æ€å˜äº†ï¼Œå¯¼è‡´æ›´æ–°åˆ°çš„ä»»åŠ¡å°‘äºé¢„æœŸï¼Œæ˜¯æ­£å¸¸çš„
            # è¿™é‡Œçš„åŸå­æ€§ç”± .where(status.in_...) ä¿è¯
            if tasks:
                tasks.sort(key=lambda x: x.created_at)
                logger.debug(f"[TaskRepo] æˆåŠŸé”å®š {len(tasks)} ä¸ªä»»åŠ¡ (è¯·æ±‚: {len(final_task_ids)})")
            
            return tasks

    @async_db_retry(max_retries=5)
    async def complete(self, task_id: int):
        """æ ‡è®°ä»»åŠ¡å®Œæˆ (çº¯ UPDATEï¼Œæœ€å°åŒ–é”æŒæœ‰æ—¶é—´)"""
        now = datetime.utcnow()
        async with self.db.get_session() as session:
            # [Optimization] å°†çŠ¶æ€éªŒè¯å†…è”åˆ° WHERE æ¡ä»¶ä¸­ï¼Œçœå» SELECT é˜¶æ®µ
            # åˆæ³•çš„å‰ç½®çŠ¶æ€: running -> completed
            result = await session.execute(
                update(TaskQueue)
                .where(TaskQueue.id == task_id)
                .where(TaskQueue.status.in_(['running', 'pending']))
                .values(
                    status='completed',
                    completed_at=now,
                    updated_at=now
                )
            )
            await session.commit()
            if result.rowcount > 0:
                logger.info(f"ä»»åŠ¡å®Œæˆ: {task_id}")
            else:
                logger.warning(f"ä»»åŠ¡å®Œæˆå¤±è´¥(çŠ¶æ€ä¸åŒ¹é…æˆ–ä¸å­˜åœ¨): {task_id}")

    @async_db_retry(max_retries=5)
    async def fail(self, task_id: int, error: str):
        """æ ‡è®°ä»»åŠ¡å¤±è´¥ (çº¯ UPDATEï¼Œæœ€å°åŒ–é”æŒæœ‰æ—¶é—´)"""
        now = datetime.utcnow()
        error_str = str(error)
        async with self.db.get_session() as session:
            # [Optimization] å°†çŠ¶æ€éªŒè¯å†…è”åˆ° WHERE æ¡ä»¶ä¸­ï¼Œçœå» SELECT é˜¶æ®µ
            # åˆæ³•çš„å‰ç½®çŠ¶æ€: running/pending -> failed
            result = await session.execute(
                update(TaskQueue)
                .where(TaskQueue.id == task_id)
                .where(TaskQueue.status.in_(['running', 'pending']))
                .values(
                    status='failed',
                    error_message=error_str,
                    updated_at=now
                )
            )
            await session.commit()
            
            if result.rowcount > 0:
                # å¯¹äº "Source message not found" è¿™ç±»é¢„æœŸå†…çš„ä¸šåŠ¡æƒ…å†µ,ä½¿ç”¨ DEBUG çº§åˆ«
                if "Source message not found" in error_str:
                    logger.debug(f"ä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {error_str}")
                else:
                    logger.error(f"ä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {error_str}")
            else:
                logger.warning(f"ä»»åŠ¡å¤±è´¥æ ‡è®°è·³è¿‡(çŠ¶æ€ä¸åŒ¹é…æˆ–ä¸å­˜åœ¨): {task_id}")
            
    @async_db_retry(max_retries=5)
    async def fail_or_retry(self, task_id: int, error: str, max_retries: int = settings.MAX_RETRIES):
        """æ ¸å¿ƒä¿®å¤ï¼šå¤±è´¥é‡è¯•æœºåˆ¶"""
        async with self.db.get_session() as session:
            stmt = select(TaskQueue).where(TaskQueue.id == task_id)
            result = await session.execute(stmt)
            task = result.scalar_one_or_none()
            
            if task:
                now = datetime.utcnow()
                task.error_message = str(error)
                if task.attempts < max_retries:
                    if validate_transition(task.status, 'pending'):
                        task.attempts += 1
                        task.status = 'pending' # é‡æ–°æ”¾å›é˜Ÿåˆ—
                        task.priority += 1      # ç¨å¾®æé«˜ä¼˜å…ˆçº§ä»¥ä¾¿é‡è¯•
                        # å®ç°æŒ‡æ•°é€€é¿ç®—æ³•ï¼š2^(attempts) ç§’
                        backoff_seconds = 2 ** task.attempts
                        task.next_retry_at = now + timedelta(seconds=backoff_seconds)
                        task.updated_at = now
                        logger.info(f"ä»»åŠ¡é‡è¯•: {task_id}, é‡è¯•æ¬¡æ•°: {task.attempts}, ä¸‹æ¬¡é‡è¯•æ—¶é—´: {task.next_retry_at}")
                else:
                    if validate_transition(task.status, 'failed'):
                        task.status = 'failed'  # å½»åº•å¤±è´¥
                        task.updated_at = now
                        logger.error(f"ä»»åŠ¡æœ€ç»ˆå¤±è´¥: {task_id}, é”™è¯¯: {error}")
                await session.commit()
            
    @async_db_retry(max_retries=5)
    async def rescue_stuck_tasks(self, timeout_minutes: int = 10):
        """åƒµå°¸ä»»åŠ¡æ•‘æ´ - å°†å¤„äº 'running' çŠ¶æ€è¶…è¿‡æŒ‡å®šæ—¶é—´çš„ä»»åŠ¡é‡ç½®ä¸º 'pending'"""
        from core.db_factory import AsyncSessionManager
        async with AsyncSessionManager() as session:
            cutoff_time = datetime.utcnow() - timedelta(minutes=timeout_minutes)
            now = datetime.utcnow()
            
            # æŸ¥æ‰¾å¹¶é‡ç½®åƒµå°¸ä»»åŠ¡
            # åªå¯¹çŠ¶æ€ä¸ºrunningä¸”æ›´æ–°æ—¶é—´è¶…è¿‡cutoff_timeçš„ä»»åŠ¡è¿›è¡Œæ“ä½œ
            stmt = update(TaskQueue).where(
                TaskQueue.status == 'running',
                TaskQueue.updated_at < cutoff_time
            ).values(
                status='pending',
                attempts=TaskQueue.attempts + 1, # å¢åŠ é‡è¯•è®¡æ•°
                error_message=TaskQueue.error_message + ' [System] Task rescued from zombie state',
                updated_at=now
            )
            
            result = await session.execute(stmt)
            await session.commit()
            
            if result.rowcount > 0:
                logger.info(f"å·²æ•‘æ´ {result.rowcount} ä¸ªåƒµå°¸ä»»åŠ¡")
            return result.rowcount
            
    @async_db_retry(max_retries=5)
    async def reschedule(self, task_id: int, next_run_time: datetime):
        """é‡æ–°è°ƒåº¦ä»»åŠ¡ (çº¯ UPDATEï¼Œæœ€å°åŒ–é”æŒæœ‰æ—¶é—´)"""
        now = datetime.utcnow()
        async with self.db.get_session() as session:
            # [Optimization] å°†çŠ¶æ€éªŒè¯å†…è”åˆ° WHERE æ¡ä»¶ä¸­
            # åˆæ³•çš„å‰ç½®çŠ¶æ€: running/pending/failed -> pending (reschedule)
            result = await session.execute(
                update(TaskQueue)
                .where(TaskQueue.id == task_id)
                .where(TaskQueue.status.in_(['running', 'pending', 'failed']))
                .values(
                    status='pending',
                    scheduled_at=next_run_time,
                    next_retry_at=next_run_time,
                    updated_at=now
                )
            )
            await session.commit()
            if result.rowcount > 0:
                logger.info(f"ä»»åŠ¡é‡æ–°è°ƒåº¦: {task_id}, ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {next_run_time}")
            else:
                logger.warning(f"ä»»åŠ¡è°ƒåº¦è·³è¿‡(çŠ¶æ€ä¸åŒ¹é…æˆ–ä¸å­˜åœ¨): {task_id}")

    @async_db_retry(max_retries=5)
    async def fetch_group_tasks(self, grouped_id: str, exclude_task_id: int):
        """
        è·å–åŒä¸€åª’ä½“ç»„çš„å…¶ä»–ç›¸å…³ä»»åŠ¡ï¼Œå¹¶åŸå­é”å®šå®ƒä»¬
        
        Args:
            grouped_id: åª’ä½“ç»„ID
            exclude_task_id: å½“å‰å·²è·å–çš„ä»»åŠ¡IDï¼ˆæ’é™¤å®ƒï¼‰
            
        Returns:
            List[TaskQueue]: ç›¸å…³ä»»åŠ¡åˆ—è¡¨
        """
        from core.db_factory import AsyncSessionManager
        async with AsyncSessionManager() as session:
            # [Optimization] å¢åŠ  30s æ—¶é—´å†—ä½™ï¼Œå½»åº•æ ¹é™¤æ—¶é’Ÿåå·®å¯¼è‡´çš„ä»»åŠ¡"ä¸å¯è§"é—®é¢˜
            now = datetime.utcnow() + timedelta(seconds=30)
            
            # 1. æŸ¥æ‰¾åŒç»„çš„å…¶ä»– pending ä»»åŠ¡
            stmt = (
                select(TaskQueue.id)
                .where(TaskQueue.grouped_id == grouped_id)
                .where(TaskQueue.id != exclude_task_id)
                .where(TaskQueue.status == 'pending')  # åªè·å– pending çš„
                .order_by(TaskQueue.id.asc())
            )
            result = await session.execute(stmt)
            task_ids = result.scalars().all()
            
            if not task_ids:
                return []
                
            # 2. åŸå­é”å®šè¿™äº›ä»»åŠ¡
            update_stmt = (
                update(TaskQueue)
                .where(TaskQueue.id.in_(task_ids))
                .values(
                    status='running',
                    started_at=now,
                    updated_at=now
                )
                .execution_options(synchronize_session=False)
                .returning(TaskQueue)
            )
            
            result = await session.execute(update_stmt)
            tasks = result.scalars().all()
            
            await session.commit()
            logger.info(f"ğŸ”’ åŸå­é”å®šå¹¶è·å–åª’ä½“ç»„ä»»åŠ¡: {len(tasks)} ä¸ª (Group: {grouped_id})")
            return tasks

    @async_db_retry(max_retries=5)
    async def get_queue_status(self):
        """è·å–é˜Ÿåˆ—çŠ¶æ€ç»Ÿè®¡ (åªè¯»)"""
        async with self.db.get_session(readonly=True) as session:
            # è·å–å„çŠ¶æ€ä»»åŠ¡æ•°é‡
            stmt = select(TaskQueue.status, func.count()).group_by(TaskQueue.status)
            res = await session.execute(stmt)
            counts = dict(res.all())
            logger.info(f"ğŸ” [TaskRepository] DB Queue Status raw counts: {counts}")
            
            pending = counts.get('pending', 0)
            running = counts.get('running', 0)
            completed = counts.get('completed', 0)
            failed = counts.get('failed', 0)
            total = sum(counts.values())
            
            # è®¡ç®—å¹³å‡å»¶è¿Ÿ (æœ€è¿‘ 100 æ¡å®Œæˆçš„ä»»åŠ¡)
            from sqlalchemy import desc
            avg_delay = 0
            try:
                delay_stmt = (
                    select(TaskQueue.started_at, TaskQueue.created_at)
                    .where(TaskQueue.status == 'completed', TaskQueue.started_at != None)
                    .order_by(desc(TaskQueue.completed_at))
                    .limit(100)
                )
                results = (await session.execute(delay_stmt)).all()
                if results:
                    total_delay = sum((r.started_at - r.created_at).total_seconds() for r in results)
                    avg_delay = total_delay / len(results)
            except Exception: pass

            # è®¡ç®—é”™è¯¯ç‡
            err_rate = 0.0
            if completed + failed > 0:
                err_rate = (failed / (completed + failed)) * 100

            return {
                'active_queues': pending,
                'running_tasks': running,
                'total_tasks': total,
                'completed_tasks': completed,
                'failed_tasks': failed,
                'error_rate': f"{err_rate:.1f}%",
                'avg_delay': f"{avg_delay:.1f}s"
            }

    async def get_rule_stats(self):
        """è·å–è§„åˆ™ç»Ÿè®¡ä¿¡æ¯ (åªè¯»)"""
        async with self.db.get_session(readonly=True) as session:
            # è·å–æ€»è§„åˆ™æ•°å’Œæ´»è·ƒè§„åˆ™æ•°
            total_rules = await session.execute(
                select(func.count(ForwardRule.id))
            )
            active_rules = await session.execute(
                select(func.count(ForwardRule.id)).where(ForwardRule.enable_rule == True)
            )
            total_chats = await session.execute(
                select(func.count(Chat.id))
            )
            
            return {
                'total_rules': total_rules.scalar() or 0,
                'active_rules': active_rules.scalar() or 0,
                'total_chats': total_chats.scalar() or 0
            }

    async def get_tasks(self, page: int = 1, limit: int = 50, status: str = None, task_type: str = None):
        """åˆ†é¡µè·å–ä»»åŠ¡åˆ—è¡¨ (åªè¯»)"""
        async with self.db.get_session(readonly=True) as session:
            # æ„å»ºæŸ¥è¯¢
            stmt = select(TaskQueue)
            if status:
                stmt = stmt.where(TaskQueue.status == status)
            if task_type:
                stmt = stmt.where(TaskQueue.task_type == task_type)
            
            # è®¡ç®—æ€»æ•°
            count_stmt = select(func.count()).select_from(stmt.subquery())
            total = (await session.execute(count_stmt)).scalar() or 0
            
            # æ’åºå’Œåˆ†é¡µ
            stmt = stmt.order_by(TaskQueue.priority.desc(), TaskQueue.created_at.desc())
            stmt = stmt.offset((page - 1) * limit).limit(limit)
            
            result = await session.execute(stmt)
            tasks = result.scalars().all()
            
            return tasks, total

    async def get_task_by_id(self, task_id: int):
        """è·å–å•ä¸ªä»»åŠ¡è¯¦æƒ… (åªè¯»)"""
        async with self.db.get_session(readonly=True) as session:
            stmt = select(TaskQueue).where(TaskQueue.id == task_id)
            result = await session.execute(stmt)
            return result.scalar_one_or_none()