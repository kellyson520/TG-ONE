from core.event_bus import EventBus
from core.pipeline import Pipeline
from repositories.task_repo import TaskRepository
from repositories.rule_repo import RuleRepository
from repositories.stats_repo import StatsRepository
from repositories.user_repo import UserRepository
from repositories.audit_repo import AuditRepository
from services.download_service import DownloadService
from services.worker_service import WorkerService
from services.queue_service import MessageQueueService
from middlewares.loader import RuleLoaderMiddleware
from middlewares.dedup import DedupMiddleware
from middlewares.download import DownloadMiddleware
from middlewares.sender import SenderMiddleware
from middlewares.sender import SenderMiddleware
from middlewares.filter import FilterMiddleware
from services.db_buffer import GroupCommitCoordinator
# å¼•å…¥å…¨å±€æ•°æ®åº“å•ä¾‹è·å–å‡½æ•°
from models.models import get_async_engine 
# å¼•å…¥ Database ç±»ï¼ˆæˆ‘ä»¬éœ€è¦ç¨å¾®æ”¹é€ å®ƒä»¥æ¥å—ç°æœ‰çš„ engineï¼‰
from core.database import Database 
import os
import asyncio
from pathlib import Path
from models.models import get_async_engine
import logging

logger = logging.getLogger(__name__)

class Container:
    def __init__(self):
        # [Critical Fix] ä¸å†åˆ›å»ºæ–°çš„ Database å®ä¾‹ï¼Œè€Œæ˜¯åŒ…è£…ç°æœ‰çš„å…¨å±€å¼•æ“
        # è¿™æ · main.py, web admin, worker éƒ½ä½¿ç”¨åŒä¸€ä¸ªè¿æ¥æ± 
        engine = get_async_engine()
        self.db = Database(engine=engine) 
        logger.info(f"Container connected to shared database engine: {engine.url}")
        
        # åˆå§‹åŒ–äº‹ä»¶æ€»çº¿
        self.bus = EventBus()
        logger.info("EventBus initialized")
        
        # åˆå§‹åŒ–ä»“åº“ (å¤ç”¨ç»Ÿä¸€çš„ db å®ä¾‹)
        self.task_repo = TaskRepository(self.db)
        self.rule_repo = RuleRepository(self.db)
        self.stats_repo = StatsRepository(self.db)
        self.user_repo = UserRepository(self.db)
        self.audit_repo = AuditRepository(self.db)
        logger.info("Repositories initialized")

        # åˆå§‹åŒ– Group Commit Coordinator (Buffer)
        # ä¼ é€’ self.db.session (async context manager) ä½œä¸º factory
        self.group_commit_coordinator = GroupCommitCoordinator(self.db.session)
        logger.info("GroupCommitCoordinator initialized")

        # åˆå§‹åŒ– Rate Limiter Pool
        from services.rate_limiter import RateLimiterPool
        self.rate_limiter_pool = RateLimiterPool
        logger.info("RateLimiterPool initialized with presets")

        # åˆå§‹åŒ– Metrics Collector
        from services.metrics_collector import metrics_collector
        self.metrics_collector = metrics_collector
        logger.info("MetricsCollector initialized")

        # åˆå§‹åŒ–èƒŒå‹é˜Ÿåˆ—æœåŠ¡ (Ingestion Buffer)
        self.queue_service = MessageQueueService(max_size=1000)
        # è®¾ç½®é˜Ÿåˆ—æ¶ˆè´¹è€…ï¼šå°†å†…å­˜é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡å†™å…¥æ•°æ®åº“
        self.queue_service.set_processor(self._process_ingestion_queue)
        logger.info("MessageQueueService initialized")
        
        # åˆå§‹åŒ–å»é‡æœåŠ¡
        from services.dedup_service import dedup_service
        dedup_service.set_db(self.db)
        dedup_service.set_coordinator(self.group_commit_coordinator)
        logger.info("Deduplication service initialized with GroupCommit Support")
        
        # åˆå§‹åŒ–èŠå¤©ä¿¡æ¯æœåŠ¡
        from services.chat_info_service import chat_info_service
        chat_info_service.set_db(self.db)
        self.chat_info_service = chat_info_service
        self.chat_info_service = chat_info_service
        logger.info("ChatInfoService initialized")
        
        # åˆå§‹åŒ– RuleManagementService
        from services.rule_management_service import RuleManagementService
        self.rule_management_service = RuleManagementService()
        logger.info("RuleManagementService initialized")
        
        # æ³¨å†Œäº‹ä»¶ç›‘å¬
        self.bus.subscribe("FORWARD_SUCCESS", self._on_stats_update)
        self.bus.subscribe("FORWARD_FAILED", self._on_forward_failed)
        # [Scheme 7 Standard] æ³¨å†Œå»é‡è®°å½•ç›‘å¬å™¨
        # åªæœ‰å½“æ¶ˆæ¯å‘é€æˆåŠŸåï¼Œæ‰å°†å…¶ç‰¹å¾æŒ‡çº¹è®°å½•åˆ°æ•°æ®åº“
        # é¿å…å› å‘é€å¤±è´¥è€Œå¯¼è‡´é”™è¯¯åœ°æ‹¦æˆªäº†é‡è¯•æ¶ˆæ¯
        from services.dedup_service import dedup_service
        self.bus.subscribe("FORWARD_SUCCESS", dedup_service.on_forward_success)
        logger.info("Event listeners registered")
        
        # æœåŠ¡åˆ—è¡¨ï¼Œç”¨äºç»Ÿä¸€ç®¡ç†ç”Ÿå‘½å‘¨æœŸ
        self.services = []
        
        # æœåŠ¡å®ä¾‹
        self.downloader = None  # éœ€è¦ client
        self.worker = None      # éœ€è¦ client
        self.scheduler = None   # éœ€è¦ client
        self.chat_updater = None  # éœ€è¦ client
        self.rss_puller = None  # éœ€è¦ client

    def init_with_client(self, user_client, bot_client):
        self.user_client = user_client
        self.bot_client = bot_client
        # åˆå§‹åŒ–æœåŠ¡
        self.downloader = DownloadService(user_client)
        logger.info("DownloadService initialized")
        
        # ç»„è£…ç®¡é“ (Order matters!)
        pipeline = Pipeline()
        pipeline.add(RuleLoaderMiddleware(self.rule_repo))  # 1. åŠ è½½è§„åˆ™
        pipeline.add(DedupMiddleware())                     # 2. å»é‡æ£€æŸ¥
        pipeline.add(FilterMiddleware())                    # 3. è¿‡æ»¤ & å†…å®¹ä¿®æ”¹
        from middlewares.ai import AIMiddleware             # å¼•å…¥AIä¸­é—´ä»¶
        pipeline.add(AIMiddleware())                        # 4. AI å¤„ç† (ä¾èµ– Filter çš„ä¿®æ”¹)
        # æš‚æ—¶ç§»é™¤ DownloadMiddleware
        # pipeline.add(DownloadMiddleware(self.downloader))   # 5. ä¸‹è½½ (å¦‚æœéœ€è¦)
        pipeline.add(SenderMiddleware(self.bus))            # 5. å‘é€ & å»é‡å›å†™
        logger.info("âœ… Pipeline assembled: Loader -> Dedup -> Filter -> AI -> Sender")
        
        # [Dependency Injection] å°† downloader ç›´æ¥æ³¨å…¥ workerï¼Œè§£è€¦å…¨å±€ä¾èµ–
        self.worker = WorkerService(user_client, self.task_repo, pipeline, self.downloader)
        logger.info("WorkerService initialized with injected dependencies")
        
        # åˆå§‹åŒ–è°ƒåº¦å™¨
        from scheduler.summary_scheduler import SummaryScheduler
        self.scheduler = SummaryScheduler(user_client, bot_client, self.task_repo, self.db)
        logger.info("SummaryScheduler initialized with injected dependencies")
        
        # åˆå§‹åŒ–ä¼˜åŒ–çš„èŠå¤©æ›´æ–°å™¨
        from scheduler.optimized_chat_updater import OptimizedChatUpdater
        self.chat_updater = OptimizedChatUpdater(user_client, self.db)
        logger.info("OptimizedChatUpdater initialized with injected dependencies")
        
        # åˆå§‹åŒ–é€šçŸ¥æœåŠ¡ (H.5.C3)
        from services.notification_service import NotificationService
        self.notification_service = NotificationService(bot_client, self.bus)
        logger.info("NotificationService initialized")

        # åˆå§‹åŒ– RSS æ‹‰å–æœåŠ¡ (AIMD)
        from services.rss_pull_service import RSSPullService
        self.rss_puller = RSSPullService(user_client, bot_client)
        logger.info("RSSPullService initialized")

        # è®© ChatInfoService èƒ½å¤Ÿè°ƒç”¨ Telegram API
        self.chat_info_service.set_client(user_client)

        return self.worker

    async def start_all(self):
        """ç»Ÿä¸€å¯åŠ¨æ‰€æœ‰æœåŠ¡"""
        if not self.worker or not self.scheduler or not self.chat_updater:
            raise RuntimeError("Clients not initialized. Call init_with_client() first.")
            
        logger.info("ğŸš€ Starting all services...")
        
        # ä½¿ç”¨ asyncio.create_task å¯åŠ¨å¹¶ç”± Container æŒæœ‰å¼•ç”¨
        self.services.append(asyncio.create_task(self.worker.start(), name="Worker"))
        self.services.append(asyncio.create_task(self.scheduler.start(), name="Scheduler"))
        self.services.append(asyncio.create_task(self.chat_updater.start(), name="ChatUpdater"))
        self.services.append(asyncio.create_task(self.rss_puller.start(), name="RSSPuller"))
        
        # å¯åŠ¨ StatsRepository çš„ç¼“å†²åˆ·æ–°ä»»åŠ¡ (H.5)
        await self.stats_repo.start()
        
        # å¯åŠ¨èƒŒå‹é˜Ÿåˆ—æœåŠ¡
        # å¯åŠ¨èƒŒå‹é˜Ÿåˆ—æœåŠ¡
        await self.queue_service.start()
        
        # å¯åŠ¨ Group Commit Coordinator
        await self.group_commit_coordinator.start()

        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å¥åº·æ£€æŸ¥æˆ–å¯åŠ¨é¡ºåºæ§åˆ¶
        logger.info(f"âœ… {len(self.services)} services started.")

    async def shutdown(self):
        """ç»Ÿä¸€ä¼˜é›…å…³é—­"""
        logger.info("ğŸ›‘ Stopping all services...")
        
        # 1. å…ˆåœæ­¢æ¥æ”¶æ–°ä»»åŠ¡ (Scheduler)
        if self.scheduler:
            self.scheduler.stop()
            logger.info("SummaryScheduler stopped accepting new tasks")
            
        # 2. åœæ­¢æ¶ˆè´¹è€… (Worker)
        if self.worker:
            logger.info("Stopping WorkerService...")
            await self.worker.stop()
            logger.info("WorkerService stopped")
            
        # 3. åœæ­¢è¾…åŠ©æœåŠ¡
        if self.chat_updater:
            logger.info("Stopping OptimizedChatUpdater...")
            await self.chat_updater.stop()
            logger.info("OptimizedChatUpdater stopped")
        
        if self.downloader:
            logger.info("Stopping DownloadService...")
            await self.downloader.shutdown()
            logger.info("DownloadService stopped")

        if self.rss_puller:
            logger.info("Stopping RSSPullService...")
            await self.rss_puller.stop()
            logger.info("RSSPullService stopped")

        # åœæ­¢ StatsRepository çš„ç¼“å†²åˆ·æ–°ä»»åŠ¡ (H.5)
        if self.stats_repo:
            logger.info("Stopping StatsRepository...")
            await self.stats_repo.stop()
            
        # åœæ­¢èƒŒå‹é˜Ÿåˆ—æœåŠ¡
        if self.queue_service:
            logger.info("Stopping MessageQueueService...")
            await self.queue_service.stop()
            logger.info("Stopping MessageQueueService...")
            await self.queue_service.stop()
            
        # åœæ­¢ Group Commit Coordinator
        if self.group_commit_coordinator:
            logger.info("Stopping GroupCommitCoordinator...")
            await self.group_commit_coordinator.stop()

        # ä¿å­˜ Bloom Filter
        try:
            from services.bloom_filter import bloom_filter_service
            bloom_filter_service.save()
            logger.info("Bloom Filter saved")
        except Exception as e:
            logger.error(f"Failed to save Bloom Filter: {e}")

        # 4. ç­‰å¾…æ‰€æœ‰åå°ä»»åŠ¡ç»“æŸ
        # cancel æ‰è¿˜åœ¨è¿è¡Œçš„ task (å¦‚ scheduler çš„æ— é™å¾ªç¯)
        logger.info(f"Cancelling {len(self.services)} running tasks...")
        for task in self.services:
            if not task.done():
                task.cancel()
        
        logger.info("Waiting for all tasks to complete...")
        await asyncio.gather(*self.services, return_exceptions=True)
        
        # æ¸…ç©ºæœåŠ¡åˆ—è¡¨
        self.services.clear()
        
        # [Fix] ä¸è¦åœ¨è¿™é‡Œ dispose engineï¼Œå› ä¸ºå®ƒæ˜¯å…¨å±€å…±äº«çš„
        # è®© main.py æˆ–ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨è´Ÿè´£æœ€ç»ˆçš„ dispose
        # await self.db.close() 
        logger.info("âœ… System shutdown complete")
    
    async def _on_stats_update(self, data):
        """å¤„ç†è½¬å‘æˆåŠŸäº‹ä»¶ï¼Œå¹¶å‘å†™å…¥æ—¥å¿—å’Œç»Ÿè®¡è¡¨"""
        try:
            await asyncio.gather(
                self.stats_repo.log_action(data['rule_id'], data['msg_id'], "success"),
                self.stats_repo.increment_stats(data['target_id']),
                self.stats_repo.increment_rule_stats(data['rule_id'], "success")
            )
            logger.debug(f"Stats updated for rule {data['rule_id']}, message {data['msg_id']}")
        except Exception as e:
            logger.error(f"Failed to update stats: {str(e)}")

    async def _on_forward_failed(self, data):
        """å¤„ç†è½¬å‘å¤±è´¥äº‹ä»¶"""
        try:
            rule_id = data.get('rule_id')
            if not rule_id:
                return
            
            await asyncio.gather(
                self.stats_repo.log_action(rule_id, 0, "error", result=data.get('error')),
                self.stats_repo.increment_rule_stats(rule_id, "error")
            )
            logger.debug(f"Error logged for rule {rule_id}: {data.get('error')}")
        except Exception as e:
            logger.error(f"Failed to log error: {str(e)}")

    async def _process_ingestion_queue(self, items):
        """
        å¤„ç† ingestion é˜Ÿåˆ—é¡¹ (Batch)
        items: List[(task_type, payload, priority)]
        """
        try:
            if not items:
                return
            # è°ƒç”¨æ‰¹é‡å†™å…¥
            await self.task_repo.push_batch(items)
        except Exception as e:
            logger.error(f"Batch ingestion failed: {e}", exc_info=True)


container = Container()