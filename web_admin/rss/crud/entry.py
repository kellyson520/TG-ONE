import json
import logging
import uuid
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from ..models.entry import Entry
from utils.processing.unified_cache import get_smart_cache
from ..core.config import settings

logger = logging.getLogger(__name__)

# ç¡®ä¿æ•°æ®å­˜å‚¨ç›®å½•å­˜åœ¨
def ensure_storage_exists():
    """ç¡®ä¿æ•°æ®å­˜å‚¨ç›®å½•å­˜åœ¨"""
    entries_dir = Path(settings.DATA_PATH)
    entries_dir.mkdir(parents=True, exist_ok=True)

# è·å–è§„åˆ™å¯¹åº”çš„æ¡ç›®å­˜å‚¨æ–‡ä»¶è·¯å¾?
def get_rule_entries_path(rule_id: int) -> Path:
    """è·å–è§„åˆ™å¯¹åº”çš„æ¡ç›®å­˜å‚¨æ–‡ä»¶è·¯å¾?""
    # ä½¿ç”¨è§„åˆ™ç‰¹å®šçš„æ•°æ®ç›®å½?
    rule_data_path = settings.get_rule_data_path(rule_id)
    return Path(rule_data_path) / "entries.json"

async def get_entries(rule_id: int, limit: int = 100, offset: int = 0) -> List[Entry]:
    """è·å–è§„åˆ™å¯¹åº”çš„æ¡ç›?""
    try:
        file_path = get_rule_entries_path(rule_id)
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡?
        if not file_path.exists():
            return []
        
        # è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆå¸¦ L1/L2 ç¼“å­˜ï¼?
        cache = get_smart_cache("rss.entries", l1_ttl=10, l2_ttl=20)
        cache_key = f"entries:{rule_id}:{file_path.stat().st_mtime_ns}"
        data = cache.get(cache_key)
        if data is None:
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            cache.set(cache_key, data)
            
        # å°†æ•°æ®è½¬æ¢ä¸ºEntryå¯¹è±¡
        entries = [Entry(**entry) for entry in data]
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæ–°çš„åœ¨å‰ï¼?
        entries.sort(key=lambda x: x.published, reverse=True)
        
        # åº”ç”¨åˆ†é¡µ
        return entries[offset:offset + limit]
    except Exception as e:
        logger.error(f"è·å–æ¡ç›®æ—¶å‡ºé”? {str(e)}")
        return []

async def create_entry(entry: Entry) -> bool:
    """åˆ›å»ºæ–°æ¡ç›?""
    try:
        # è®¾ç½®æ¡ç›®IDå’Œåˆ›å»ºæ—¶é—?
        if not entry.id:
            entry.id = str(uuid.uuid4())
        
        entry.created_at = datetime.now().isoformat()
        
        # è·å–è§„åˆ™å¯¹åº”çš„æ¡ç›?
        file_path = get_rule_entries_path(entry.rule_id)
        
        entries = []
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ¡ç›®
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as file:
                try:
                    entries = json.load(file)
                except json.JSONDecodeError:
                    logger.warning(f"è§£ææ¡ç›®æ–‡ä»¶æ—¶å‡ºé”™ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {file_path}")
                    entries = []
        
        # è½¬æ¢Entryå¯¹è±¡ä¸ºå­—å…¸å¹¶æ·»åŠ åˆ°åˆ—è¡?
        entries.append(entry.dict())
        
        # è·å–è§„åˆ™çš„RSSé…ç½®ï¼Œè·å–æœ€å¤§æ¡ç›®æ•°é‡?
        try:
            from models.models import get_read_session as get_session, RSSConfig
            session = get_session()
            rss_config = session.query(RSSConfig).filter(RSSConfig.rule_id == entry.rule_id).first()
            max_items = rss_config.max_items if rss_config and hasattr(rss_config, 'max_items') else 50
            session.close()
        except Exception as e:
            logger.warning(f"è·å–RSSé…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æœ€å¤§æ¡ç›®æ•°é‡?50): {str(e)}")
            max_items = 50
        
        # é™åˆ¶æ¡ç›®æ•°é‡ï¼Œä¿ç•™æœ€æ–°çš„Næ?
        if len(entries) > max_items:
            # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæ–°çš„åœ¨å‰ï¼?
            entries.sort(key=lambda x: x.get('published', ''), reverse=True)
            entries = entries[:max_items]
        
        # ä¿å­˜åˆ°æ–‡ä»?
        with open(file_path, 'w', encoding='utf-8') as file:
            json.dump(entries, file, ensure_ascii=False, indent=2)
        
        return True
    except Exception as e:
        logger.error(f"åˆ›å»ºæ¡ç›®æ—¶å‡ºé”? {str(e)}")
        return False

async def update_entry(rule_id: int, entry_id: str, updated_data: Dict[str, Any]) -> bool:
    """æ›´æ–°æ¡ç›®"""
    try:
        file_path = get_rule_entries_path(rule_id)
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›False
        if not file_path.exists():
            return False
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as file:
            entries = json.load(file)
        
        # æŸ¥æ‰¾å¹¶æ›´æ–°æ¡ç›?
        found = False
        for i, entry in enumerate(entries):
            if entry.get('id') == entry_id:
                entries[i].update(updated_data)
                found = True
                break
        
        if not found:
            return False
        
        # ä¿å­˜åˆ°æ–‡ä»?
        with open(file_path, 'w', encoding='utf-8') as file:
            json.dump(entries, file, ensure_ascii=False, indent=2)
        
        return True
    except Exception as e:
        logger.error(f"æ›´æ–°æ¡ç›®æ—¶å‡ºé”? {str(e)}")
        return False

async def delete_entry(rule_id: int, entry_id: str) -> bool:
    """åˆ é™¤æ¡ç›®"""
    try:
        file_path = get_rule_entries_path(rule_id)
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›False
        if not file_path.exists():
            return False
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as file:
            entries = json.load(file)
        
        # æŸ¥æ‰¾å¹¶åˆ é™¤æ¡ç›?
        original_length = len(entries)
        entries = [entry for entry in entries if entry.get('id') != entry_id]
        
        if len(entries) == original_length:
            return False  # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”IDçš„æ¡ç›?
        
        # ä¿å­˜åˆ°æ–‡ä»?
        with open(file_path, 'w', encoding='utf-8') as file:
            json.dump(entries, file, ensure_ascii=False, indent=2)
        
        return True
    except Exception as e:
        logger.error(f"åˆ é™¤æ¡ç›®æ—¶å‡ºé”? {str(e)}")
        return False 