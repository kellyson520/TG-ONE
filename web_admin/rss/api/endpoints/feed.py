from fastapi import APIRouter, HTTPException, Depends, Body, Request
from fastapi.responses import Response, FileResponse
from typing import Dict, Any
import logging
import os
import json
from pathlib import Path
from ...services.feed_generator import FeedService
from ...models.entry import Entry
from ...core.config import settings
from ...crud.entry import get_entries, create_entry, delete_entry
from utils.processing.unified_cache import cached
import mimetypes
from models.models import get_read_session as get_session, RSSConfig
from datetime import datetime
from ai import get_ai_provider
from models.models import get_session, ForwardRule
import re
from models.models import RSSPattern
import shutil
import time
import os
import subprocess
import platform
from pydantic import ValidationError
from utils.core.constants import RSS_MEDIA_BASE_URL

logger = logging.getLogger(__name__)
router = APIRouter()

# æ·»åŠ æœ¬åœ°è®¿é—®éªŒè¯ä¾èµ–
async def verify_local_access(request: Request):
    """éªŒè¯è¯·æ±‚æ˜¯å¦æ¥è‡ªæœ¬åœ°æˆ–Dockerå†…éƒ¨ç½‘ç»œ"""
    client_host = request.client.host if request.client else None
    
    # å…è®¸çš„æœ¬åœ°IPåœ°å€åˆ—è¡¨
    local_addresses = ['127.0.0.1', '::1', 'localhost', '0.0.0.0']
    
    # å¦‚æœè®¾ç½®äº?HOST ç¯å¢ƒå˜é‡ï¼Œä¹Ÿå°†å…¶æ·»åŠ åˆ°å…è®¸åˆ—è¡¨ä¸­
    if hasattr(settings, 'HOST') and settings.HOST:
        local_addresses.append(settings.HOST)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯Dockerå†…éƒ¨ç½‘ç»œIP (å¸¸è§çš„ç§æœ‰ç½‘ç»œèŒƒå›?
    docker_ip = False
    if client_host:
        docker_prefixes = ['172.', '192.168.', '10.']
        docker_ip = any(client_host.startswith(prefix) for prefix in docker_prefixes)
    
    # å¦‚æœæ˜¯æœ¬åœ°åœ°å€æˆ–Dockerå†…éƒ¨ç½‘ç»œIPï¼Œå…è®¸è®¿é—?
    if client_host in local_addresses or docker_ip:
        logger.debug(f"å·²éªŒè¯è®¿é—®æƒé™? {client_host}")
        return True
    
    # æ‹’ç»æ¥è‡ªå¤–éƒ¨ç½‘ç»œçš„è®¿é—?
    logger.warning(f"æ‹’ç»æ¥è‡ªå¤–éƒ¨ç½‘ç»œçš„è®¿é—? {client_host}")
    raise HTTPException(
        status_code=403, 
        detail="æ­¤APIç«¯ç‚¹ä»…å…è®¸æœ¬åœ°æˆ–å†…éƒ¨ç½‘ç»œè®¿é—®"
    )

@router.get("/")
async def root():
    """æœåŠ¡çŠ¶æ€æ£€æŸ?""
    return {
        "status": "ok",
        "service": "TG Forwarder RSS"
    }

@router.get("/rss/feed/{rule_id}")
@cached(cache_name="rss.get_feed", ttl=30)
async def get_feed(rule_id: int, request: Request):
    """è¿”å›è§„åˆ™å¯¹åº”çš„RSS Feed"""
    session = None
    try:
        # åˆ›å»ºæ•°æ®åº“ä¼šè¯?
        session = get_session()
        
        # æŸ¥è¯¢è§„åˆ™é…ç½®
        rss_config = session.query(RSSConfig).filter(RSSConfig.rule_id == rule_id).first()
        if not rss_config or not rss_config.enable_rss:
            logger.warning(f"è§„åˆ™ {rule_id} çš„RSSæœªå¯ç”¨æˆ–ä¸å­˜åœ?)
            raise HTTPException(status_code=404, detail="RSS feed æœªå¯ç”¨æˆ–ä¸å­˜åœ?)
        
        # è·å–è¯·æ±‚URLçš„åŸºç¡€éƒ¨åˆ†
        base_url = str(request.base_url).rstrip('/')
        logger.info(f"è¯·æ±‚åŸºç¡€URL: {base_url}")
        logger.info(f"è¯·æ±‚å¤? {request.headers}")
        logger.info(f"è¯·æ±‚å®¢æˆ·ç«? {request.client}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¯å¢ƒå˜é‡ä¸­é…ç½®çš„åŸºç¡€URL
        if RSS_MEDIA_BASE_URL:
            logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­é…ç½®çš„åª’ä½“åŸºç¡€URL: {RSS_MEDIA_BASE_URL}")
            base_url = RSS_MEDIA_BASE_URL.rstrip('/')
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰X-Forwarded-Hostæˆ–Hostå¤?
            forwarded_host = request.headers.get("X-Forwarded-Host")
            host_header = request.headers.get("Host")
            if forwarded_host:
                logger.info(f"æ£€æµ‹åˆ°X-Forwarded-Host: {forwarded_host}")
                # æ„å»ºåŸºäºforwarded_hostçš„URL
                scheme = request.headers.get("X-Forwarded-Proto", "http")
                base_url = f"{scheme}://{forwarded_host}"
                logger.info(f"åŸºäºX-Forwarded-Hostçš„åª’ä½“åŸºç¡€URL: {base_url}")
            elif host_header and host_header != f"{settings.HOST}:{settings.PORT}":
                logger.info(f"æ£€æµ‹åˆ°è‡ªå®šä¹‰Host: {host_header}")
                # æ„å»ºåŸºäºHostçš„URL
                scheme = request.url.scheme
                base_url = f"{scheme}://{host_header}"
                logger.info(f"åŸºäºHostçš„åª’ä½“åŸºç¡€URL: {base_url}")
        
        logger.info(f"æœ€ç»ˆä½¿ç”¨çš„åª’ä½“åŸºç¡€URL: {base_url}")
        
        # è·å–è§„åˆ™å¯¹åº”çš„æ¡ç›?
        entries = await get_entries(rule_id)
        logger.info(f"è·å–åˆ?{len(entries)} ä¸ªæ¡ç›?)
        
        # å¦‚æœæ²¡æœ‰æ¡ç›®ï¼Œè¿”å›æµ‹è¯•æ•°æ?
        if not entries:
            logger.warning(f"è§„åˆ™ {rule_id} æ²¡æœ‰æ¡ç›®æ•°æ®ï¼Œè¿”å›æµ‹è¯•æ•°æ?)
            try:
                fg = FeedService.generate_test_feed(rule_id, base_url)
                
                # ç”Ÿæˆ RSS XML
                rss_xml = fg.rss_str(pretty=True)
                
                # ç¡®ä¿rss_xmlæ˜¯å­—ç¬¦ä¸²ç±»å‹
                if isinstance(rss_xml, bytes):
                    logger.info("å°†RSS XMLä»å­—èŠ‚è½¬æ¢ä¸ºå­—ç¬¦ä¸?)
                    rss_xml = rss_xml.decode('utf-8')
                
                # è®°å½•XMLå†…å®¹çš„ä¸€éƒ¨åˆ†
                xml_sample = rss_xml[:500] + "..." if len(rss_xml) > 500 else rss_xml
                logger.info(f"ç”Ÿæˆçš„æµ‹è¯•RSS XML (å‰?00å­—ç¬¦): {xml_sample}")
                
                # æ£€æŸ¥XMLä¸­æ˜¯å¦è¿˜æœ‰ç¡¬ç¼–ç çš„localhostæˆ?27.0.0.1åœ°å€
                if "127.0.0.1" in rss_xml or "localhost" in rss_xml:
                    logger.warning(f"RSS XMLä¸­ä»åŒ…å«ç¡¬ç¼–ç çš„æœ¬åœ°åœ°å€")
                    
                    # æ›¿æ¢ç¡¬ç¼–ç çš„åœ°å€
                    rss_xml = rss_xml.replace(f"http://127.0.0.1:{settings.PORT}", base_url)
                    rss_xml = rss_xml.replace(f"http://localhost:{settings.PORT}", base_url)
                    rss_xml = rss_xml.replace(f"http://{settings.HOST}:{settings.PORT}", base_url)
                    
                    logger.info(f"å·²æ›¿æ¢ç¡¬ç¼–ç çš„æœ¬åœ°åœ°å€ä¸? {base_url}")
                
                # ç¡®ä¿è¿”å›çš„æ˜¯å­—èŠ‚ç±»å‹
                if isinstance(rss_xml, str):
                    rss_xml = rss_xml.encode('utf-8')
                
                return Response(
                    content=rss_xml,
                    media_type="application/xml; charset=utf-8"
                )
            except Exception as e:
                logger.error(f"ç”Ÿæˆæµ‹è¯•Feedæ—¶å‡ºé”? {str(e)}", exc_info=True)
                raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæµ‹è¯•Feedå¤±è´¥: {str(e)}")
        else:
            # æ ¹æ®çœŸå®æ•°æ®ç”Ÿæˆ Feedï¼Œä¼ å…¥åŸºç¡€URL
            try:
                fg = await FeedService.generate_feed_from_entries(rule_id, entries, base_url)
                
                # ç”Ÿæˆ RSS XML
                rss_xml = fg.rss_str(pretty=True)
                
                # ç¡®ä¿rss_xmlæ˜¯å­—ç¬¦ä¸²ç±»å‹
                if isinstance(rss_xml, bytes):
                    logger.info("å°†RSS XMLä»å­—èŠ‚è½¬æ¢ä¸ºå­—ç¬¦ä¸?)
                    rss_xml = rss_xml.decode('utf-8')
                
                # è®°å½•XMLå†…å®¹çš„ä¸€éƒ¨åˆ†
                xml_sample = rss_xml[:500] + "..." if len(rss_xml) > 500 else rss_xml
                logger.info(f"ç”Ÿæˆçš„RSS XML (å‰?00å­—ç¬¦): {xml_sample}")
                
                # æ£€æŸ¥XMLä¸­æ˜¯å¦è¿˜æœ‰ç¡¬ç¼–ç çš„localhostæˆ?27.0.0.1åœ°å€
                if "127.0.0.1" in rss_xml or "localhost" in rss_xml:
                    logger.warning(f"RSS XMLä¸­ä»åŒ…å«ç¡¬ç¼–ç çš„æœ¬åœ°åœ°å€")
                    
                    # æ›¿æ¢ç¡¬ç¼–ç çš„åœ°å€
                    rss_xml = rss_xml.replace(f"http://127.0.0.1:{settings.PORT}", base_url)
                    rss_xml = rss_xml.replace(f"http://localhost:{settings.PORT}", base_url)
                    rss_xml = rss_xml.replace(f"http://{settings.HOST}:{settings.PORT}", base_url)
                    
                    logger.info(f"å·²æ›¿æ¢ç¡¬ç¼–ç çš„æœ¬åœ°åœ°å€ä¸? {base_url}")
                
                # ç¡®ä¿è¿”å›çš„æ˜¯å­—èŠ‚ç±»å‹
                if isinstance(rss_xml, str):
                    rss_xml = rss_xml.encode('utf-8')
                
                return Response(
                    content=rss_xml,
                    media_type="application/xml; charset=utf-8"
                )
            except Exception as e:
                logger.error(f"ç”ŸæˆçœŸå®æ¡ç›®Feedæ—¶å‡ºé”? {str(e)}", exc_info=True)
                raise HTTPException(status_code=500, detail=f"ç”ŸæˆFeedå¤±è´¥: {str(e)}")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç”ŸæˆRSS feedæ—¶å‡ºé”? {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
    finally:
        # ç¡®ä¿ä¼šè¯è¢«å…³é—?
        if session:
            session.close()

@router.get("/media/{rule_id}/{filename}")
async def get_media(rule_id: int, filename: str, request: Request):
    """è¿”å›åª’ä½“æ–‡ä»¶"""
    # è®°å½•è¯·æ±‚ä¿¡æ¯
    logger.info(f"åª’ä½“è¯·æ±‚ - è§„åˆ™ID: {rule_id}, æ–‡ä»¶å? {filename}")
    logger.info(f"è¯·æ±‚URL: {request.url}")
    logger.info(f"è¯·æ±‚å¤? {request.headers}")
    
    # è·å–åŸºç¡€URLï¼Œç”¨äºæ—¥å¿—è®°å½?
    base_url = str(request.base_url).rstrip('/')
    if RSS_MEDIA_BASE_URL:
        logger.info(f"ç¯å¢ƒå˜é‡ä¸­é…ç½®çš„åª’ä½“åŸºç¡€URL: {RSS_MEDIA_BASE_URL}")
        base_url = RSS_MEDIA_BASE_URL.rstrip('/')
    else:
        # æ£€æŸ¥æ˜¯å¦æœ‰X-Forwarded-Hostæˆ–Hostå¤?
        forwarded_host = request.headers.get("X-Forwarded-Host")
        host_header = request.headers.get("Host")
        if forwarded_host:
            logger.info(f"æ£€æµ‹åˆ°X-Forwarded-Host: {forwarded_host}")
            scheme = request.headers.get("X-Forwarded-Proto", "http")
            base_url = f"{scheme}://{forwarded_host}"
        elif host_header and host_header != f"{settings.HOST}:{settings.PORT}":
            logger.info(f"æ£€æµ‹åˆ°è‡ªå®šä¹‰Host: {host_header}")
            scheme = request.url.scheme
            base_url = f"{scheme}://{host_header}"
    
    logger.info(f"æœ€ç»ˆä½¿ç”¨çš„åª’ä½“åŸºç¡€URL: {base_url}")
    
    # æ„å»ºè§„åˆ™ç‰¹å®šçš„åª’ä½“æ–‡ä»¶è·¯å¾?
    media_path = Path(settings.get_rule_media_path(rule_id)) / filename
    
    # è®°å½•å°è¯•è®¿é—®çš„è·¯å¾?
    logger.info(f"å°è¯•è®¿é—®åª’ä½“æ–‡ä»¶: {media_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ?
    if not media_path.exists():
        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›404
        logger.error(f"åª’ä½“æ–‡ä»¶æœªæ‰¾åˆ? {filename}")
        raise HTTPException(status_code=404, detail=f"åª’ä½“æ–‡ä»¶æœªæ‰¾åˆ? {filename}")
    
    # ç¡®å®šæ­£ç¡®çš„MIMEç±»å‹
    mime_type = mimetypes.guess_type(str(media_path))[0]
    if not mime_type:
        # å¦‚æœæ— æ³•ç¡®å®šMIMEç±»å‹ï¼Œæ ¹æ®æ–‡ä»¶æ‰©å±•åçŒœæµ‹
        ext = filename.split('.')[-1].lower() if '.' in filename else ''
        if ext in ['mp4', 'mov', 'avi', 'webm']:
            mime_type = f"video/{ext}"
        elif ext in ['mp3', 'wav', 'ogg', 'flac']:
            mime_type = f"audio/{ext}"
        elif ext in ['jpg', 'jpeg', 'png', 'gif']:
            mime_type = f"image/{ext}"
        else:
            mime_type = "application/octet-stream"
            
    logger.info(f"å‘é€åª’ä½“æ–‡ä»? {filename}, MIMEç±»å‹: {mime_type}, å¤§å°: {os.path.getsize(media_path)} å­—èŠ‚")
    
    # è¿”å›æ–‡ä»¶ï¼Œå¹¶è®¾ç½®æ­£ç¡®çš„Content-Type
    return FileResponse(
        path=media_path,
        media_type=mime_type,
        filename=filename
    )

@router.post("/api/entries/{rule_id}/add", dependencies=[Depends(verify_local_access)])
async def add_entry(rule_id: int, entry_data: Dict[str, Any] = Body(...)):
    """æ·»åŠ æ–°çš„æ¡ç›® (ä»…é™æœ¬åœ°è®¿é—®)"""
    try:
        # è®°å½•æ¥æ”¶åˆ°çš„æ•°æ®æ‘˜è¦
        media_count = len(entry_data.get("media", []))
        has_context = "context" in entry_data and entry_data["context"] is not None
        logger.info(f"æ¥æ”¶åˆ°æ–°æ¡ç›®æ•°æ®: è§„åˆ™ID={rule_id}, æ ‡é¢˜='{entry_data.get('title', 'æ— æ ‡é¢?)}', åª’ä½“æ•°é‡={media_count}, åŒ…å«ä¸Šä¸‹æ–?{has_context}")
        
        # è·å– RSS é…ç½®ä¿¡æ¯ï¼Œç¡®å®šæœ€å¤§æ¡ç›®æ•°é‡?
        session = get_session()
        max_items = None
        try:
            rss_config = session.query(RSSConfig).filter(RSSConfig.rule_id == rule_id).first()
            max_items = rss_config.max_items
        finally:
            session.close()
        
        # éªŒè¯åª’ä½“æ•°æ®
        if media_count > 0:
            media_filenames = []
            for m in entry_data.get("media", []):
                if isinstance(m, dict):
                    media_filenames.append(m.get('filename', 'æœªçŸ¥'))
                else:
                    media_filenames.append(getattr(m, 'filename', 'æœªçŸ¥'))
            logger.info(f"åª’ä½“æ–‡ä»¶åˆ—è¡¨: {media_filenames}")
            
            # ç¡®ä¿åª’ä½“æ–‡ä»¶å­˜åœ¨
            for media in entry_data.get("media", []):
                if isinstance(media, dict):
                    filename = media.get("filename", "")
                else:
                    filename = getattr(media, "filename", "")
                
                media_path = os.path.join(settings.MEDIA_PATH, filename)
                if not os.path.exists(media_path):
                    logger.warning(f"åª’ä½“æ–‡ä»¶ä¸å­˜åœ? {media_path}")
        
        # è®°å½•ä¸Šä¸‹æ–‡ä¿¡æ?
        if has_context:
            logger.info(f"æ¡ç›®åŒ…å«åŸå§‹ä¸Šä¸‹æ–‡å¯¹è±¡ï¼Œå±æ€? {', '.join(entry_data['context'].keys()) if hasattr(entry_data['context'], 'keys') else 'æ— æ³•è·å–å±æ€?}")
        
        # ç¡®ä¿å¿…è¦çš„å­—æ®µå­˜åœ?
        entry_data["rule_id"] = rule_id
        if not entry_data.get("message_id"):
            entry_data["message_id"] = entry_data.get("id", "")
        
        # æ£€æŸ¥å½“å‰æ¡ç›®æ•°é‡ï¼Œå¦‚æœæ¥è¿‘é™åˆ¶åˆ™åˆ é™¤æœ€æ—§çš„æ¡ç›®
        current_entries = await get_entries(rule_id)
        if len(current_entries) >= max_items - 1:
            # è®¡ç®—éœ€è¦åˆ é™¤çš„æ¡ç›®æ•°é‡ï¼Œç¡®ä¿æ·»åŠ æ–°æ¡ç›®åæ€»æ•°ä¸è¶…è¿‡æœ€å¤§é™åˆ?
            to_delete_count = len(current_entries) - (max_items - 1)
            if to_delete_count > 0:
                logger.info(f"å½“å‰æ¡ç›®æ•°é‡({len(current_entries)})å°†è¶…è¿‡é™åˆ?{max_items})ï¼Œéœ€è¦åˆ é™?{to_delete_count} ä¸ªæœ€æ—©çš„æ¡ç›®")
                
                # å¯¹æ¡ç›®æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
                sorted_entries = sorted(current_entries, key=lambda e: datetime.fromisoformat(e.published) if hasattr(e, 'published') else datetime.now())
                
                # è·å–è¦åˆ é™¤çš„æ¡ç›®
                entries_to_delete = sorted_entries[:to_delete_count]
                
                # åˆ é™¤å¤šä½™æ¡ç›®
                for entry in entries_to_delete:
                    try:
                        # åˆ é™¤æ¡ç›®å‰å…ˆå¤„ç†å…¶åª’ä½“æ–‡ä»?
                        if hasattr(entry, 'media') and entry.media:
                            logger.info(f"æ¡ç›® {entry.id} åŒ…å« {len(entry.media)} ä¸ªåª’ä½“æ–‡ä»¶ï¼Œå°†ä¸€å¹¶åˆ é™?)
                            
                            # åˆ é™¤åª’ä½“æ–‡ä»¶
                            media_dir = Path(settings.get_rule_media_path(rule_id))
                            for media in entry.media:
                                if hasattr(media, 'filename'):
                                    media_path = media_dir / media.filename
                                    if media_path.exists():
                                        try:
                                            os.remove(media_path)
                                            logger.info(f"å·²åˆ é™¤åª’ä½“æ–‡ä»? {media_path}")
                                        except Exception as e:
                                            logger.error(f"åˆ é™¤åª’ä½“æ–‡ä»¶å¤±è´¥: {media_path}, é”™è¯¯: {str(e)}")
                        
                        # åˆ é™¤æ¡ç›®
                        success = await delete_entry(rule_id, entry.id)
                        if success:
                            logger.info(f"å·²åˆ é™¤æ¡ç›? {entry.id}")
                        else:
                            logger.warning(f"åˆ é™¤æ¡ç›®å¤±è´¥: {entry.id}")
                    except Exception as e:
                        logger.error(f"å¤„ç†è¿‡æœŸæ¡ç›®æ—¶å‡ºé”? {str(e)}")
        
        # è½¬æ¢ä¸ºEntryå¯¹è±¡
        entry = Entry(
            rule_id=rule_id,
            message_id=entry_data.get("message_id", entry_data.get("id", "")),
            title=entry_data.get("title", "æ–°æ¶ˆæ?),
            content=entry_data.get("content", ""),
            published=entry_data.get("published"),
            author=entry_data.get("author", ""),
            link=entry_data.get("link", ""),
            media=entry_data.get("media", []),
            original_link=entry_data.get("original_link"),
            sender_info=entry_data.get("sender_info")
        )

        

        # ä½¿ç”¨AIæå–å†…å®¹
        if rss_config.is_ai_extract:
            try:
                rule = session.query(ForwardRule).filter(ForwardRule.id == rule_id).first()
                provider = await get_ai_provider(rule.ai_model)
                json_text = await provider.process_message(
                    message=entry.content or "",
                    prompt=rss_config.ai_extract_prompt,
                    model=rule.ai_model
                )
                logger.info(f"AIæå–å†…å®¹: {json_text}")
                
                # å»é™¤ä»£ç å—æ ‡è®°ï¼Œå¦‚æœæœ‰çš„è¯?
                if "```" in json_text:
                    # ç§»é™¤æ‰€æœ‰ä»£ç å—æ ‡è®°ï¼ŒåŒ…æ‹¬è¯­è¨€æ ‡è¯†å’Œç»“æŸæ ‡è®?
                    json_text = re.sub(r'```(\w+)?\n', '', json_text)  # å¼€å§‹æ ‡è®°ï¼ˆå¸¦å¯é€‰çš„è¯­è¨€æ ‡è¯†ï¼?
                    json_text = re.sub(r'\n```', '', json_text)  # ç»“æŸæ ‡è®°
                    json_text = json_text.strip()
                    logger.info(f"å»é™¤ä»£ç å—æ ‡è®°åçš„å†…å®? {json_text}")
                
                # è§£æJSONæ•°æ®
                try:
                    json_data = json.loads(json_text)
                    logger.info(f"è§£æåçš„JSONæ•°æ®: {json_data}")
                    
                    # æå–æ ‡é¢˜å’Œå†…å®?
                    title = json_data.get("title", "")
                    content = json_data.get("content", "")
                    entry.title = title
                    entry.content = content
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æé”™è¯¯: {str(e)}, åŸå§‹æ–‡æœ¬: {json_text}")
                    # å°è¯•å…¶ä»–æ¸…ç†æ–¹å¼
                    try:
                        # åŒ¹é…å¤§æ‹¬å·ä¹‹é—´çš„JSONå†…å®¹
                        json_match = re.search(r'\{.*\}', json_text, re.DOTALL)
                        if json_match:
                            clean_json = json_match.group(0)
                            logger.info(f"å°è¯•æå–JSON: {clean_json}")
                            json_data = json.loads(clean_json)
                            
                            # æå–æ ‡é¢˜å’Œå†…å®?
                            title = json_data.get("title", "")
                            content = json_data.get("content", "")
                            entry.title = title
                            entry.content = content
                            logger.info(f"æˆåŠŸä»æ–‡æœ¬ä¸­æå–JSONæ•°æ®")
                        else:
                            logger.error("æ— æ³•ä»AIå“åº”ä¸­æå–æœ‰æ•ˆJSON")
                    except Exception as inner_e:
                        logger.error(f"å°è¯•äºŒæ¬¡è§£æJSONæ—¶å‡ºé”? {str(inner_e)}")
                except Exception as e:
                    logger.error(f"å¤„ç†JSONæ•°æ®æ—¶å‡ºé”? {str(e)}")
            except Exception as e:
                logger.error(f"AIæå–å†…å®¹æ—¶å‡ºé”? {str(e)}")
            finally:
                if session:
                    session.close()
        
        logger.info(f"å¯ç”¨è‡ªå®šä¹‰æ ‡é¢˜æ¨¡å¼? {rss_config.enable_custom_title_pattern}, å¯ç”¨è‡ªå®šä¹‰å†…å®¹æ¨¡å¼? {rss_config.enable_custom_content_pattern}")
        if rss_config.enable_custom_title_pattern or rss_config.enable_custom_content_pattern:
            try:
                # è·å–åŸå§‹å†…å®¹
                original_content = entry.content or ""
                original_title = entry.title
                
                # å¦‚æœå¯ç”¨äº†æ ‡é¢˜æ­£åˆ™è¡¨è¾¾å¼æå–
                if rss_config.enable_custom_title_pattern:
                    # ç›´æ¥ä½¿ç”¨ä¼šè¯æŸ¥è¯¢æ ‡é¢˜æ¨¡å¼å¹¶æŒ‰ä¼˜å…ˆçº§æ’åº?
                    title_patterns = session.query(RSSPattern).filter_by(
                        rss_config_id=rss_config.id, 
                        pattern_type='title'
                    ).order_by(RSSPattern.priority).all()
                    
                    logger.info(f"æ‰¾åˆ° {len(title_patterns)} ä¸ªæ ‡é¢˜æ¨¡å¼?)
                    
                    # è®¾ç½®åˆå§‹å¤„ç†æ–‡æœ¬
                    processing_content = original_content
                    logger.info(f"æ ‡é¢˜æå–åˆå§‹æ–‡æœ¬: {processing_content[:100]}..." if len(processing_content) > 100 else processing_content)
                    
                    # ä¾æ¬¡åº”ç”¨æ¯ä¸ªæ¨¡å¼ï¼Œæ¯æ¬¡å¤„ç†åçš„ç»“æœä½œä¸ºä¸‹ä¸€ä¸ªæ¨¡å¼çš„è¾“å…¥
                    for pattern in title_patterns:
                        logger.info(f"å¼€å§‹å°è¯•æ ‡é¢˜æ¨¡å¼? {pattern.pattern}")
                        try:
                            logger.info(f"å¯¹å†…å®¹åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼: {pattern.pattern}")
                            match = re.search(pattern.pattern, processing_content)
                            if match:
                                logger.info(f"æ‰¾åˆ°åŒ¹é…: {match.groups()}")
                                if match.groups():
                                    entry.title = match.group(1)
                                    logger.info(f"ä½¿ç”¨æ ‡é¢˜æ¨¡å¼ '{pattern.pattern}' æå–åˆ°æ ‡é¢? {entry.title}")
                                else:
                                    logger.warning(f"æ¨¡å¼ '{pattern.pattern}' åŒ¹é…æˆåŠŸä½†æ²¡æœ‰æ•è·ç»„")
                            else:
                                logger.info(f"æ¨¡å¼ '{pattern.pattern}' æœªæ‰¾åˆ°åŒ¹é…?)
                        except Exception as e:
                            logger.error(f"åº”ç”¨æ ‡é¢˜æ­£åˆ™è¡¨è¾¾å¼?'{pattern.pattern}' æ—¶å‡ºé”? {str(e)}")
                            logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                
                # å¦‚æœå¯ç”¨äº†å†…å®¹æ­£åˆ™è¡¨è¾¾å¼æå–
                if rss_config.enable_custom_content_pattern:
                    # ç›´æ¥ä½¿ç”¨ä¼šè¯æŸ¥è¯¢å†…å®¹æ¨¡å¼å¹¶æŒ‰ä¼˜å…ˆçº§æ’åº?
                    content_patterns = session.query(RSSPattern).filter_by(
                        rss_config_id=rss_config.id, 
                        pattern_type='content'
                    ).order_by(RSSPattern.priority).all()
                    
                    logger.info(f"æ‰¾åˆ° {len(content_patterns)} ä¸ªå†…å®¹æ¨¡å¼?)
                    
                    # è®¾ç½®åˆå§‹å¤„ç†æ–‡æœ¬
                    processing_content = original_content
                    logger.info(f"å†…å®¹æå–åˆå§‹æ–‡æœ¬: {processing_content[:100]}..." if len(processing_content) > 100 else processing_content)
                    
                    # ä¾æ¬¡åº”ç”¨æ¯ä¸ªæ¨¡å¼ï¼Œæ¯æ¬¡å¤„ç†åçš„ç»“æœä½œä¸ºä¸‹ä¸€ä¸ªæ¨¡å¼çš„è¾“å…¥
                    for i, pattern in enumerate(content_patterns):
                        try:
                            logger.info(f"[æ­¥éª¤ {i+1}/{len(content_patterns)}] å¯¹å†…å®¹åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼: {pattern.pattern}")
                            logger.info(f"å¤„ç†å‰çš„å†…å®¹é•¿åº¦: {len(processing_content)}, é¢„è§ˆ: {processing_content[:150]}..." if len(processing_content) > 150 else processing_content)
                            
                            match = re.search(pattern.pattern, processing_content)
                            if match and match.groups():
                                extracted_content = match.group(1)
                                processing_content = extracted_content  # æ›´æ–°å¤„ç†å†…å®¹ä¸ºæå–ç»“æ?
                                entry.content = extracted_content
                                
                                logger.info(f"ä½¿ç”¨å†…å®¹æ¨¡å¼ '{pattern.pattern}' æå–åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(extracted_content)}")
                                logger.info(f"å¤„ç†åçš„å†…å®¹é•¿åº¦: {len(processing_content)}, é¢„è§ˆ: {processing_content[:150]}..." if len(processing_content) > 150 else processing_content)
                            else:
                                logger.info(f"æ¨¡å¼ '{pattern.pattern}' æœªæ‰¾åˆ°åŒ¹é…æˆ–æ²¡æœ‰æ•è·ç»„ï¼Œå†…å®¹ä¿æŒä¸å˜")
                        except Exception as e:
                            logger.error(f"åº”ç”¨å†…å®¹æ­£åˆ™è¡¨è¾¾å¼?'{pattern.pattern}' æ—¶å‡ºé”? {str(e)}")
                
                
                # å¦‚æœæ‰§è¡Œåˆ°è¿™é‡Œä½†æ²¡æœ‰æå–åˆ°æ ‡é¢˜ï¼Œåˆ™æ¢å¤åŸæ ‡é¢˜
                if not entry.title and original_title:
                    entry.title = original_title
                    logger.info(f"æ¢å¤åŸæ ‡é¢? {entry.title}")
                
            except Exception as e:
                logger.error(f"ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ ‡é¢˜å’Œå†…å®¹æ—¶å‡ºé”? {str(e)}")


        if entry.sender_info:
            # æ¸…æ¥šç©ºæ ¼å’Œæ¢è¡?
            entry.sender_info = entry.sender_info.strip()
            entry.content = entry.sender_info +":" +"\n\n" + entry.content

        # æ·»åŠ åŸå§‹é“¾æ¥
        if entry.original_link:
            # æ¸…ç†é“¾æ¥ä¸­çš„å‰ç¼€ã€æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ?
            clean_link = entry.original_link.replace("åŸå§‹æ¶ˆæ¯:", "").strip()
            # åˆ é™¤é“¾æ¥ä¸­çš„æ‰€æœ‰æ¢è¡Œç¬¦
            clean_link = clean_link.replace("\n", "").replace("\r", "")
            # å¤„ç†é“¾æ¥ä¸­çš„å¤šä½™ç©ºæ ¼
            clean_link = re.sub(r'\s+', ' ', clean_link).strip()
            
            # ç¡®ä¿é“¾æ¥æ˜¯URLæ ¼å¼
            if clean_link.startswith("http"):
                if entry.author:
                    # ä½¿ç”¨Markdownæ ¼å¼çš„é“¾æ?
                    entry.content += f'\n\n[æ¥æº: {entry.author}]({clean_link})'
                else:
                    # ä½¿ç”¨Markdownæ ¼å¼çš„é“¾æ?
                    entry.content += f'\n\n[æ¥æº]({clean_link})'
                logger.info(f"å·²æ·»åŠ æ¸…ç†åçš„é“¾æ?Markdownæ ¼å¼): {clean_link}")
            else:
                logger.warning(f"é“¾æ¥æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡æ·»åŠ : {clean_link}")
        
        # å¤„ç†åçš„æ¶ˆæ¯
        logger.info(f"å¤„ç†åçš„æ¶ˆæ¯: {entry.content}")

       

        
        # æ·»åŠ æ¡ç›®
        success = await create_entry(entry)
        if success:
            return {"status": "success", "message": f"æ¡ç›®å·²æ·»åŠ ï¼Œåª’ä½“æ–‡ä»¶æ•°é‡: {media_count}"}
        else:
            logger.error("æ·»åŠ æ¡ç›®å¤±è´¥")
            raise HTTPException(status_code=500, detail="æ·»åŠ æ¡ç›®å¤±è´¥")
            
    except ValidationError as e:
        logger.error(f"éªŒè¯é”™è¯¯: {str(e)}")
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.error(f"æ·»åŠ æ¡ç›®æ—¶å‡ºé”? {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/api/entries/{rule_id}/{entry_id}", dependencies=[Depends(verify_local_access)])
async def delete_entry_api(rule_id: int, entry_id: str):
    """åˆ é™¤æ¡ç›® (ä»…é™æœ¬åœ°è®¿é—®)"""
    try:
        # åˆ é™¤æ¡ç›®
        success = await delete_entry(rule_id, entry_id)
        if not success:
            raise HTTPException(status_code=404, detail="æ¡ç›®æœªæ‰¾åˆ?)
        
        return {"status": "success", "message": "æ¡ç›®å·²åˆ é™?}
    except Exception as e:
        logger.error(f"åˆ é™¤æ¡ç›®æ—¶å‡ºé”? {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/api/entries/{rule_id}")
async def list_entries(rule_id: int, limit: int = 20, offset: int = 0):
    """åˆ—å‡ºè§„åˆ™å¯¹åº”çš„æ‰€æœ‰æ¡ç›?""
    try:
        entries = await get_entries(rule_id, limit, offset)
        return {"entries": entries, "total": len(entries), "limit": limit, "offset": offset}
    except Exception as e:
        logger.error(f"è·å–æ¡ç›®åˆ—è¡¨æ—¶å‡ºé”? {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/api/rule/{rule_id}", dependencies=[Depends(verify_local_access)])
async def delete_rule_data(rule_id: int):
    """åˆ é™¤è§„åˆ™ç›¸å…³çš„æ‰€æœ‰æ•°æ®å’Œåª’ä½“æ–‡ä»¶ (ä»…é™æœ¬åœ°è®¿é—®)"""
    try:
        
        
        # è·å–è§„åˆ™çš„æ•°æ®ç›®å½•å’Œåª’ä½“ç›®å½•
        data_path = Path(settings.get_rule_data_path(rule_id))
        media_path = Path(settings.get_rule_media_path(rule_id))
        
        deleted_files = 0
        deleted_dirs = 0
        failed_paths = []
        
        # è¾…åŠ©å‡½æ•°ï¼šå¼ºåˆ¶åˆ é™¤ç›®å½?
        def force_delete_directory(dir_path):
            if not dir_path.exists():
                return True, "ç›®å½•ä¸å­˜åœ?
                
            # æ–¹æ³•1: ä½¿ç”¨ shutil.rmtree
            try:
                shutil.rmtree(dir_path, ignore_errors=True)
                if not dir_path.exists():
                    return True, "ä½¿ç”¨ shutil.rmtree æˆåŠŸåˆ é™¤"
            except Exception as e:
                pass
                
            # æ–¹æ³•2: ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤
            try:
                system = platform.system()
                if system == "Windows":
                    # Windows: ä½¿ç”¨ rd /s /q
                    subprocess.run(["rd", "/s", "/q", str(dir_path)], 
                                  shell=True, 
                                  stderr=subprocess.PIPE, 
                                  stdout=subprocess.PIPE)
                else:
                    # Linux/Mac: ä½¿ç”¨ rm -rf
                    subprocess.run(["rm", "-rf", str(dir_path)], 
                                  stderr=subprocess.PIPE, 
                                  stdout=subprocess.PIPE)
                    
                if not dir_path.exists():
                    return True, "ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤æˆåŠŸåˆ é™¤"
            except Exception as e:
                pass
                
            # æ–¹æ³•3: é‡å‘½åååˆ é™¤
            try:
                temp_path = dir_path.parent / f"temp_delete_{time.time()}"
                os.rename(dir_path, temp_path)
                shutil.rmtree(temp_path, ignore_errors=True)
                if not dir_path.exists() and not temp_path.exists():
                    return True, "ä½¿ç”¨é‡å‘½åååˆ é™¤æˆåŠŸ"
            except Exception as e:
                pass
            
            return False, "æ‰€æœ‰åˆ é™¤æ–¹æ³•éƒ½å¤±è´¥"
        
        # åˆ é™¤åª’ä½“ç›®å½•
        if media_path.exists():
            logger.info(f"å¼€å§‹åˆ é™¤åª’ä½“ç›®å½? {media_path}")
            success, method = force_delete_directory(media_path)
            if success:
                deleted_dirs += 1
                logger.info(f"å·²åˆ é™¤åª’ä½“ç›®å½? {media_path} - {method}")
            else:
                logger.error(f"æ— æ³•åˆ é™¤åª’ä½“ç›®å½•: {media_path} - {method}")
                failed_paths.append(str(media_path))
        
        # åˆ é™¤æ•°æ®ç›®å½•
        if data_path.exists():
            logger.info(f"å¼€å§‹åˆ é™¤æ•°æ®ç›®å½? {data_path}")
            success, method = force_delete_directory(data_path)
            if success:
                deleted_dirs += 1
                logger.info(f"å·²åˆ é™¤æ•°æ®ç›®å½? {data_path} - {method}")
            else:
                logger.error(f"æ— æ³•åˆ é™¤æ•°æ®ç›®å½•: {data_path} - {method}")
                failed_paths.append(str(data_path))
        
        # éªŒè¯åˆ é™¤ç»“æœ
        remaining_paths = []
        if media_path.exists():
            remaining_paths.append(str(media_path))
        if data_path.exists():
            remaining_paths.append(str(data_path))
        
        # è¿”å›åˆ é™¤ç»“æœ
        status = "success" if not remaining_paths else "failed"
        return {
            "status": status,
            "message": f"å¤„ç†è§„åˆ™ {rule_id} çš„æ•°æ®{'å¤±è´¥ï¼Œç›®å½•ä»ç„¶å­˜åœ? if remaining_paths else 'æˆåŠŸï¼Œç›®å½•å·²åˆ é™¤'}",
            "details": {
                "data_path": str(data_path),
                "media_path": str(media_path),
                "deleted_files": deleted_files,
                "deleted_dirs": deleted_dirs,
                "failed_paths": failed_paths,
                "remaining_paths": remaining_paths,
                "data_dir_exists": data_path.exists(),
                "media_dir_exists": media_path.exists()
            }
        }
    except Exception as e:
        logger.error(f"åˆ é™¤è§„åˆ™æ•°æ®æ—¶å‡ºé”? {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è§„åˆ™æ•°æ®æ—¶å‡ºé”? {str(e)}") 